{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product) \n",
    "\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "X_val = X_train[:1000]\n",
    "X_train = X_train[1000:]\n",
    "y_val = y_train[:1000]\n",
    "y_train = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "from keras import models, layers, optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(), loss='mse',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "6500/6500 [==============================] - 1s 80us/step - loss: 0.1140 - acc: 0.3055 - val_loss: 0.0986 - val_acc: 0.5160\n",
      "Epoch 2/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.0817 - acc: 0.6168 - val_loss: 0.0695 - val_acc: 0.6720\n",
      "Epoch 3/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.0531 - acc: 0.7760 - val_loss: 0.0563 - val_acc: 0.7280\n",
      "Epoch 4/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.0379 - acc: 0.8502 - val_loss: 0.0504 - val_acc: 0.7600\n",
      "Epoch 5/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.0293 - acc: 0.8865 - val_loss: 0.0483 - val_acc: 0.7580\n",
      "Epoch 6/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.0238 - acc: 0.9117 - val_loss: 0.0483 - val_acc: 0.7600\n",
      "Epoch 7/120\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.0194 - acc: 0.9334 - val_loss: 0.0474 - val_acc: 0.7650\n",
      "Epoch 8/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0163 - acc: 0.9452 - val_loss: 0.0489 - val_acc: 0.7550\n",
      "Epoch 9/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.0137 - acc: 0.9555 - val_loss: 0.0488 - val_acc: 0.7550\n",
      "Epoch 10/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.0118 - acc: 0.9628 - val_loss: 0.0497 - val_acc: 0.7600\n",
      "Epoch 11/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.0101 - acc: 0.9688 - val_loss: 0.0502 - val_acc: 0.7620\n",
      "Epoch 12/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.0087 - acc: 0.9740 - val_loss: 0.0512 - val_acc: 0.7590\n",
      "Epoch 13/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.0076 - acc: 0.9780 - val_loss: 0.0513 - val_acc: 0.7640\n",
      "Epoch 14/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.0066 - acc: 0.9808 - val_loss: 0.0524 - val_acc: 0.7620\n",
      "Epoch 15/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.0059 - acc: 0.9826 - val_loss: 0.0526 - val_acc: 0.7580\n",
      "Epoch 16/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.0053 - acc: 0.9842 - val_loss: 0.0529 - val_acc: 0.7600\n",
      "Epoch 17/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.0048 - acc: 0.9851 - val_loss: 0.0543 - val_acc: 0.7520\n",
      "Epoch 18/120\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.0044 - acc: 0.9858 - val_loss: 0.0548 - val_acc: 0.7560\n",
      "Epoch 19/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.0041 - acc: 0.9862 - val_loss: 0.0551 - val_acc: 0.7530\n",
      "Epoch 20/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.0038 - acc: 0.9875 - val_loss: 0.0557 - val_acc: 0.7430\n",
      "Epoch 21/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.0035 - acc: 0.9892 - val_loss: 0.0556 - val_acc: 0.7480\n",
      "Epoch 22/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0033 - acc: 0.9895 - val_loss: 0.0560 - val_acc: 0.7520\n",
      "Epoch 23/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0031 - acc: 0.9903 - val_loss: 0.0569 - val_acc: 0.7490\n",
      "Epoch 24/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.0029 - acc: 0.9909 - val_loss: 0.0567 - val_acc: 0.7460\n",
      "Epoch 25/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.0027 - acc: 0.9911 - val_loss: 0.0574 - val_acc: 0.7440\n",
      "Epoch 26/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.0026 - acc: 0.9914 - val_loss: 0.0574 - val_acc: 0.7450\n",
      "Epoch 27/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.0025 - acc: 0.9912 - val_loss: 0.0573 - val_acc: 0.7430\n",
      "Epoch 28/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.0025 - acc: 0.9918 - val_loss: 0.0588 - val_acc: 0.7360\n",
      "Epoch 29/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0023 - acc: 0.9923 - val_loss: 0.0587 - val_acc: 0.7370\n",
      "Epoch 30/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.0023 - acc: 0.9922 - val_loss: 0.0587 - val_acc: 0.7480\n",
      "Epoch 31/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.0022 - acc: 0.9926 - val_loss: 0.0589 - val_acc: 0.7350\n",
      "Epoch 32/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.0021 - acc: 0.9926 - val_loss: 0.0593 - val_acc: 0.7340\n",
      "Epoch 33/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.0021 - acc: 0.9926 - val_loss: 0.0594 - val_acc: 0.7320\n",
      "Epoch 34/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.0020 - acc: 0.9925 - val_loss: 0.0596 - val_acc: 0.7310\n",
      "Epoch 35/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.0020 - acc: 0.9926 - val_loss: 0.0598 - val_acc: 0.7370\n",
      "Epoch 36/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.0020 - acc: 0.9926 - val_loss: 0.0598 - val_acc: 0.7350\n",
      "Epoch 37/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0019 - acc: 0.9928 - val_loss: 0.0602 - val_acc: 0.7340\n",
      "Epoch 38/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.0019 - acc: 0.9928 - val_loss: 0.0603 - val_acc: 0.7320\n",
      "Epoch 39/120\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.0019 - acc: 0.9928 - val_loss: 0.0606 - val_acc: 0.7340\n",
      "Epoch 40/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.0019 - acc: 0.9929 - val_loss: 0.0612 - val_acc: 0.7280\n",
      "Epoch 41/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.0019 - acc: 0.9928 - val_loss: 0.0606 - val_acc: 0.7350\n",
      "Epoch 42/120\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.0018 - acc: 0.9928 - val_loss: 0.0610 - val_acc: 0.7310\n",
      "Epoch 43/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0019 - acc: 0.9929 - val_loss: 0.0609 - val_acc: 0.7340\n",
      "Epoch 44/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.0018 - acc: 0.9929 - val_loss: 0.0608 - val_acc: 0.7320\n",
      "Epoch 45/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0018 - acc: 0.9931 - val_loss: 0.0607 - val_acc: 0.7340\n",
      "Epoch 46/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0018 - acc: 0.9929 - val_loss: 0.0612 - val_acc: 0.7310\n",
      "Epoch 47/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.0018 - acc: 0.9929 - val_loss: 0.0616 - val_acc: 0.7360\n",
      "Epoch 48/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0018 - acc: 0.9934 - val_loss: 0.0617 - val_acc: 0.7280\n",
      "Epoch 49/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.0018 - acc: 0.9934 - val_loss: 0.0616 - val_acc: 0.7270\n",
      "Epoch 50/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0017 - acc: 0.9934 - val_loss: 0.0617 - val_acc: 0.7220\n",
      "Epoch 51/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0017 - acc: 0.9934 - val_loss: 0.0619 - val_acc: 0.7280\n",
      "Epoch 52/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.0017 - acc: 0.9935 - val_loss: 0.0624 - val_acc: 0.7270\n",
      "Epoch 53/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.0017 - acc: 0.9934 - val_loss: 0.0619 - val_acc: 0.7280\n",
      "Epoch 54/120\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.0017 - acc: 0.9934 - val_loss: 0.0620 - val_acc: 0.7300\n",
      "Epoch 55/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 0.9935 - val_loss: 0.0622 - val_acc: 0.7250\n",
      "Epoch 56/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.0017 - acc: 0.9935 - val_loss: 0.0620 - val_acc: 0.7310\n",
      "Epoch 57/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.0017 - acc: 0.9932 - val_loss: 0.0620 - val_acc: 0.7340\n",
      "Epoch 58/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.0017 - acc: 0.9935 - val_loss: 0.0629 - val_acc: 0.7240\n",
      "Epoch 59/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.0017 - acc: 0.9935 - val_loss: 0.0628 - val_acc: 0.7260\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.0016 - acc: 0.9937 - val_loss: 0.0635 - val_acc: 0.7260\n",
      "Epoch 61/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.0016 - acc: 0.9938 - val_loss: 0.0631 - val_acc: 0.7230\n",
      "Epoch 62/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0016 - acc: 0.9940 - val_loss: 0.0631 - val_acc: 0.7270\n",
      "Epoch 63/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0633 - val_acc: 0.7290\n",
      "Epoch 64/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.0015 - acc: 0.9943 - val_loss: 0.0636 - val_acc: 0.7260\n",
      "Epoch 65/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0641 - val_acc: 0.7230\n",
      "Epoch 66/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.0016 - acc: 0.9942 - val_loss: 0.0629 - val_acc: 0.7270\n",
      "Epoch 67/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.0016 - acc: 0.9942 - val_loss: 0.0636 - val_acc: 0.7240\n",
      "Epoch 68/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.0016 - acc: 0.9940 - val_loss: 0.0635 - val_acc: 0.7240\n",
      "Epoch 69/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.0016 - acc: 0.9942 - val_loss: 0.0637 - val_acc: 0.7240\n",
      "Epoch 70/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0633 - val_acc: 0.7240\n",
      "Epoch 71/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.0016 - acc: 0.9942 - val_loss: 0.0633 - val_acc: 0.7240\n",
      "Epoch 72/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.0016 - acc: 0.9938 - val_loss: 0.0627 - val_acc: 0.7300\n",
      "Epoch 73/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0639 - val_acc: 0.7240\n",
      "Epoch 74/120\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0638 - val_acc: 0.7260\n",
      "Epoch 75/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0637 - val_acc: 0.7310\n",
      "Epoch 76/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0635 - val_acc: 0.7230\n",
      "Epoch 77/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0636 - val_acc: 0.7230\n",
      "Epoch 78/120\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.0015 - acc: 0.9940 - val_loss: 0.0633 - val_acc: 0.7260\n",
      "Epoch 79/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 0.9940 - val_loss: 0.0635 - val_acc: 0.7240\n",
      "Epoch 80/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0642 - val_acc: 0.7230\n",
      "Epoch 81/120\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0637 - val_acc: 0.7260\n",
      "Epoch 82/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0641 - val_acc: 0.7230\n",
      "Epoch 83/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0639 - val_acc: 0.7310\n",
      "Epoch 84/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.0016 - acc: 0.9935 - val_loss: 0.0643 - val_acc: 0.7270\n",
      "Epoch 85/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0635 - val_acc: 0.7250\n",
      "Epoch 86/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0650 - val_acc: 0.7230\n",
      "Epoch 87/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 0.9940 - val_loss: 0.0649 - val_acc: 0.7280\n",
      "Epoch 88/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0649 - val_acc: 0.7170\n",
      "Epoch 89/120\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0653 - val_acc: 0.7170\n",
      "Epoch 90/120\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0650 - val_acc: 0.7210\n",
      "Epoch 91/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.0016 - acc: 0.9940 - val_loss: 0.0648 - val_acc: 0.7230\n",
      "Epoch 92/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.0016 - acc: 0.9942 - val_loss: 0.0653 - val_acc: 0.7250\n",
      "Epoch 93/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0645 - val_acc: 0.7250\n",
      "Epoch 94/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.0015 - acc: 0.9940 - val_loss: 0.0659 - val_acc: 0.7240\n",
      "Epoch 95/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0651 - val_acc: 0.7160\n",
      "Epoch 96/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.0015 - acc: 0.9943 - val_loss: 0.0646 - val_acc: 0.7260\n",
      "Epoch 97/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0652 - val_acc: 0.7210\n",
      "Epoch 98/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0015 - acc: 0.9943 - val_loss: 0.0649 - val_acc: 0.7290\n",
      "Epoch 99/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0657 - val_acc: 0.7230\n",
      "Epoch 100/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0656 - val_acc: 0.7250\n",
      "Epoch 101/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0648 - val_acc: 0.7260\n",
      "Epoch 102/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.0015 - acc: 0.9943 - val_loss: 0.0648 - val_acc: 0.7250\n",
      "Epoch 103/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.0015 - acc: 0.9943 - val_loss: 0.0646 - val_acc: 0.7220\n",
      "Epoch 104/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.0015 - acc: 0.9943 - val_loss: 0.0651 - val_acc: 0.7230\n",
      "Epoch 105/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.0015 - acc: 0.9943 - val_loss: 0.0653 - val_acc: 0.7180\n",
      "Epoch 106/120\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.0015 - acc: 0.9943 - val_loss: 0.0670 - val_acc: 0.7190\n",
      "Epoch 107/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0654 - val_acc: 0.7280\n",
      "Epoch 108/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.0015 - acc: 0.9943 - val_loss: 0.0658 - val_acc: 0.7230\n",
      "Epoch 109/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.0015 - acc: 0.9943 - val_loss: 0.0658 - val_acc: 0.7260\n",
      "Epoch 110/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0647 - val_acc: 0.7240\n",
      "Epoch 111/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.0015 - acc: 0.9943 - val_loss: 0.0664 - val_acc: 0.7190\n",
      "Epoch 112/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.0015 - acc: 0.9942 - val_loss: 0.0654 - val_acc: 0.7260\n",
      "Epoch 113/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.0016 - acc: 0.9942 - val_loss: 0.0656 - val_acc: 0.7200\n",
      "Epoch 114/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.0015 - acc: 0.9943 - val_loss: 0.0647 - val_acc: 0.7290\n",
      "Epoch 115/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.0016 - acc: 0.9938 - val_loss: 0.0661 - val_acc: 0.7180\n",
      "Epoch 116/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.0018 - acc: 0.9932 - val_loss: 0.0668 - val_acc: 0.7190\n",
      "Epoch 117/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.0033 - acc: 0.9862 - val_loss: 0.0698 - val_acc: 0.7070\n",
      "Epoch 118/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.0060 - acc: 0.9746 - val_loss: 0.0676 - val_acc: 0.7170\n",
      "Epoch 119/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.0057 - acc: 0.9762 - val_loss: 0.0666 - val_acc: 0.7300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.0039 - acc: 0.9843 - val_loss: 0.0657 - val_acc: 0.7300\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 56us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 0s 86us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.003181432510587351, 0.9881538461538462]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0636395072221756, 0.7408]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "6500/6500 [==============================] - 0s 76us/step - loss: 1.9480 - acc: 0.1708 - val_loss: 1.9355 - val_acc: 0.1670\n",
      "Epoch 2/60\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.9302 - acc: 0.1982 - val_loss: 1.9230 - val_acc: 0.1820\n",
      "Epoch 3/60\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.9170 - acc: 0.2174 - val_loss: 1.9118 - val_acc: 0.2120\n",
      "Epoch 4/60\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.9045 - acc: 0.2325 - val_loss: 1.9001 - val_acc: 0.2240\n",
      "Epoch 5/60\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.8918 - acc: 0.2466 - val_loss: 1.8880 - val_acc: 0.2260\n",
      "Epoch 6/60\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 1.8786 - acc: 0.2569 - val_loss: 1.8743 - val_acc: 0.2400\n",
      "Epoch 7/60\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.8639 - acc: 0.2637 - val_loss: 1.8593 - val_acc: 0.2500\n",
      "Epoch 8/60\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.8477 - acc: 0.2751 - val_loss: 1.8424 - val_acc: 0.2580\n",
      "Epoch 9/60\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.8293 - acc: 0.2831 - val_loss: 1.8234 - val_acc: 0.2780\n",
      "Epoch 10/60\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.8084 - acc: 0.3005 - val_loss: 1.8018 - val_acc: 0.2880\n",
      "Epoch 11/60\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.7848 - acc: 0.3111 - val_loss: 1.7774 - val_acc: 0.2990\n",
      "Epoch 12/60\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.7580 - acc: 0.3305 - val_loss: 1.7502 - val_acc: 0.3230\n",
      "Epoch 13/60\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.7286 - acc: 0.3512 - val_loss: 1.7205 - val_acc: 0.3430\n",
      "Epoch 14/60\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.6962 - acc: 0.3711 - val_loss: 1.6887 - val_acc: 0.3620\n",
      "Epoch 15/60\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.6611 - acc: 0.3963 - val_loss: 1.6548 - val_acc: 0.3870\n",
      "Epoch 16/60\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.6234 - acc: 0.4258 - val_loss: 1.6190 - val_acc: 0.4030\n",
      "Epoch 17/60\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.5834 - acc: 0.4534 - val_loss: 1.5809 - val_acc: 0.4390\n",
      "Epoch 18/60\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.5420 - acc: 0.4860 - val_loss: 1.5432 - val_acc: 0.4580\n",
      "Epoch 19/60\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.4992 - acc: 0.5143 - val_loss: 1.5027 - val_acc: 0.4910\n",
      "Epoch 20/60\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.4553 - acc: 0.5438 - val_loss: 1.4617 - val_acc: 0.5120\n",
      "Epoch 21/60\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.4101 - acc: 0.5746 - val_loss: 1.4192 - val_acc: 0.5390\n",
      "Epoch 22/60\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.3647 - acc: 0.5994 - val_loss: 1.3759 - val_acc: 0.5550\n",
      "Epoch 23/60\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.3189 - acc: 0.6198 - val_loss: 1.3342 - val_acc: 0.5690\n",
      "Epoch 24/60\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 1.2738 - acc: 0.6374 - val_loss: 1.2927 - val_acc: 0.5970\n",
      "Epoch 25/60\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 1.2301 - acc: 0.6482 - val_loss: 1.2523 - val_acc: 0.6090\n",
      "Epoch 26/60\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.1878 - acc: 0.6600 - val_loss: 1.2155 - val_acc: 0.6330\n",
      "Epoch 27/60\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.1470 - acc: 0.6740 - val_loss: 1.1813 - val_acc: 0.6330\n",
      "Epoch 28/60\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 1.1089 - acc: 0.6848 - val_loss: 1.1458 - val_acc: 0.6500\n",
      "Epoch 29/60\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.0725 - acc: 0.6920 - val_loss: 1.1169 - val_acc: 0.6550\n",
      "Epoch 30/60\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.0381 - acc: 0.7034 - val_loss: 1.0837 - val_acc: 0.6590\n",
      "Epoch 31/60\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0059 - acc: 0.7105 - val_loss: 1.0568 - val_acc: 0.6650\n",
      "Epoch 32/60\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9757 - acc: 0.7177 - val_loss: 1.0315 - val_acc: 0.6730\n",
      "Epoch 33/60\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.9471 - acc: 0.7232 - val_loss: 1.0065 - val_acc: 0.6750\n",
      "Epoch 34/60\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9210 - acc: 0.7289 - val_loss: 0.9875 - val_acc: 0.6720\n",
      "Epoch 35/60\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8962 - acc: 0.7334 - val_loss: 0.9638 - val_acc: 0.6860\n",
      "Epoch 36/60\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8733 - acc: 0.7395 - val_loss: 0.9446 - val_acc: 0.6910\n",
      "Epoch 37/60\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8515 - acc: 0.7442 - val_loss: 0.9269 - val_acc: 0.6910\n",
      "Epoch 38/60\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8314 - acc: 0.7474 - val_loss: 0.9114 - val_acc: 0.6930\n",
      "Epoch 39/60\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8128 - acc: 0.7515 - val_loss: 0.8962 - val_acc: 0.6960\n",
      "Epoch 40/60\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.7955 - acc: 0.7560 - val_loss: 0.8858 - val_acc: 0.7020\n",
      "Epoch 41/60\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.7788 - acc: 0.7603 - val_loss: 0.8712 - val_acc: 0.7120\n",
      "Epoch 42/60\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.7631 - acc: 0.7606 - val_loss: 0.8589 - val_acc: 0.7060\n",
      "Epoch 43/60\n",
      "6500/6500 [==============================] - 0s 67us/step - loss: 0.7489 - acc: 0.7640 - val_loss: 0.8479 - val_acc: 0.7040\n",
      "Epoch 44/60\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.7349 - acc: 0.7695 - val_loss: 0.8372 - val_acc: 0.7150\n",
      "Epoch 45/60\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.7223 - acc: 0.7722 - val_loss: 0.8322 - val_acc: 0.7100\n",
      "Epoch 46/60\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.7102 - acc: 0.7742 - val_loss: 0.8211 - val_acc: 0.7040\n",
      "Epoch 47/60\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.6986 - acc: 0.7762 - val_loss: 0.8130 - val_acc: 0.7120\n",
      "Epoch 48/60\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.6879 - acc: 0.7805 - val_loss: 0.8037 - val_acc: 0.7160\n",
      "Epoch 49/60\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.6776 - acc: 0.7814 - val_loss: 0.8002 - val_acc: 0.7140\n",
      "Epoch 50/60\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.6683 - acc: 0.7829 - val_loss: 0.7931 - val_acc: 0.7180\n",
      "Epoch 51/60\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.6581 - acc: 0.7877 - val_loss: 0.7863 - val_acc: 0.7280\n",
      "Epoch 52/60\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.6491 - acc: 0.7871 - val_loss: 0.7837 - val_acc: 0.7220\n",
      "Epoch 53/60\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.6414 - acc: 0.7911 - val_loss: 0.7747 - val_acc: 0.7260\n",
      "Epoch 54/60\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.6328 - acc: 0.7922 - val_loss: 0.7701 - val_acc: 0.7280\n",
      "Epoch 55/60\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.6248 - acc: 0.7960 - val_loss: 0.7658 - val_acc: 0.7310\n",
      "Epoch 56/60\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 0.6176 - acc: 0.7986 - val_loss: 0.7604 - val_acc: 0.7300\n",
      "Epoch 57/60\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.6102 - acc: 0.7991 - val_loss: 0.7578 - val_acc: 0.7250\n",
      "Epoch 58/60\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.6031 - acc: 0.8020 - val_loss: 0.7519 - val_acc: 0.7300\n",
      "Epoch 59/60\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.5958 - acc: 0.8025 - val_loss: 0.7525 - val_acc: 0.7280\n",
      "Epoch 60/60\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.5897 - acc: 0.8058 - val_loss: 0.7459 - val_acc: 0.7310\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 68us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 0s 111us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.58393699158155, 0.8084615384615385]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.706142099571228, 0.7392]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "6500/6500 [==============================] - 1s 94us/step - loss: 2.5998 - acc: 0.1589 - val_loss: 2.5884 - val_acc: 0.1850\n",
      "Epoch 2/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 2.5721 - acc: 0.1891 - val_loss: 2.5675 - val_acc: 0.2070\n",
      "Epoch 3/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 2.5506 - acc: 0.2131 - val_loss: 2.5478 - val_acc: 0.2160\n",
      "Epoch 4/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 2.5293 - acc: 0.2192 - val_loss: 2.5275 - val_acc: 0.2200\n",
      "Epoch 5/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 2.5063 - acc: 0.2326 - val_loss: 2.5049 - val_acc: 0.2310\n",
      "Epoch 6/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 2.4801 - acc: 0.2474 - val_loss: 2.4789 - val_acc: 0.2480\n",
      "Epoch 7/120\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 2.4503 - acc: 0.2780 - val_loss: 2.4486 - val_acc: 0.2650\n",
      "Epoch 8/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 2.4174 - acc: 0.3017 - val_loss: 2.4157 - val_acc: 0.2830\n",
      "Epoch 9/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 2.3819 - acc: 0.3263 - val_loss: 2.3808 - val_acc: 0.2990\n",
      "Epoch 10/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 2.3443 - acc: 0.3586 - val_loss: 2.3430 - val_acc: 0.3230\n",
      "Epoch 11/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 2.3050 - acc: 0.3840 - val_loss: 2.3038 - val_acc: 0.3560\n",
      "Epoch 12/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 2.2642 - acc: 0.4135 - val_loss: 2.2634 - val_acc: 0.3820\n",
      "Epoch 13/120\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 2.2223 - acc: 0.4451 - val_loss: 2.2237 - val_acc: 0.4110\n",
      "Epoch 14/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 2.1794 - acc: 0.4718 - val_loss: 2.1799 - val_acc: 0.4470\n",
      "Epoch 15/120\n",
      "6500/6500 [==============================] - 0s 66us/step - loss: 2.1361 - acc: 0.4997 - val_loss: 2.1397 - val_acc: 0.4650\n",
      "Epoch 16/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 2.0926 - acc: 0.5235 - val_loss: 2.0978 - val_acc: 0.4790\n",
      "Epoch 17/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 2.0497 - acc: 0.5394 - val_loss: 2.0558 - val_acc: 0.5100\n",
      "Epoch 18/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 2.0071 - acc: 0.5632 - val_loss: 2.0164 - val_acc: 0.5240\n",
      "Epoch 19/120\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 1.9656 - acc: 0.5751 - val_loss: 1.9776 - val_acc: 0.5490\n",
      "Epoch 20/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.9257 - acc: 0.5855 - val_loss: 1.9414 - val_acc: 0.5410\n",
      "Epoch 21/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.8866 - acc: 0.5994 - val_loss: 1.9053 - val_acc: 0.5570\n",
      "Epoch 22/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.8499 - acc: 0.6129 - val_loss: 1.8692 - val_acc: 0.5720\n",
      "Epoch 23/120\n",
      "6500/6500 [==============================] - 0s 61us/step - loss: 1.8141 - acc: 0.6223 - val_loss: 1.8374 - val_acc: 0.5770\n",
      "Epoch 24/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.7804 - acc: 0.6305 - val_loss: 1.8068 - val_acc: 0.5950\n",
      "Epoch 25/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.7483 - acc: 0.6398 - val_loss: 1.7770 - val_acc: 0.6110\n",
      "Epoch 26/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.7183 - acc: 0.6455 - val_loss: 1.7504 - val_acc: 0.6190\n",
      "Epoch 27/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.6893 - acc: 0.6555 - val_loss: 1.7262 - val_acc: 0.6240\n",
      "Epoch 28/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.6618 - acc: 0.6626 - val_loss: 1.6999 - val_acc: 0.6360\n",
      "Epoch 29/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.6361 - acc: 0.6695 - val_loss: 1.6814 - val_acc: 0.6320\n",
      "Epoch 30/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.6119 - acc: 0.6757 - val_loss: 1.6593 - val_acc: 0.6450\n",
      "Epoch 31/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.5891 - acc: 0.6812 - val_loss: 1.6372 - val_acc: 0.6480\n",
      "Epoch 32/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.5673 - acc: 0.6883 - val_loss: 1.6190 - val_acc: 0.6510\n",
      "Epoch 33/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.5465 - acc: 0.6931 - val_loss: 1.6017 - val_acc: 0.6580\n",
      "Epoch 34/120\n",
      "6500/6500 [==============================] - 0s 73us/step - loss: 1.5272 - acc: 0.6952 - val_loss: 1.5845 - val_acc: 0.6650\n",
      "Epoch 35/120\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 1.5085 - acc: 0.6982 - val_loss: 1.5710 - val_acc: 0.6670\n",
      "Epoch 36/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.4910 - acc: 0.7040 - val_loss: 1.5521 - val_acc: 0.6680\n",
      "Epoch 37/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.4744 - acc: 0.7043 - val_loss: 1.5396 - val_acc: 0.6680\n",
      "Epoch 38/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.4582 - acc: 0.7102 - val_loss: 1.5264 - val_acc: 0.6700\n",
      "Epoch 39/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.4429 - acc: 0.7138 - val_loss: 1.5126 - val_acc: 0.6760\n",
      "Epoch 40/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.4280 - acc: 0.7175 - val_loss: 1.5030 - val_acc: 0.6770\n",
      "Epoch 41/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.4142 - acc: 0.7223 - val_loss: 1.4892 - val_acc: 0.6810\n",
      "Epoch 42/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.4010 - acc: 0.7248 - val_loss: 1.4791 - val_acc: 0.6830\n",
      "Epoch 43/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.3877 - acc: 0.7278 - val_loss: 1.4662 - val_acc: 0.6910\n",
      "Epoch 44/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.3753 - acc: 0.7322 - val_loss: 1.4606 - val_acc: 0.6820\n",
      "Epoch 45/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.3636 - acc: 0.7345 - val_loss: 1.4452 - val_acc: 0.7040\n",
      "Epoch 46/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.3522 - acc: 0.7369 - val_loss: 1.4367 - val_acc: 0.7020\n",
      "Epoch 47/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.3407 - acc: 0.7406 - val_loss: 1.4288 - val_acc: 0.7010\n",
      "Epoch 48/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.3304 - acc: 0.7442 - val_loss: 1.4223 - val_acc: 0.6960\n",
      "Epoch 49/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.3199 - acc: 0.7463 - val_loss: 1.4146 - val_acc: 0.7060\n",
      "Epoch 50/120\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 1.3100 - acc: 0.7498 - val_loss: 1.4041 - val_acc: 0.7120\n",
      "Epoch 51/120\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.2999 - acc: 0.7518 - val_loss: 1.3962 - val_acc: 0.7110\n",
      "Epoch 52/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.2903 - acc: 0.7538 - val_loss: 1.3892 - val_acc: 0.7100\n",
      "Epoch 53/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.2811 - acc: 0.7571 - val_loss: 1.3826 - val_acc: 0.7180\n",
      "Epoch 54/120\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 1.2719 - acc: 0.7583 - val_loss: 1.3779 - val_acc: 0.7180\n",
      "Epoch 55/120\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 1.2638 - acc: 0.7595 - val_loss: 1.3662 - val_acc: 0.7260\n",
      "Epoch 56/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.2552 - acc: 0.7637 - val_loss: 1.3624 - val_acc: 0.7220\n",
      "Epoch 57/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.2466 - acc: 0.7615 - val_loss: 1.3531 - val_acc: 0.7220\n",
      "Epoch 58/120\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 1.2383 - acc: 0.7655 - val_loss: 1.3484 - val_acc: 0.7260\n",
      "Epoch 59/120\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 1.2306 - acc: 0.7669 - val_loss: 1.3437 - val_acc: 0.7230\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.2227 - acc: 0.7688 - val_loss: 1.3378 - val_acc: 0.7270\n",
      "Epoch 61/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.2154 - acc: 0.7726 - val_loss: 1.3312 - val_acc: 0.7300\n",
      "Epoch 62/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.2078 - acc: 0.7766 - val_loss: 1.3246 - val_acc: 0.7310\n",
      "Epoch 63/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.2006 - acc: 0.7743 - val_loss: 1.3188 - val_acc: 0.7330\n",
      "Epoch 64/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.1934 - acc: 0.7752 - val_loss: 1.3165 - val_acc: 0.7310\n",
      "Epoch 65/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.1861 - acc: 0.7795 - val_loss: 1.3092 - val_acc: 0.7370\n",
      "Epoch 66/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.1799 - acc: 0.7800 - val_loss: 1.3022 - val_acc: 0.7370\n",
      "Epoch 67/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.1728 - acc: 0.7838 - val_loss: 1.2972 - val_acc: 0.7360\n",
      "Epoch 68/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.1663 - acc: 0.7860 - val_loss: 1.2947 - val_acc: 0.7370\n",
      "Epoch 69/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.1595 - acc: 0.7874 - val_loss: 1.2891 - val_acc: 0.7410\n",
      "Epoch 70/120\n",
      "6500/6500 [==============================] - 0s 66us/step - loss: 1.1533 - acc: 0.7871 - val_loss: 1.2870 - val_acc: 0.7390\n",
      "Epoch 71/120\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 1.1471 - acc: 0.7902 - val_loss: 1.2796 - val_acc: 0.7380\n",
      "Epoch 72/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.1411 - acc: 0.7917 - val_loss: 1.2754 - val_acc: 0.7370\n",
      "Epoch 73/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.1349 - acc: 0.7940 - val_loss: 1.2742 - val_acc: 0.7370\n",
      "Epoch 74/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.1290 - acc: 0.7946 - val_loss: 1.2696 - val_acc: 0.7390\n",
      "Epoch 75/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.1228 - acc: 0.7991 - val_loss: 1.2644 - val_acc: 0.7440\n",
      "Epoch 76/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.1172 - acc: 0.7985 - val_loss: 1.2596 - val_acc: 0.7420\n",
      "Epoch 77/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.1113 - acc: 0.8003 - val_loss: 1.2576 - val_acc: 0.7420\n",
      "Epoch 78/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.1060 - acc: 0.8011 - val_loss: 1.2556 - val_acc: 0.7430\n",
      "Epoch 79/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0998 - acc: 0.8045 - val_loss: 1.2473 - val_acc: 0.7470\n",
      "Epoch 80/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0947 - acc: 0.8055 - val_loss: 1.2468 - val_acc: 0.7450\n",
      "Epoch 81/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0894 - acc: 0.8069 - val_loss: 1.2481 - val_acc: 0.7420\n",
      "Epoch 82/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0837 - acc: 0.8086 - val_loss: 1.2403 - val_acc: 0.7440\n",
      "Epoch 83/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.0784 - acc: 0.8077 - val_loss: 1.2398 - val_acc: 0.7450\n",
      "Epoch 84/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.0736 - acc: 0.8106 - val_loss: 1.2321 - val_acc: 0.7430\n",
      "Epoch 85/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.0683 - acc: 0.8115 - val_loss: 1.2294 - val_acc: 0.7450\n",
      "Epoch 86/120\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 1.0633 - acc: 0.8123 - val_loss: 1.2277 - val_acc: 0.7470\n",
      "Epoch 87/120\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 1.0580 - acc: 0.8165 - val_loss: 1.2211 - val_acc: 0.7490\n",
      "Epoch 88/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0529 - acc: 0.8158 - val_loss: 1.2186 - val_acc: 0.7480\n",
      "Epoch 89/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.0482 - acc: 0.8172 - val_loss: 1.2182 - val_acc: 0.7500\n",
      "Epoch 90/120\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 1.0433 - acc: 0.8200 - val_loss: 1.2166 - val_acc: 0.7470\n",
      "Epoch 91/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.0385 - acc: 0.8200 - val_loss: 1.2123 - val_acc: 0.7540\n",
      "Epoch 92/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0334 - acc: 0.8205 - val_loss: 1.2110 - val_acc: 0.7490\n",
      "Epoch 93/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.0290 - acc: 0.8231 - val_loss: 1.2054 - val_acc: 0.7510\n",
      "Epoch 94/120\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 1.0245 - acc: 0.8258 - val_loss: 1.2013 - val_acc: 0.7610\n",
      "Epoch 95/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.0195 - acc: 0.8271 - val_loss: 1.2016 - val_acc: 0.7550\n",
      "Epoch 96/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.0151 - acc: 0.8246 - val_loss: 1.2021 - val_acc: 0.7490\n",
      "Epoch 97/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.0108 - acc: 0.8291 - val_loss: 1.1966 - val_acc: 0.7550\n",
      "Epoch 98/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0062 - acc: 0.8286 - val_loss: 1.1911 - val_acc: 0.7590\n",
      "Epoch 99/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0018 - acc: 0.8302 - val_loss: 1.1894 - val_acc: 0.7500\n",
      "Epoch 100/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9974 - acc: 0.8320 - val_loss: 1.1890 - val_acc: 0.7520\n",
      "Epoch 101/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9933 - acc: 0.8340 - val_loss: 1.1868 - val_acc: 0.7500\n",
      "Epoch 102/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9883 - acc: 0.8332 - val_loss: 1.1819 - val_acc: 0.7540\n",
      "Epoch 103/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9844 - acc: 0.8362 - val_loss: 1.1836 - val_acc: 0.7530\n",
      "Epoch 104/120\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.9801 - acc: 0.8362 - val_loss: 1.1791 - val_acc: 0.7520\n",
      "Epoch 105/120\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.9762 - acc: 0.8377 - val_loss: 1.1780 - val_acc: 0.7540\n",
      "Epoch 106/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.9719 - acc: 0.8398 - val_loss: 1.1730 - val_acc: 0.7590\n",
      "Epoch 107/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.9679 - acc: 0.8394 - val_loss: 1.1722 - val_acc: 0.7540\n",
      "Epoch 108/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.9641 - acc: 0.8392 - val_loss: 1.1693 - val_acc: 0.7550\n",
      "Epoch 109/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9597 - acc: 0.8408 - val_loss: 1.1673 - val_acc: 0.7540\n",
      "Epoch 110/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9559 - acc: 0.8437 - val_loss: 1.1642 - val_acc: 0.7590\n",
      "Epoch 111/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.9519 - acc: 0.8434 - val_loss: 1.1692 - val_acc: 0.7530\n",
      "Epoch 112/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.9479 - acc: 0.8472 - val_loss: 1.1604 - val_acc: 0.7560\n",
      "Epoch 113/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.9439 - acc: 0.8455 - val_loss: 1.1583 - val_acc: 0.7580\n",
      "Epoch 114/120\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9400 - acc: 0.8451 - val_loss: 1.1568 - val_acc: 0.7630\n",
      "Epoch 115/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9364 - acc: 0.8483 - val_loss: 1.1545 - val_acc: 0.7590\n",
      "Epoch 116/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.9326 - acc: 0.8478 - val_loss: 1.1528 - val_acc: 0.7550\n",
      "Epoch 117/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9286 - acc: 0.8508 - val_loss: 1.1519 - val_acc: 0.7530\n",
      "Epoch 118/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9253 - acc: 0.8506 - val_loss: 1.1510 - val_acc: 0.7600\n",
      "Epoch 119/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.9214 - acc: 0.8523 - val_loss: 1.1501 - val_acc: 0.7580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.9176 - acc: 0.8554 - val_loss: 1.1457 - val_acc: 0.7600\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'o', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'y', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "6500/6500 [==============================] - 1s 95us/step - loss: 16.1185 - acc: 0.1388 - val_loss: 15.7570 - val_acc: 0.1500\n",
      "Epoch 2/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 15.4457 - acc: 0.1668 - val_loss: 15.1025 - val_acc: 0.1750\n",
      "Epoch 3/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 14.7993 - acc: 0.1985 - val_loss: 14.4661 - val_acc: 0.2090\n",
      "Epoch 4/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 14.1703 - acc: 0.2272 - val_loss: 13.8455 - val_acc: 0.2380\n",
      "Epoch 5/120\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 13.5554 - acc: 0.2608 - val_loss: 13.2380 - val_acc: 0.2630\n",
      "Epoch 6/120\n",
      "6500/6500 [==============================] - 0s 68us/step - loss: 12.9525 - acc: 0.2935 - val_loss: 12.6412 - val_acc: 0.2910\n",
      "Epoch 7/120\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 12.3609 - acc: 0.3246 - val_loss: 12.0565 - val_acc: 0.3290\n",
      "Epoch 8/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 11.7823 - acc: 0.3626 - val_loss: 11.4871 - val_acc: 0.3610\n",
      "Epoch 9/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 11.2185 - acc: 0.3849 - val_loss: 10.9294 - val_acc: 0.3870\n",
      "Epoch 10/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 10.6686 - acc: 0.4137 - val_loss: 10.3898 - val_acc: 0.4210\n",
      "Epoch 11/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 10.1342 - acc: 0.4422 - val_loss: 9.8663 - val_acc: 0.4400\n",
      "Epoch 12/120\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 9.6156 - acc: 0.4672 - val_loss: 9.3562 - val_acc: 0.4630\n",
      "Epoch 13/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 9.1130 - acc: 0.4885 - val_loss: 8.8647 - val_acc: 0.4980\n",
      "Epoch 14/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 8.6284 - acc: 0.5166 - val_loss: 8.3925 - val_acc: 0.5060\n",
      "Epoch 15/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 8.1606 - acc: 0.5354 - val_loss: 7.9349 - val_acc: 0.5240\n",
      "Epoch 16/120\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 7.7096 - acc: 0.5618 - val_loss: 7.4960 - val_acc: 0.5440\n",
      "Epoch 17/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 7.2756 - acc: 0.5768 - val_loss: 7.0720 - val_acc: 0.5650\n",
      "Epoch 18/120\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 6.8591 - acc: 0.5983 - val_loss: 6.6682 - val_acc: 0.5670\n",
      "Epoch 19/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 6.4612 - acc: 0.6174 - val_loss: 6.2794 - val_acc: 0.5930\n",
      "Epoch 20/120\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 6.0818 - acc: 0.6303 - val_loss: 5.9102 - val_acc: 0.5990\n",
      "Epoch 21/120\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 5.7205 - acc: 0.6422 - val_loss: 5.5593 - val_acc: 0.6200\n",
      "Epoch 22/120\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 5.3780 - acc: 0.6480 - val_loss: 5.2277 - val_acc: 0.6270\n",
      "Epoch 23/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 5.0546 - acc: 0.6594 - val_loss: 4.9145 - val_acc: 0.6390\n",
      "Epoch 24/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 4.7489 - acc: 0.6674 - val_loss: 4.6184 - val_acc: 0.6490\n",
      "Epoch 25/120\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 4.4609 - acc: 0.6658 - val_loss: 4.3402 - val_acc: 0.6550\n",
      "Epoch 26/120\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 4.1907 - acc: 0.6712 - val_loss: 4.0806 - val_acc: 0.6530\n",
      "Epoch 27/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 3.9375 - acc: 0.6711 - val_loss: 3.8357 - val_acc: 0.6590\n",
      "Epoch 28/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 3.7016 - acc: 0.6737 - val_loss: 3.6092 - val_acc: 0.6720\n",
      "Epoch 29/120\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 3.4827 - acc: 0.6782 - val_loss: 3.3988 - val_acc: 0.6710\n",
      "Epoch 30/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 3.2804 - acc: 0.6834 - val_loss: 3.2057 - val_acc: 0.6770\n",
      "Epoch 31/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 3.0948 - acc: 0.6840 - val_loss: 3.0292 - val_acc: 0.6710\n",
      "Epoch 32/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 2.9261 - acc: 0.6825 - val_loss: 2.8665 - val_acc: 0.6810\n",
      "Epoch 33/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 2.7730 - acc: 0.6874 - val_loss: 2.7236 - val_acc: 0.6790\n",
      "Epoch 34/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 2.6360 - acc: 0.6855 - val_loss: 2.5951 - val_acc: 0.6790\n",
      "Epoch 35/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 2.5147 - acc: 0.6900 - val_loss: 2.4806 - val_acc: 0.6760\n",
      "Epoch 36/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 2.4088 - acc: 0.6885 - val_loss: 2.3832 - val_acc: 0.6760\n",
      "Epoch 37/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 2.3172 - acc: 0.6872 - val_loss: 2.2978 - val_acc: 0.6750\n",
      "Epoch 38/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 2.2393 - acc: 0.6875 - val_loss: 2.2253 - val_acc: 0.6790\n",
      "Epoch 39/120\n",
      "6500/6500 [==============================] - 0s 76us/step - loss: 2.1751 - acc: 0.6882 - val_loss: 2.1685 - val_acc: 0.6760\n",
      "Epoch 40/120\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 2.1240 - acc: 0.6877 - val_loss: 2.1205 - val_acc: 0.6810\n",
      "Epoch 41/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 2.0833 - acc: 0.6897 - val_loss: 2.0887 - val_acc: 0.6700\n",
      "Epoch 42/120\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 2.0527 - acc: 0.6875 - val_loss: 2.0597 - val_acc: 0.6770\n",
      "Epoch 43/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 2.0283 - acc: 0.6857 - val_loss: 2.0357 - val_acc: 0.6760\n",
      "Epoch 44/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 2.0070 - acc: 0.6885 - val_loss: 2.0159 - val_acc: 0.6800\n",
      "Epoch 45/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.9887 - acc: 0.6878 - val_loss: 1.9999 - val_acc: 0.6730\n",
      "Epoch 46/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.9716 - acc: 0.6880 - val_loss: 1.9825 - val_acc: 0.6770\n",
      "Epoch 47/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.9563 - acc: 0.6882 - val_loss: 1.9666 - val_acc: 0.6790\n",
      "Epoch 48/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.9420 - acc: 0.6889 - val_loss: 1.9533 - val_acc: 0.6790\n",
      "Epoch 49/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.9281 - acc: 0.6885 - val_loss: 1.9401 - val_acc: 0.6750\n",
      "Epoch 50/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.9144 - acc: 0.6886 - val_loss: 1.9292 - val_acc: 0.6810\n",
      "Epoch 51/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.9021 - acc: 0.6891 - val_loss: 1.9145 - val_acc: 0.6820\n",
      "Epoch 52/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.8902 - acc: 0.6888 - val_loss: 1.9032 - val_acc: 0.6830\n",
      "Epoch 53/120\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 1.8782 - acc: 0.6885 - val_loss: 1.8938 - val_acc: 0.6810\n",
      "Epoch 54/120\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 1.8673 - acc: 0.6911 - val_loss: 1.8808 - val_acc: 0.6890\n",
      "Epoch 55/120\n",
      "6500/6500 [==============================] - 0s 61us/step - loss: 1.8563 - acc: 0.6908 - val_loss: 1.8701 - val_acc: 0.6810\n",
      "Epoch 56/120\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 1.8465 - acc: 0.6915 - val_loss: 1.8627 - val_acc: 0.6830\n",
      "Epoch 57/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.8355 - acc: 0.6906 - val_loss: 1.8498 - val_acc: 0.6870\n",
      "Epoch 58/120\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 1.8258 - acc: 0.6914 - val_loss: 1.8416 - val_acc: 0.6880\n",
      "Epoch 59/120\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 1.8159 - acc: 0.6925 - val_loss: 1.8319 - val_acc: 0.6860\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.8059 - acc: 0.6909 - val_loss: 1.8213 - val_acc: 0.6900\n",
      "Epoch 61/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.7970 - acc: 0.6937 - val_loss: 1.8123 - val_acc: 0.6860\n",
      "Epoch 62/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.7882 - acc: 0.6942 - val_loss: 1.8080 - val_acc: 0.6850\n",
      "Epoch 63/120\n",
      "6500/6500 [==============================] - 0s 66us/step - loss: 1.7793 - acc: 0.6932 - val_loss: 1.7948 - val_acc: 0.6870\n",
      "Epoch 64/120\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 1.7703 - acc: 0.6938 - val_loss: 1.7871 - val_acc: 0.6900\n",
      "Epoch 65/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.7612 - acc: 0.6951 - val_loss: 1.7798 - val_acc: 0.6870\n",
      "Epoch 66/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.7536 - acc: 0.6958 - val_loss: 1.7679 - val_acc: 0.6920\n",
      "Epoch 67/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.7453 - acc: 0.6951 - val_loss: 1.7634 - val_acc: 0.6890\n",
      "Epoch 68/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.7373 - acc: 0.6942 - val_loss: 1.7536 - val_acc: 0.6960\n",
      "Epoch 69/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.7292 - acc: 0.6943 - val_loss: 1.7462 - val_acc: 0.6960\n",
      "Epoch 70/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.7215 - acc: 0.6963 - val_loss: 1.7410 - val_acc: 0.6930\n",
      "Epoch 71/120\n",
      "6500/6500 [==============================] - 0s 74us/step - loss: 1.7140 - acc: 0.6962 - val_loss: 1.7329 - val_acc: 0.6860\n",
      "Epoch 72/120\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 1.7066 - acc: 0.6955 - val_loss: 1.7236 - val_acc: 0.7000\n",
      "Epoch 73/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.6986 - acc: 0.6966 - val_loss: 1.7152 - val_acc: 0.6960\n",
      "Epoch 74/120\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 1.6915 - acc: 0.6975 - val_loss: 1.7154 - val_acc: 0.6880\n",
      "Epoch 75/120\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 1.6845 - acc: 0.6971 - val_loss: 1.7020 - val_acc: 0.6940\n",
      "Epoch 76/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.6767 - acc: 0.6968 - val_loss: 1.6958 - val_acc: 0.6990\n",
      "Epoch 77/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.6703 - acc: 0.6989 - val_loss: 1.6874 - val_acc: 0.6980\n",
      "Epoch 78/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.6628 - acc: 0.6991 - val_loss: 1.6806 - val_acc: 0.6990\n",
      "Epoch 79/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.6560 - acc: 0.6994 - val_loss: 1.6824 - val_acc: 0.6940\n",
      "Epoch 80/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.6494 - acc: 0.6995 - val_loss: 1.6684 - val_acc: 0.6970\n",
      "Epoch 81/120\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.6422 - acc: 0.7008 - val_loss: 1.6616 - val_acc: 0.6960\n",
      "Epoch 82/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.6358 - acc: 0.7009 - val_loss: 1.6571 - val_acc: 0.7010\n",
      "Epoch 83/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.6286 - acc: 0.7015 - val_loss: 1.6497 - val_acc: 0.6980\n",
      "Epoch 84/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.6228 - acc: 0.7018 - val_loss: 1.6405 - val_acc: 0.7000\n",
      "Epoch 85/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.6162 - acc: 0.7009 - val_loss: 1.6338 - val_acc: 0.7030\n",
      "Epoch 86/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.6093 - acc: 0.7023 - val_loss: 1.6285 - val_acc: 0.7020\n",
      "Epoch 87/120\n",
      "6500/6500 [==============================] - 0s 65us/step - loss: 1.6031 - acc: 0.7022 - val_loss: 1.6234 - val_acc: 0.7040\n",
      "Epoch 88/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.5976 - acc: 0.7029 - val_loss: 1.6185 - val_acc: 0.7020\n",
      "Epoch 89/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.5915 - acc: 0.7035 - val_loss: 1.6111 - val_acc: 0.7050\n",
      "Epoch 90/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.5846 - acc: 0.7026 - val_loss: 1.6054 - val_acc: 0.7030\n",
      "Epoch 91/120\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 1.5788 - acc: 0.7043 - val_loss: 1.6005 - val_acc: 0.7050\n",
      "Epoch 92/120\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 1.5733 - acc: 0.7029 - val_loss: 1.5931 - val_acc: 0.7080\n",
      "Epoch 93/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.5666 - acc: 0.7046 - val_loss: 1.5893 - val_acc: 0.7040\n",
      "Epoch 94/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.5610 - acc: 0.7040 - val_loss: 1.5836 - val_acc: 0.7040\n",
      "Epoch 95/120\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 1.5548 - acc: 0.7054 - val_loss: 1.5799 - val_acc: 0.7060\n",
      "Epoch 96/120\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 1.5491 - acc: 0.7063 - val_loss: 1.5711 - val_acc: 0.7040\n",
      "Epoch 97/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.5436 - acc: 0.7066 - val_loss: 1.5660 - val_acc: 0.7070\n",
      "Epoch 98/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.5380 - acc: 0.7065 - val_loss: 1.5659 - val_acc: 0.7090\n",
      "Epoch 99/120\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.5326 - acc: 0.7058 - val_loss: 1.5553 - val_acc: 0.7060\n",
      "Epoch 100/120\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.5272 - acc: 0.7074 - val_loss: 1.5491 - val_acc: 0.7020\n",
      "Epoch 101/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.5214 - acc: 0.7068 - val_loss: 1.5435 - val_acc: 0.7060\n",
      "Epoch 102/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.5157 - acc: 0.7077 - val_loss: 1.5377 - val_acc: 0.7070\n",
      "Epoch 103/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.5106 - acc: 0.7094 - val_loss: 1.5333 - val_acc: 0.7120\n",
      "Epoch 104/120\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.5048 - acc: 0.7069 - val_loss: 1.5274 - val_acc: 0.7060\n",
      "Epoch 105/120\n",
      "6500/6500 [==============================] - 0s 77us/step - loss: 1.4999 - acc: 0.7077 - val_loss: 1.5218 - val_acc: 0.7060\n",
      "Epoch 106/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.4947 - acc: 0.7071 - val_loss: 1.5182 - val_acc: 0.7110\n",
      "Epoch 107/120\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.4895 - acc: 0.7080 - val_loss: 1.5134 - val_acc: 0.7110\n",
      "Epoch 108/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.4843 - acc: 0.7097 - val_loss: 1.5057 - val_acc: 0.7110\n",
      "Epoch 109/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.4792 - acc: 0.7077 - val_loss: 1.5021 - val_acc: 0.7120\n",
      "Epoch 110/120\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.4746 - acc: 0.7088 - val_loss: 1.4966 - val_acc: 0.7100\n",
      "Epoch 111/120\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.4691 - acc: 0.7097 - val_loss: 1.4905 - val_acc: 0.7100\n",
      "Epoch 112/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.4638 - acc: 0.7097 - val_loss: 1.4868 - val_acc: 0.7110\n",
      "Epoch 113/120\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.4592 - acc: 0.7102 - val_loss: 1.4818 - val_acc: 0.7110\n",
      "Epoch 114/120\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 1.4534 - acc: 0.7129 - val_loss: 1.4786 - val_acc: 0.7130\n",
      "Epoch 115/120\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.4494 - acc: 0.7108 - val_loss: 1.4745 - val_acc: 0.7100\n",
      "Epoch 116/120\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 1.4442 - acc: 0.7123 - val_loss: 1.4685 - val_acc: 0.7120\n",
      "Epoch 117/120\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 1.4393 - acc: 0.7117 - val_loss: 1.4632 - val_acc: 0.7130\n",
      "Epoch 118/120\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.4351 - acc: 0.7129 - val_loss: 1.4673 - val_acc: 0.7150\n",
      "Epoch 119/120\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.4306 - acc: 0.7143 - val_loss: 1.4548 - val_acc: 0.7160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 1.4256 - acc: 0.7117 - val_loss: 1.4506 - val_acc: 0.7090\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "6500/6500 [==============================] - 1s 101us/step - loss: 16.0725 - acc: 0.1671 - val_loss: 15.7129 - val_acc: 0.1630\n",
      "Epoch 2/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 15.4080 - acc: 0.1846 - val_loss: 15.0616 - val_acc: 0.1750\n",
      "Epoch 3/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 14.7641 - acc: 0.2065 - val_loss: 14.4280 - val_acc: 0.2010\n",
      "Epoch 4/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 14.1367 - acc: 0.2357 - val_loss: 13.8103 - val_acc: 0.2300\n",
      "Epoch 5/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 13.5247 - acc: 0.2660 - val_loss: 13.2066 - val_acc: 0.2560\n",
      "Epoch 6/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 12.9263 - acc: 0.2962 - val_loss: 12.6158 - val_acc: 0.2970\n",
      "Epoch 7/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 12.3421 - acc: 0.3283 - val_loss: 12.0391 - val_acc: 0.3210\n",
      "Epoch 8/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 11.7714 - acc: 0.3543 - val_loss: 11.4762 - val_acc: 0.3370\n",
      "Epoch 9/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 11.2149 - acc: 0.3768 - val_loss: 10.9266 - val_acc: 0.3750\n",
      "Epoch 10/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 10.6723 - acc: 0.4025 - val_loss: 10.3929 - val_acc: 0.3870\n",
      "Epoch 11/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 10.1452 - acc: 0.4157 - val_loss: 9.8752 - val_acc: 0.4040\n",
      "Epoch 12/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 9.6341 - acc: 0.4315 - val_loss: 9.3738 - val_acc: 0.4130\n",
      "Epoch 13/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 9.1383 - acc: 0.4452 - val_loss: 8.8874 - val_acc: 0.4390\n",
      "Epoch 14/1000\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 8.6593 - acc: 0.4615 - val_loss: 8.4179 - val_acc: 0.4440\n",
      "Epoch 15/1000\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 8.1972 - acc: 0.4823 - val_loss: 7.9680 - val_acc: 0.4540\n",
      "Epoch 16/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 7.7525 - acc: 0.4895 - val_loss: 7.5339 - val_acc: 0.4820\n",
      "Epoch 17/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 7.3252 - acc: 0.5045 - val_loss: 7.1167 - val_acc: 0.4950\n",
      "Epoch 18/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 6.9158 - acc: 0.5222 - val_loss: 6.7175 - val_acc: 0.5110\n",
      "Epoch 19/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 6.5244 - acc: 0.5337 - val_loss: 6.3373 - val_acc: 0.5190\n",
      "Epoch 20/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 6.1499 - acc: 0.5472 - val_loss: 5.9743 - val_acc: 0.5210\n",
      "Epoch 21/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 5.7932 - acc: 0.5595 - val_loss: 5.6264 - val_acc: 0.5460\n",
      "Epoch 22/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 5.4535 - acc: 0.5723 - val_loss: 5.2975 - val_acc: 0.5490\n",
      "Epoch 23/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 5.1304 - acc: 0.5842 - val_loss: 4.9843 - val_acc: 0.5520\n",
      "Epoch 24/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 4.8249 - acc: 0.5943 - val_loss: 4.6888 - val_acc: 0.5670\n",
      "Epoch 25/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 4.5367 - acc: 0.6029 - val_loss: 4.4102 - val_acc: 0.5710\n",
      "Epoch 26/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 4.2656 - acc: 0.6085 - val_loss: 4.1488 - val_acc: 0.5890\n",
      "Epoch 27/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 4.0114 - acc: 0.6163 - val_loss: 3.9031 - val_acc: 0.6010\n",
      "Epoch 28/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 3.7732 - acc: 0.6242 - val_loss: 3.6764 - val_acc: 0.5960\n",
      "Epoch 29/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 3.5522 - acc: 0.6249 - val_loss: 3.4634 - val_acc: 0.6120\n",
      "Epoch 30/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 3.3477 - acc: 0.6302 - val_loss: 3.2671 - val_acc: 0.6130\n",
      "Epoch 31/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 3.1592 - acc: 0.6328 - val_loss: 3.0877 - val_acc: 0.6150\n",
      "Epoch 32/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 2.9868 - acc: 0.6418 - val_loss: 2.9230 - val_acc: 0.6140\n",
      "Epoch 33/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 2.8305 - acc: 0.6411 - val_loss: 2.7760 - val_acc: 0.6130\n",
      "Epoch 34/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 2.6899 - acc: 0.6446 - val_loss: 2.6415 - val_acc: 0.6260\n",
      "Epoch 35/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 2.5647 - acc: 0.6462 - val_loss: 2.5242 - val_acc: 0.6250\n",
      "Epoch 36/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 2.4547 - acc: 0.6498 - val_loss: 2.4234 - val_acc: 0.6340\n",
      "Epoch 37/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 2.3609 - acc: 0.6548 - val_loss: 2.3356 - val_acc: 0.6290\n",
      "Epoch 38/1000\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 2.2804 - acc: 0.6529 - val_loss: 2.2618 - val_acc: 0.6280\n",
      "Epoch 39/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 2.2135 - acc: 0.6555 - val_loss: 2.2036 - val_acc: 0.6390\n",
      "Epoch 40/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 2.1609 - acc: 0.6562 - val_loss: 2.1536 - val_acc: 0.6390\n",
      "Epoch 41/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 2.1185 - acc: 0.6574 - val_loss: 2.1180 - val_acc: 0.6410\n",
      "Epoch 42/1000\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 2.0864 - acc: 0.6580 - val_loss: 2.0889 - val_acc: 0.6390\n",
      "Epoch 43/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 2.0614 - acc: 0.6592 - val_loss: 2.0684 - val_acc: 0.6440\n",
      "Epoch 44/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 2.0395 - acc: 0.6618 - val_loss: 2.0456 - val_acc: 0.6440\n",
      "Epoch 45/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 2.0209 - acc: 0.6625 - val_loss: 2.0269 - val_acc: 0.6500\n",
      "Epoch 46/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 2.0035 - acc: 0.6640 - val_loss: 2.0117 - val_acc: 0.6460\n",
      "Epoch 47/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.9872 - acc: 0.6642 - val_loss: 1.9962 - val_acc: 0.6520\n",
      "Epoch 48/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.9712 - acc: 0.6646 - val_loss: 1.9815 - val_acc: 0.6450\n",
      "Epoch 49/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.9567 - acc: 0.6643 - val_loss: 1.9669 - val_acc: 0.6500\n",
      "Epoch 50/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.9427 - acc: 0.6671 - val_loss: 1.9519 - val_acc: 0.6540\n",
      "Epoch 51/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.9292 - acc: 0.6666 - val_loss: 1.9381 - val_acc: 0.6600\n",
      "Epoch 52/1000\n",
      "6500/6500 [==============================] - 0s 64us/step - loss: 1.9166 - acc: 0.6671 - val_loss: 1.9305 - val_acc: 0.6520\n",
      "Epoch 53/1000\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.9034 - acc: 0.6712 - val_loss: 1.9138 - val_acc: 0.6630\n",
      "Epoch 54/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.8920 - acc: 0.6714 - val_loss: 1.9032 - val_acc: 0.6560\n",
      "Epoch 55/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.8806 - acc: 0.6729 - val_loss: 1.8946 - val_acc: 0.6520\n",
      "Epoch 56/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.8691 - acc: 0.6709 - val_loss: 1.8792 - val_acc: 0.6670\n",
      "Epoch 57/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.8577 - acc: 0.6742 - val_loss: 1.8698 - val_acc: 0.6610\n",
      "Epoch 58/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.8470 - acc: 0.6728 - val_loss: 1.8588 - val_acc: 0.6720\n",
      "Epoch 59/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.8366 - acc: 0.6769 - val_loss: 1.8472 - val_acc: 0.6690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.8267 - acc: 0.6783 - val_loss: 1.8371 - val_acc: 0.6720\n",
      "Epoch 61/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.8164 - acc: 0.6791 - val_loss: 1.8273 - val_acc: 0.6710\n",
      "Epoch 62/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.8067 - acc: 0.6782 - val_loss: 1.8234 - val_acc: 0.6590\n",
      "Epoch 63/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.7975 - acc: 0.6798 - val_loss: 1.8081 - val_acc: 0.6720\n",
      "Epoch 64/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.7881 - acc: 0.6812 - val_loss: 1.8004 - val_acc: 0.6660\n",
      "Epoch 65/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.7792 - acc: 0.6808 - val_loss: 1.7917 - val_acc: 0.6730\n",
      "Epoch 66/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.7702 - acc: 0.6808 - val_loss: 1.7832 - val_acc: 0.6690\n",
      "Epoch 67/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.7617 - acc: 0.6823 - val_loss: 1.7744 - val_acc: 0.6720\n",
      "Epoch 68/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.7532 - acc: 0.6822 - val_loss: 1.7650 - val_acc: 0.6760\n",
      "Epoch 69/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.7446 - acc: 0.6848 - val_loss: 1.7580 - val_acc: 0.6670\n",
      "Epoch 70/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 1.7368 - acc: 0.6849 - val_loss: 1.7488 - val_acc: 0.6790\n",
      "Epoch 71/1000\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 1.7279 - acc: 0.6848 - val_loss: 1.7410 - val_acc: 0.6750\n",
      "Epoch 72/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.7200 - acc: 0.6858 - val_loss: 1.7331 - val_acc: 0.6750\n",
      "Epoch 73/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.7122 - acc: 0.6857 - val_loss: 1.7271 - val_acc: 0.6790\n",
      "Epoch 74/1000\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 1.7042 - acc: 0.6865 - val_loss: 1.7173 - val_acc: 0.6790\n",
      "Epoch 75/1000\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 1.6965 - acc: 0.6866 - val_loss: 1.7099 - val_acc: 0.6760\n",
      "Epoch 76/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.6889 - acc: 0.6866 - val_loss: 1.7037 - val_acc: 0.6760\n",
      "Epoch 77/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.6817 - acc: 0.6898 - val_loss: 1.6962 - val_acc: 0.6820\n",
      "Epoch 78/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.6744 - acc: 0.6880 - val_loss: 1.6868 - val_acc: 0.6820\n",
      "Epoch 79/1000\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.6667 - acc: 0.6902 - val_loss: 1.6823 - val_acc: 0.6850\n",
      "Epoch 80/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.6601 - acc: 0.6895 - val_loss: 1.6730 - val_acc: 0.6830\n",
      "Epoch 81/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.6525 - acc: 0.6915 - val_loss: 1.6673 - val_acc: 0.6840\n",
      "Epoch 82/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.6460 - acc: 0.6909 - val_loss: 1.6602 - val_acc: 0.6890\n",
      "Epoch 83/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.6381 - acc: 0.6935 - val_loss: 1.6551 - val_acc: 0.6790\n",
      "Epoch 84/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.6319 - acc: 0.6905 - val_loss: 1.6474 - val_acc: 0.6780\n",
      "Epoch 85/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.6256 - acc: 0.6925 - val_loss: 1.6425 - val_acc: 0.6830\n",
      "Epoch 86/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.6190 - acc: 0.6929 - val_loss: 1.6313 - val_acc: 0.6840\n",
      "Epoch 87/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.6123 - acc: 0.6951 - val_loss: 1.6267 - val_acc: 0.6840\n",
      "Epoch 88/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.6059 - acc: 0.6946 - val_loss: 1.6205 - val_acc: 0.6850\n",
      "Epoch 89/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.5999 - acc: 0.6948 - val_loss: 1.6134 - val_acc: 0.6830\n",
      "Epoch 90/1000\n",
      "6500/6500 [==============================] - 0s 67us/step - loss: 1.5933 - acc: 0.6940 - val_loss: 1.6076 - val_acc: 0.6850\n",
      "Epoch 91/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.5868 - acc: 0.6966 - val_loss: 1.6015 - val_acc: 0.6850\n",
      "Epoch 92/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.5813 - acc: 0.6954 - val_loss: 1.5980 - val_acc: 0.6880\n",
      "Epoch 93/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.5751 - acc: 0.6957 - val_loss: 1.5891 - val_acc: 0.6870\n",
      "Epoch 94/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.5687 - acc: 0.6971 - val_loss: 1.5848 - val_acc: 0.6910\n",
      "Epoch 95/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.5632 - acc: 0.6974 - val_loss: 1.5775 - val_acc: 0.6860\n",
      "Epoch 96/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.5568 - acc: 0.6995 - val_loss: 1.5795 - val_acc: 0.6870\n",
      "Epoch 97/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.5516 - acc: 0.6963 - val_loss: 1.5666 - val_acc: 0.6920\n",
      "Epoch 98/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.5452 - acc: 0.6974 - val_loss: 1.5612 - val_acc: 0.6900\n",
      "Epoch 99/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.5399 - acc: 0.6998 - val_loss: 1.5533 - val_acc: 0.6920\n",
      "Epoch 100/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.5340 - acc: 0.7002 - val_loss: 1.5516 - val_acc: 0.6920\n",
      "Epoch 101/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.5283 - acc: 0.7006 - val_loss: 1.5430 - val_acc: 0.6890\n",
      "Epoch 102/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.5230 - acc: 0.7003 - val_loss: 1.5437 - val_acc: 0.6930\n",
      "Epoch 103/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.5179 - acc: 0.7003 - val_loss: 1.5342 - val_acc: 0.6960\n",
      "Epoch 104/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.5122 - acc: 0.7014 - val_loss: 1.5288 - val_acc: 0.6940\n",
      "Epoch 105/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.5066 - acc: 0.7020 - val_loss: 1.5222 - val_acc: 0.6940\n",
      "Epoch 106/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.5017 - acc: 0.7031 - val_loss: 1.5172 - val_acc: 0.6950\n",
      "Epoch 107/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.4966 - acc: 0.7031 - val_loss: 1.5106 - val_acc: 0.6960\n",
      "Epoch 108/1000\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 1.4910 - acc: 0.7035 - val_loss: 1.5079 - val_acc: 0.6950\n",
      "Epoch 109/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.4865 - acc: 0.7040 - val_loss: 1.5015 - val_acc: 0.6950\n",
      "Epoch 110/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.4812 - acc: 0.7038 - val_loss: 1.4992 - val_acc: 0.6940\n",
      "Epoch 111/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.4763 - acc: 0.7055 - val_loss: 1.4920 - val_acc: 0.6930\n",
      "Epoch 112/1000\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.4718 - acc: 0.7035 - val_loss: 1.4871 - val_acc: 0.7010\n",
      "Epoch 113/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.4664 - acc: 0.7040 - val_loss: 1.4828 - val_acc: 0.6960\n",
      "Epoch 114/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.4619 - acc: 0.7060 - val_loss: 1.4836 - val_acc: 0.7000\n",
      "Epoch 115/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.4573 - acc: 0.7069 - val_loss: 1.4796 - val_acc: 0.7000\n",
      "Epoch 116/1000\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.4531 - acc: 0.7062 - val_loss: 1.4706 - val_acc: 0.6980\n",
      "Epoch 117/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.4476 - acc: 0.7068 - val_loss: 1.4642 - val_acc: 0.7010\n",
      "Epoch 118/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.4424 - acc: 0.7075 - val_loss: 1.4591 - val_acc: 0.7010\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.4384 - acc: 0.7080 - val_loss: 1.4551 - val_acc: 0.7020\n",
      "Epoch 120/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.4339 - acc: 0.7098 - val_loss: 1.4501 - val_acc: 0.7000\n",
      "Epoch 121/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.4293 - acc: 0.7078 - val_loss: 1.4491 - val_acc: 0.6980\n",
      "Epoch 122/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.4251 - acc: 0.7078 - val_loss: 1.4423 - val_acc: 0.7040\n",
      "Epoch 123/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.4208 - acc: 0.7089 - val_loss: 1.4445 - val_acc: 0.6990\n",
      "Epoch 124/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.4168 - acc: 0.7111 - val_loss: 1.4362 - val_acc: 0.7020\n",
      "Epoch 125/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.4121 - acc: 0.7102 - val_loss: 1.4343 - val_acc: 0.7060\n",
      "Epoch 126/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.4082 - acc: 0.7100 - val_loss: 1.4239 - val_acc: 0.7020\n",
      "Epoch 127/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.4028 - acc: 0.7106 - val_loss: 1.4204 - val_acc: 0.7000\n",
      "Epoch 128/1000\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.3992 - acc: 0.7111 - val_loss: 1.4184 - val_acc: 0.7000\n",
      "Epoch 129/1000\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 1.3951 - acc: 0.7128 - val_loss: 1.4126 - val_acc: 0.7050\n",
      "Epoch 130/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.3914 - acc: 0.7115 - val_loss: 1.4079 - val_acc: 0.7080\n",
      "Epoch 131/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.3867 - acc: 0.7132 - val_loss: 1.4043 - val_acc: 0.7040\n",
      "Epoch 132/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.3831 - acc: 0.7122 - val_loss: 1.4028 - val_acc: 0.7070\n",
      "Epoch 133/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.3791 - acc: 0.7112 - val_loss: 1.3969 - val_acc: 0.7040\n",
      "Epoch 134/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.3755 - acc: 0.7125 - val_loss: 1.3917 - val_acc: 0.7040\n",
      "Epoch 135/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.3709 - acc: 0.7142 - val_loss: 1.3887 - val_acc: 0.7030\n",
      "Epoch 136/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.3668 - acc: 0.7131 - val_loss: 1.3918 - val_acc: 0.7000\n",
      "Epoch 137/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.3630 - acc: 0.7128 - val_loss: 1.3814 - val_acc: 0.7060\n",
      "Epoch 138/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.3604 - acc: 0.7140 - val_loss: 1.3777 - val_acc: 0.7040\n",
      "Epoch 139/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.3563 - acc: 0.7143 - val_loss: 1.3731 - val_acc: 0.7030\n",
      "Epoch 140/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.3526 - acc: 0.7125 - val_loss: 1.3717 - val_acc: 0.7080\n",
      "Epoch 141/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.3482 - acc: 0.7149 - val_loss: 1.3799 - val_acc: 0.7100\n",
      "Epoch 142/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.3451 - acc: 0.7146 - val_loss: 1.3613 - val_acc: 0.7050\n",
      "Epoch 143/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.3412 - acc: 0.7165 - val_loss: 1.3593 - val_acc: 0.7070\n",
      "Epoch 144/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.3375 - acc: 0.7154 - val_loss: 1.3555 - val_acc: 0.7050\n",
      "Epoch 145/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.3343 - acc: 0.7166 - val_loss: 1.3528 - val_acc: 0.7110\n",
      "Epoch 146/1000\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 1.3301 - acc: 0.7158 - val_loss: 1.3484 - val_acc: 0.7040\n",
      "Epoch 147/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.3268 - acc: 0.7198 - val_loss: 1.3454 - val_acc: 0.7070\n",
      "Epoch 148/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.3232 - acc: 0.7163 - val_loss: 1.3427 - val_acc: 0.7110\n",
      "Epoch 149/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.3202 - acc: 0.7166 - val_loss: 1.3399 - val_acc: 0.7080\n",
      "Epoch 150/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.3169 - acc: 0.7175 - val_loss: 1.3336 - val_acc: 0.7090\n",
      "Epoch 151/1000\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.3134 - acc: 0.7178 - val_loss: 1.3324 - val_acc: 0.7130\n",
      "Epoch 152/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.3109 - acc: 0.7165 - val_loss: 1.3305 - val_acc: 0.7120\n",
      "Epoch 153/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.3076 - acc: 0.7178 - val_loss: 1.3253 - val_acc: 0.7110\n",
      "Epoch 154/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.3039 - acc: 0.7185 - val_loss: 1.3241 - val_acc: 0.7120\n",
      "Epoch 155/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.2999 - acc: 0.7198 - val_loss: 1.3220 - val_acc: 0.7060\n",
      "Epoch 156/1000\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 1.2974 - acc: 0.7208 - val_loss: 1.3178 - val_acc: 0.7080\n",
      "Epoch 157/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.2937 - acc: 0.7205 - val_loss: 1.3118 - val_acc: 0.7120\n",
      "Epoch 158/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.2901 - acc: 0.7206 - val_loss: 1.3086 - val_acc: 0.7100\n",
      "Epoch 159/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.2878 - acc: 0.7211 - val_loss: 1.3059 - val_acc: 0.7130\n",
      "Epoch 160/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.2843 - acc: 0.7215 - val_loss: 1.3026 - val_acc: 0.7150\n",
      "Epoch 161/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.2819 - acc: 0.7228 - val_loss: 1.3046 - val_acc: 0.7150\n",
      "Epoch 162/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.2792 - acc: 0.7218 - val_loss: 1.2975 - val_acc: 0.7120\n",
      "Epoch 163/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.2758 - acc: 0.7217 - val_loss: 1.2960 - val_acc: 0.7200\n",
      "Epoch 164/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.2728 - acc: 0.7222 - val_loss: 1.2939 - val_acc: 0.7100\n",
      "Epoch 165/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.2701 - acc: 0.7218 - val_loss: 1.2898 - val_acc: 0.7170\n",
      "Epoch 166/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 1.2667 - acc: 0.7229 - val_loss: 1.2926 - val_acc: 0.7180\n",
      "Epoch 167/1000\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 1.2647 - acc: 0.7232 - val_loss: 1.2829 - val_acc: 0.7200\n",
      "Epoch 168/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.2617 - acc: 0.7218 - val_loss: 1.2792 - val_acc: 0.7100\n",
      "Epoch 169/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.2587 - acc: 0.7220 - val_loss: 1.2772 - val_acc: 0.7140\n",
      "Epoch 170/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.2559 - acc: 0.7255 - val_loss: 1.2746 - val_acc: 0.7110\n",
      "Epoch 171/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.2532 - acc: 0.7237 - val_loss: 1.2744 - val_acc: 0.7140\n",
      "Epoch 172/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.2503 - acc: 0.7229 - val_loss: 1.2686 - val_acc: 0.7170\n",
      "Epoch 173/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.2479 - acc: 0.7235 - val_loss: 1.2685 - val_acc: 0.7080\n",
      "Epoch 174/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 1.2448 - acc: 0.7257 - val_loss: 1.2640 - val_acc: 0.7130\n",
      "Epoch 175/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.2428 - acc: 0.7260 - val_loss: 1.2620 - val_acc: 0.7160\n",
      "Epoch 176/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.2394 - acc: 0.7274 - val_loss: 1.2606 - val_acc: 0.7120\n",
      "Epoch 177/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.2372 - acc: 0.7266 - val_loss: 1.2562 - val_acc: 0.7170\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.2345 - acc: 0.7263 - val_loss: 1.2511 - val_acc: 0.7190\n",
      "Epoch 179/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.2310 - acc: 0.7257 - val_loss: 1.2568 - val_acc: 0.7150\n",
      "Epoch 180/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.2298 - acc: 0.7275 - val_loss: 1.2524 - val_acc: 0.7150\n",
      "Epoch 181/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.2275 - acc: 0.7277 - val_loss: 1.2464 - val_acc: 0.7220\n",
      "Epoch 182/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.2245 - acc: 0.7274 - val_loss: 1.2457 - val_acc: 0.7170\n",
      "Epoch 183/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.2222 - acc: 0.7269 - val_loss: 1.2414 - val_acc: 0.7200\n",
      "Epoch 184/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.2196 - acc: 0.7294 - val_loss: 1.2424 - val_acc: 0.7160\n",
      "Epoch 185/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.2171 - acc: 0.7288 - val_loss: 1.2418 - val_acc: 0.7180\n",
      "Epoch 186/1000\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 1.2151 - acc: 0.7274 - val_loss: 1.2357 - val_acc: 0.7220\n",
      "Epoch 187/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.2124 - acc: 0.7295 - val_loss: 1.2326 - val_acc: 0.7190\n",
      "Epoch 188/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.2106 - acc: 0.7274 - val_loss: 1.2286 - val_acc: 0.7170\n",
      "Epoch 189/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.2082 - acc: 0.7274 - val_loss: 1.2332 - val_acc: 0.7170\n",
      "Epoch 190/1000\n",
      "6500/6500 [==============================] - 0s 61us/step - loss: 1.2067 - acc: 0.7291 - val_loss: 1.2297 - val_acc: 0.7200\n",
      "Epoch 191/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.2045 - acc: 0.7288 - val_loss: 1.2247 - val_acc: 0.7200\n",
      "Epoch 192/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.2022 - acc: 0.7278 - val_loss: 1.2223 - val_acc: 0.7170\n",
      "Epoch 193/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.2003 - acc: 0.7302 - val_loss: 1.2173 - val_acc: 0.7210\n",
      "Epoch 194/1000\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.1973 - acc: 0.7305 - val_loss: 1.2196 - val_acc: 0.7180\n",
      "Epoch 195/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 1.1953 - acc: 0.7298 - val_loss: 1.2145 - val_acc: 0.7250\n",
      "Epoch 196/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.1933 - acc: 0.7283 - val_loss: 1.2137 - val_acc: 0.7220\n",
      "Epoch 197/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.1913 - acc: 0.7314 - val_loss: 1.2099 - val_acc: 0.7240\n",
      "Epoch 198/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1890 - acc: 0.7305 - val_loss: 1.2075 - val_acc: 0.7200\n",
      "Epoch 199/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.1874 - acc: 0.7306 - val_loss: 1.2070 - val_acc: 0.7210\n",
      "Epoch 200/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.1854 - acc: 0.7322 - val_loss: 1.2018 - val_acc: 0.7200\n",
      "Epoch 201/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.1827 - acc: 0.7311 - val_loss: 1.2011 - val_acc: 0.7260\n",
      "Epoch 202/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1816 - acc: 0.7312 - val_loss: 1.2011 - val_acc: 0.7240\n",
      "Epoch 203/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1794 - acc: 0.7306 - val_loss: 1.2045 - val_acc: 0.7160\n",
      "Epoch 204/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1779 - acc: 0.7320 - val_loss: 1.1957 - val_acc: 0.7220\n",
      "Epoch 205/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1757 - acc: 0.7337 - val_loss: 1.1935 - val_acc: 0.7260\n",
      "Epoch 206/1000\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 1.1740 - acc: 0.7332 - val_loss: 1.1925 - val_acc: 0.7240\n",
      "Epoch 207/1000\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 1.1722 - acc: 0.7329 - val_loss: 1.1891 - val_acc: 0.7250\n",
      "Epoch 208/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.1702 - acc: 0.7325 - val_loss: 1.1955 - val_acc: 0.7230\n",
      "Epoch 209/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1684 - acc: 0.7325 - val_loss: 1.1882 - val_acc: 0.7250\n",
      "Epoch 210/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.1668 - acc: 0.7318 - val_loss: 1.1892 - val_acc: 0.7150\n",
      "Epoch 211/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1655 - acc: 0.7331 - val_loss: 1.1817 - val_acc: 0.7260\n",
      "Epoch 212/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1631 - acc: 0.7340 - val_loss: 1.1853 - val_acc: 0.7180\n",
      "Epoch 213/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1620 - acc: 0.7326 - val_loss: 1.1835 - val_acc: 0.7230\n",
      "Epoch 214/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1608 - acc: 0.7334 - val_loss: 1.1760 - val_acc: 0.7240\n",
      "Epoch 215/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 1.1583 - acc: 0.7348 - val_loss: 1.1795 - val_acc: 0.7240\n",
      "Epoch 216/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.1567 - acc: 0.7329 - val_loss: 1.1769 - val_acc: 0.7230\n",
      "Epoch 217/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.1555 - acc: 0.7338 - val_loss: 1.1823 - val_acc: 0.7240\n",
      "Epoch 218/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.1543 - acc: 0.7320 - val_loss: 1.1709 - val_acc: 0.7260\n",
      "Epoch 219/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.1519 - acc: 0.7348 - val_loss: 1.1706 - val_acc: 0.7320\n",
      "Epoch 220/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1501 - acc: 0.7351 - val_loss: 1.1692 - val_acc: 0.7240\n",
      "Epoch 221/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1493 - acc: 0.7362 - val_loss: 1.1691 - val_acc: 0.7270\n",
      "Epoch 222/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1473 - acc: 0.7346 - val_loss: 1.1693 - val_acc: 0.7250\n",
      "Epoch 223/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.1459 - acc: 0.7354 - val_loss: 1.1658 - val_acc: 0.7250\n",
      "Epoch 224/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1440 - acc: 0.7357 - val_loss: 1.1636 - val_acc: 0.7290\n",
      "Epoch 225/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.1428 - acc: 0.7363 - val_loss: 1.1667 - val_acc: 0.7270\n",
      "Epoch 226/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.1418 - acc: 0.7348 - val_loss: 1.1601 - val_acc: 0.7270\n",
      "Epoch 227/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1406 - acc: 0.7360 - val_loss: 1.1588 - val_acc: 0.7290\n",
      "Epoch 228/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1394 - acc: 0.7363 - val_loss: 1.1658 - val_acc: 0.7300\n",
      "Epoch 229/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.1373 - acc: 0.7355 - val_loss: 1.1565 - val_acc: 0.7280\n",
      "Epoch 230/1000\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.1363 - acc: 0.7375 - val_loss: 1.1534 - val_acc: 0.7290\n",
      "Epoch 231/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.1341 - acc: 0.7372 - val_loss: 1.1548 - val_acc: 0.7290\n",
      "Epoch 232/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.1324 - acc: 0.7368 - val_loss: 1.1516 - val_acc: 0.7300\n",
      "Epoch 233/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.1322 - acc: 0.7352 - val_loss: 1.1486 - val_acc: 0.7300\n",
      "Epoch 234/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.1300 - acc: 0.7366 - val_loss: 1.1546 - val_acc: 0.7240\n",
      "Epoch 235/1000\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 1.1294 - acc: 0.7371 - val_loss: 1.1483 - val_acc: 0.7300\n",
      "Epoch 236/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.1273 - acc: 0.7380 - val_loss: 1.1486 - val_acc: 0.7280\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 30us/step - loss: 1.1260 - acc: 0.7380 - val_loss: 1.1428 - val_acc: 0.7310\n",
      "Epoch 238/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 1.1243 - acc: 0.7385 - val_loss: 1.1439 - val_acc: 0.7250\n",
      "Epoch 239/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.1233 - acc: 0.7378 - val_loss: 1.1413 - val_acc: 0.7230\n",
      "Epoch 240/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.1216 - acc: 0.7371 - val_loss: 1.1392 - val_acc: 0.7340\n",
      "Epoch 241/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1210 - acc: 0.7378 - val_loss: 1.1376 - val_acc: 0.7310\n",
      "Epoch 242/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.1188 - acc: 0.7375 - val_loss: 1.1403 - val_acc: 0.7270\n",
      "Epoch 243/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1184 - acc: 0.7366 - val_loss: 1.1391 - val_acc: 0.7250\n",
      "Epoch 244/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.1172 - acc: 0.7366 - val_loss: 1.1347 - val_acc: 0.7290\n",
      "Epoch 245/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1157 - acc: 0.7375 - val_loss: 1.1320 - val_acc: 0.7320\n",
      "Epoch 246/1000\n",
      "6500/6500 [==============================] - 0s 64us/step - loss: 1.1148 - acc: 0.7385 - val_loss: 1.1331 - val_acc: 0.7330\n",
      "Epoch 247/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.1140 - acc: 0.7382 - val_loss: 1.1337 - val_acc: 0.7370\n",
      "Epoch 248/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.1123 - acc: 0.7394 - val_loss: 1.1276 - val_acc: 0.7310\n",
      "Epoch 249/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1107 - acc: 0.7395 - val_loss: 1.1277 - val_acc: 0.7360\n",
      "Epoch 250/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.1097 - acc: 0.7405 - val_loss: 1.1268 - val_acc: 0.7320\n",
      "Epoch 251/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.1084 - acc: 0.7391 - val_loss: 1.1289 - val_acc: 0.7320\n",
      "Epoch 252/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.1071 - acc: 0.7398 - val_loss: 1.1229 - val_acc: 0.7330\n",
      "Epoch 253/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1062 - acc: 0.7402 - val_loss: 1.1229 - val_acc: 0.7320\n",
      "Epoch 254/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.1048 - acc: 0.7395 - val_loss: 1.1319 - val_acc: 0.7300\n",
      "Epoch 255/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1038 - acc: 0.7395 - val_loss: 1.1233 - val_acc: 0.7280\n",
      "Epoch 256/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.1024 - acc: 0.7380 - val_loss: 1.1188 - val_acc: 0.7360\n",
      "Epoch 257/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.1011 - acc: 0.7402 - val_loss: 1.1202 - val_acc: 0.7280\n",
      "Epoch 258/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.1001 - acc: 0.7395 - val_loss: 1.1199 - val_acc: 0.7360\n",
      "Epoch 259/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.0988 - acc: 0.7394 - val_loss: 1.1219 - val_acc: 0.7300\n",
      "Epoch 260/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.0976 - acc: 0.7420 - val_loss: 1.1170 - val_acc: 0.7330\n",
      "Epoch 261/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0967 - acc: 0.7405 - val_loss: 1.1187 - val_acc: 0.7310\n",
      "Epoch 262/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 1.0958 - acc: 0.7409 - val_loss: 1.1125 - val_acc: 0.7340\n",
      "Epoch 263/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0955 - acc: 0.7400 - val_loss: 1.1194 - val_acc: 0.7330\n",
      "Epoch 264/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0946 - acc: 0.7405 - val_loss: 1.1150 - val_acc: 0.7340\n",
      "Epoch 265/1000\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.0925 - acc: 0.7409 - val_loss: 1.1121 - val_acc: 0.7340\n",
      "Epoch 266/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 1.0916 - acc: 0.7397 - val_loss: 1.1121 - val_acc: 0.7400\n",
      "Epoch 267/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0908 - acc: 0.7411 - val_loss: 1.1115 - val_acc: 0.7370\n",
      "Epoch 268/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0890 - acc: 0.7417 - val_loss: 1.1076 - val_acc: 0.7290\n",
      "Epoch 269/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.0882 - acc: 0.7414 - val_loss: 1.1070 - val_acc: 0.7300\n",
      "Epoch 270/1000\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 1.0871 - acc: 0.7417 - val_loss: 1.1071 - val_acc: 0.7360\n",
      "Epoch 271/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0860 - acc: 0.7425 - val_loss: 1.1056 - val_acc: 0.7350\n",
      "Epoch 272/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0849 - acc: 0.7415 - val_loss: 1.1070 - val_acc: 0.7250\n",
      "Epoch 273/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0837 - acc: 0.7400 - val_loss: 1.1044 - val_acc: 0.7260\n",
      "Epoch 274/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0829 - acc: 0.7432 - val_loss: 1.1039 - val_acc: 0.7290\n",
      "Epoch 275/1000\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 1.0817 - acc: 0.7435 - val_loss: 1.1059 - val_acc: 0.7360\n",
      "Epoch 276/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.0804 - acc: 0.7422 - val_loss: 1.0978 - val_acc: 0.7360\n",
      "Epoch 277/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0800 - acc: 0.7437 - val_loss: 1.0982 - val_acc: 0.7400\n",
      "Epoch 278/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0781 - acc: 0.7432 - val_loss: 1.0979 - val_acc: 0.7340\n",
      "Epoch 279/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0777 - acc: 0.7426 - val_loss: 1.1043 - val_acc: 0.7260\n",
      "Epoch 280/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0771 - acc: 0.7429 - val_loss: 1.0953 - val_acc: 0.7400\n",
      "Epoch 281/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.0767 - acc: 0.7406 - val_loss: 1.0955 - val_acc: 0.7350\n",
      "Epoch 282/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0745 - acc: 0.7452 - val_loss: 1.0943 - val_acc: 0.7350\n",
      "Epoch 283/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0733 - acc: 0.7435 - val_loss: 1.0910 - val_acc: 0.7330\n",
      "Epoch 284/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0730 - acc: 0.7429 - val_loss: 1.0905 - val_acc: 0.7370\n",
      "Epoch 285/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 1.0719 - acc: 0.7443 - val_loss: 1.0902 - val_acc: 0.7360\n",
      "Epoch 286/1000\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 1.0708 - acc: 0.7437 - val_loss: 1.0890 - val_acc: 0.7390\n",
      "Epoch 287/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.0705 - acc: 0.7458 - val_loss: 1.0930 - val_acc: 0.7360\n",
      "Epoch 288/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0691 - acc: 0.7425 - val_loss: 1.0867 - val_acc: 0.7370\n",
      "Epoch 289/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0683 - acc: 0.7437 - val_loss: 1.0876 - val_acc: 0.7400\n",
      "Epoch 290/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0674 - acc: 0.7454 - val_loss: 1.0864 - val_acc: 0.7350\n",
      "Epoch 291/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0663 - acc: 0.7451 - val_loss: 1.0926 - val_acc: 0.7320\n",
      "Epoch 292/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0659 - acc: 0.7451 - val_loss: 1.0836 - val_acc: 0.7380\n",
      "Epoch 293/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0644 - acc: 0.7446 - val_loss: 1.0815 - val_acc: 0.7410\n",
      "Epoch 294/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0633 - acc: 0.7460 - val_loss: 1.0830 - val_acc: 0.7360\n",
      "Epoch 295/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0622 - acc: 0.7449 - val_loss: 1.0810 - val_acc: 0.7380\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.0612 - acc: 0.7469 - val_loss: 1.0802 - val_acc: 0.7380\n",
      "Epoch 297/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0597 - acc: 0.7469 - val_loss: 1.0808 - val_acc: 0.7430\n",
      "Epoch 298/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.0593 - acc: 0.7463 - val_loss: 1.0764 - val_acc: 0.7370\n",
      "Epoch 299/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0582 - acc: 0.7475 - val_loss: 1.0803 - val_acc: 0.7250\n",
      "Epoch 300/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.0580 - acc: 0.7449 - val_loss: 1.0753 - val_acc: 0.7400\n",
      "Epoch 301/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0567 - acc: 0.7466 - val_loss: 1.0803 - val_acc: 0.7410\n",
      "Epoch 302/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.0563 - acc: 0.7457 - val_loss: 1.0864 - val_acc: 0.7320\n",
      "Epoch 303/1000\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 1.0555 - acc: 0.7469 - val_loss: 1.0753 - val_acc: 0.7360\n",
      "Epoch 304/1000\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.0547 - acc: 0.7477 - val_loss: 1.0777 - val_acc: 0.7380\n",
      "Epoch 305/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.0530 - acc: 0.7480 - val_loss: 1.0752 - val_acc: 0.7380\n",
      "Epoch 306/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.0524 - acc: 0.7480 - val_loss: 1.0745 - val_acc: 0.7420\n",
      "Epoch 307/1000\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 1.0512 - acc: 0.7469 - val_loss: 1.0725 - val_acc: 0.7380\n",
      "Epoch 308/1000\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 1.0499 - acc: 0.7462 - val_loss: 1.0694 - val_acc: 0.7430\n",
      "Epoch 309/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.0495 - acc: 0.7483 - val_loss: 1.0702 - val_acc: 0.7370\n",
      "Epoch 310/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0483 - acc: 0.7475 - val_loss: 1.0689 - val_acc: 0.7400\n",
      "Epoch 311/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0476 - acc: 0.7485 - val_loss: 1.0653 - val_acc: 0.7370\n",
      "Epoch 312/1000\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 1.0460 - acc: 0.7483 - val_loss: 1.0671 - val_acc: 0.7410\n",
      "Epoch 313/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0456 - acc: 0.7485 - val_loss: 1.0722 - val_acc: 0.7400\n",
      "Epoch 314/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0454 - acc: 0.7474 - val_loss: 1.0635 - val_acc: 0.7400\n",
      "Epoch 315/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0444 - acc: 0.7468 - val_loss: 1.0678 - val_acc: 0.7290\n",
      "Epoch 316/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0437 - acc: 0.7474 - val_loss: 1.0666 - val_acc: 0.7420\n",
      "Epoch 317/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0420 - acc: 0.7488 - val_loss: 1.0605 - val_acc: 0.7390\n",
      "Epoch 318/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0411 - acc: 0.7494 - val_loss: 1.0638 - val_acc: 0.7330\n",
      "Epoch 319/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0410 - acc: 0.7494 - val_loss: 1.0736 - val_acc: 0.7300\n",
      "Epoch 320/1000\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 1.0404 - acc: 0.7491 - val_loss: 1.0659 - val_acc: 0.7460\n",
      "Epoch 321/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.0391 - acc: 0.7505 - val_loss: 1.0632 - val_acc: 0.7430\n",
      "Epoch 322/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 1.0388 - acc: 0.7502 - val_loss: 1.0708 - val_acc: 0.7260\n",
      "Epoch 323/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0372 - acc: 0.7508 - val_loss: 1.0612 - val_acc: 0.7340\n",
      "Epoch 324/1000\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 1.0362 - acc: 0.7512 - val_loss: 1.0559 - val_acc: 0.7430\n",
      "Epoch 325/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.0356 - acc: 0.7512 - val_loss: 1.0572 - val_acc: 0.7350\n",
      "Epoch 326/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.0344 - acc: 0.7508 - val_loss: 1.0555 - val_acc: 0.7410\n",
      "Epoch 327/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0338 - acc: 0.7498 - val_loss: 1.0533 - val_acc: 0.7430\n",
      "Epoch 328/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 1.0332 - acc: 0.7508 - val_loss: 1.0542 - val_acc: 0.7400\n",
      "Epoch 329/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.0319 - acc: 0.7525 - val_loss: 1.0600 - val_acc: 0.7340\n",
      "Epoch 330/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0315 - acc: 0.7537 - val_loss: 1.0501 - val_acc: 0.7380\n",
      "Epoch 331/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0306 - acc: 0.7518 - val_loss: 1.0517 - val_acc: 0.7420\n",
      "Epoch 332/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0292 - acc: 0.7495 - val_loss: 1.0563 - val_acc: 0.7410\n",
      "Epoch 333/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0289 - acc: 0.7503 - val_loss: 1.0509 - val_acc: 0.7440\n",
      "Epoch 334/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0273 - acc: 0.7525 - val_loss: 1.0507 - val_acc: 0.7390\n",
      "Epoch 335/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0270 - acc: 0.7526 - val_loss: 1.0466 - val_acc: 0.7420\n",
      "Epoch 336/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0262 - acc: 0.7528 - val_loss: 1.0494 - val_acc: 0.7390\n",
      "Epoch 337/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0263 - acc: 0.7520 - val_loss: 1.0520 - val_acc: 0.7390\n",
      "Epoch 338/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0249 - acc: 0.7528 - val_loss: 1.0473 - val_acc: 0.7460\n",
      "Epoch 339/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0239 - acc: 0.7529 - val_loss: 1.0541 - val_acc: 0.7390\n",
      "Epoch 340/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0233 - acc: 0.7529 - val_loss: 1.0474 - val_acc: 0.7400\n",
      "Epoch 341/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0227 - acc: 0.7534 - val_loss: 1.0424 - val_acc: 0.7410\n",
      "Epoch 342/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0214 - acc: 0.7543 - val_loss: 1.0472 - val_acc: 0.7390\n",
      "Epoch 343/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0205 - acc: 0.7531 - val_loss: 1.0427 - val_acc: 0.7440\n",
      "Epoch 344/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.0199 - acc: 0.7546 - val_loss: 1.0431 - val_acc: 0.7400\n",
      "Epoch 345/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 1.0184 - acc: 0.7538 - val_loss: 1.0403 - val_acc: 0.7450\n",
      "Epoch 346/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0178 - acc: 0.7549 - val_loss: 1.0388 - val_acc: 0.7470\n",
      "Epoch 347/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0172 - acc: 0.7545 - val_loss: 1.0441 - val_acc: 0.7420\n",
      "Epoch 348/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 1.0164 - acc: 0.7545 - val_loss: 1.0400 - val_acc: 0.7360\n",
      "Epoch 349/1000\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.0168 - acc: 0.7551 - val_loss: 1.0411 - val_acc: 0.7410\n",
      "Epoch 350/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0151 - acc: 0.7540 - val_loss: 1.0378 - val_acc: 0.7470\n",
      "Epoch 351/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0146 - acc: 0.7560 - val_loss: 1.0387 - val_acc: 0.7430\n",
      "Epoch 352/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0137 - acc: 0.7557 - val_loss: 1.0353 - val_acc: 0.7470\n",
      "Epoch 353/1000\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.0131 - acc: 0.7551 - val_loss: 1.0346 - val_acc: 0.7430\n",
      "Epoch 354/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 1.0123 - acc: 0.7545 - val_loss: 1.0391 - val_acc: 0.7450\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 31us/step - loss: 1.0112 - acc: 0.7534 - val_loss: 1.0344 - val_acc: 0.7480\n",
      "Epoch 356/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 1.0108 - acc: 0.7557 - val_loss: 1.0325 - val_acc: 0.7460\n",
      "Epoch 357/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0099 - acc: 0.7560 - val_loss: 1.0336 - val_acc: 0.7430\n",
      "Epoch 358/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 1.0091 - acc: 0.7560 - val_loss: 1.0337 - val_acc: 0.7460\n",
      "Epoch 359/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 1.0082 - acc: 0.7545 - val_loss: 1.0317 - val_acc: 0.7480\n",
      "Epoch 360/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 1.0079 - acc: 0.7551 - val_loss: 1.0313 - val_acc: 0.7460\n",
      "Epoch 361/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0072 - acc: 0.7578 - val_loss: 1.0301 - val_acc: 0.7430\n",
      "Epoch 362/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0058 - acc: 0.7571 - val_loss: 1.0283 - val_acc: 0.7470\n",
      "Epoch 363/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 1.0049 - acc: 0.7572 - val_loss: 1.0287 - val_acc: 0.7460\n",
      "Epoch 364/1000\n",
      "6500/6500 [==============================] - 0s 74us/step - loss: 1.0046 - acc: 0.7563 - val_loss: 1.0330 - val_acc: 0.7480\n",
      "Epoch 365/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0045 - acc: 0.7566 - val_loss: 1.0279 - val_acc: 0.7470\n",
      "Epoch 366/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 1.0033 - acc: 0.7577 - val_loss: 1.0310 - val_acc: 0.7500\n",
      "Epoch 367/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.0034 - acc: 0.7562 - val_loss: 1.0250 - val_acc: 0.7490\n",
      "Epoch 368/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 1.0014 - acc: 0.7572 - val_loss: 1.0273 - val_acc: 0.7500\n",
      "Epoch 369/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 1.0004 - acc: 0.7583 - val_loss: 1.0263 - val_acc: 0.7440\n",
      "Epoch 370/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 1.0005 - acc: 0.7558 - val_loss: 1.0235 - val_acc: 0.7490\n",
      "Epoch 371/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9996 - acc: 0.7588 - val_loss: 1.0239 - val_acc: 0.7470\n",
      "Epoch 372/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9983 - acc: 0.7555 - val_loss: 1.0247 - val_acc: 0.7460\n",
      "Epoch 373/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9986 - acc: 0.7578 - val_loss: 1.0240 - val_acc: 0.7470\n",
      "Epoch 374/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9974 - acc: 0.7583 - val_loss: 1.0213 - val_acc: 0.7480\n",
      "Epoch 375/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.9963 - acc: 0.7594 - val_loss: 1.0305 - val_acc: 0.7420\n",
      "Epoch 376/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9973 - acc: 0.7583 - val_loss: 1.0248 - val_acc: 0.7440\n",
      "Epoch 377/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9954 - acc: 0.7580 - val_loss: 1.0209 - val_acc: 0.7490\n",
      "Epoch 378/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9941 - acc: 0.7583 - val_loss: 1.0248 - val_acc: 0.7400\n",
      "Epoch 379/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.9940 - acc: 0.7597 - val_loss: 1.0188 - val_acc: 0.7460\n",
      "Epoch 380/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9936 - acc: 0.7614 - val_loss: 1.0176 - val_acc: 0.7480\n",
      "Epoch 381/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 0.9926 - acc: 0.7603 - val_loss: 1.0164 - val_acc: 0.7500\n",
      "Epoch 382/1000\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.9917 - acc: 0.7612 - val_loss: 1.0148 - val_acc: 0.7480\n",
      "Epoch 383/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9911 - acc: 0.7600 - val_loss: 1.0173 - val_acc: 0.7480\n",
      "Epoch 384/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9906 - acc: 0.7602 - val_loss: 1.0187 - val_acc: 0.7430\n",
      "Epoch 385/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9899 - acc: 0.7606 - val_loss: 1.0180 - val_acc: 0.7450\n",
      "Epoch 386/1000\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.9891 - acc: 0.7594 - val_loss: 1.0171 - val_acc: 0.7450\n",
      "Epoch 387/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9892 - acc: 0.7603 - val_loss: 1.0183 - val_acc: 0.7450\n",
      "Epoch 388/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9877 - acc: 0.7600 - val_loss: 1.0236 - val_acc: 0.7350\n",
      "Epoch 389/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9873 - acc: 0.7583 - val_loss: 1.0143 - val_acc: 0.7490\n",
      "Epoch 390/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.9862 - acc: 0.7594 - val_loss: 1.0160 - val_acc: 0.7520\n",
      "Epoch 391/1000\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.9855 - acc: 0.7602 - val_loss: 1.0190 - val_acc: 0.7430\n",
      "Epoch 392/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9853 - acc: 0.7605 - val_loss: 1.0122 - val_acc: 0.7470\n",
      "Epoch 393/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9851 - acc: 0.7609 - val_loss: 1.0101 - val_acc: 0.7440\n",
      "Epoch 394/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9837 - acc: 0.7614 - val_loss: 1.0130 - val_acc: 0.7510\n",
      "Epoch 395/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9832 - acc: 0.7598 - val_loss: 1.0084 - val_acc: 0.7490\n",
      "Epoch 396/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9817 - acc: 0.7638 - val_loss: 1.0082 - val_acc: 0.7480\n",
      "Epoch 397/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9815 - acc: 0.7625 - val_loss: 1.0131 - val_acc: 0.7420\n",
      "Epoch 398/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9806 - acc: 0.7638 - val_loss: 1.0060 - val_acc: 0.7480\n",
      "Epoch 399/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9800 - acc: 0.7622 - val_loss: 1.0079 - val_acc: 0.7480\n",
      "Epoch 400/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9799 - acc: 0.7622 - val_loss: 1.0079 - val_acc: 0.7490\n",
      "Epoch 401/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9793 - acc: 0.7623 - val_loss: 1.0073 - val_acc: 0.7510\n",
      "Epoch 402/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9786 - acc: 0.7605 - val_loss: 1.0079 - val_acc: 0.7460\n",
      "Epoch 403/1000\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 0.9772 - acc: 0.7612 - val_loss: 1.0052 - val_acc: 0.7480\n",
      "Epoch 404/1000\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.9769 - acc: 0.7623 - val_loss: 1.0122 - val_acc: 0.7440\n",
      "Epoch 405/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9760 - acc: 0.7628 - val_loss: 1.0068 - val_acc: 0.7470\n",
      "Epoch 406/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9760 - acc: 0.7632 - val_loss: 1.0040 - val_acc: 0.7520\n",
      "Epoch 407/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.9743 - acc: 0.7623 - val_loss: 1.0029 - val_acc: 0.7500\n",
      "Epoch 408/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9743 - acc: 0.7623 - val_loss: 1.0017 - val_acc: 0.7550\n",
      "Epoch 409/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9733 - acc: 0.7625 - val_loss: 1.0045 - val_acc: 0.7450\n",
      "Epoch 410/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9741 - acc: 0.7614 - val_loss: 1.0034 - val_acc: 0.7470\n",
      "Epoch 411/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9718 - acc: 0.7634 - val_loss: 1.0021 - val_acc: 0.7490\n",
      "Epoch 412/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.9725 - acc: 0.7629 - val_loss: 1.0014 - val_acc: 0.7460\n",
      "Epoch 413/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.9713 - acc: 0.7628 - val_loss: 1.0023 - val_acc: 0.7530\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9707 - acc: 0.7655 - val_loss: 0.9998 - val_acc: 0.7490\n",
      "Epoch 415/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9695 - acc: 0.7634 - val_loss: 0.9980 - val_acc: 0.7470\n",
      "Epoch 416/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.9697 - acc: 0.7637 - val_loss: 0.9991 - val_acc: 0.7490\n",
      "Epoch 417/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9693 - acc: 0.7618 - val_loss: 1.0021 - val_acc: 0.7490\n",
      "Epoch 418/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9685 - acc: 0.7629 - val_loss: 1.0027 - val_acc: 0.7440\n",
      "Epoch 419/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9670 - acc: 0.7640 - val_loss: 1.0001 - val_acc: 0.7460\n",
      "Epoch 420/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9670 - acc: 0.7651 - val_loss: 0.9994 - val_acc: 0.7450\n",
      "Epoch 421/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9666 - acc: 0.7657 - val_loss: 1.0054 - val_acc: 0.7480\n",
      "Epoch 422/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 0.9662 - acc: 0.7648 - val_loss: 0.9982 - val_acc: 0.7510\n",
      "Epoch 423/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9653 - acc: 0.7657 - val_loss: 1.0064 - val_acc: 0.7400\n",
      "Epoch 424/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9650 - acc: 0.7683 - val_loss: 1.0046 - val_acc: 0.7390\n",
      "Epoch 425/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9643 - acc: 0.7637 - val_loss: 0.9929 - val_acc: 0.7520\n",
      "Epoch 426/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.9640 - acc: 0.7657 - val_loss: 0.9959 - val_acc: 0.7460\n",
      "Epoch 427/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 0.9623 - acc: 0.7665 - val_loss: 0.9974 - val_acc: 0.7520\n",
      "Epoch 428/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9629 - acc: 0.7654 - val_loss: 0.9918 - val_acc: 0.7500\n",
      "Epoch 429/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9616 - acc: 0.7675 - val_loss: 0.9925 - val_acc: 0.7470\n",
      "Epoch 430/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9608 - acc: 0.7658 - val_loss: 0.9956 - val_acc: 0.7460\n",
      "Epoch 431/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.9611 - acc: 0.7643 - val_loss: 0.9953 - val_acc: 0.7550\n",
      "Epoch 432/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9601 - acc: 0.7655 - val_loss: 0.9928 - val_acc: 0.7520\n",
      "Epoch 433/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9600 - acc: 0.7671 - val_loss: 1.0020 - val_acc: 0.7450\n",
      "Epoch 434/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9591 - acc: 0.7682 - val_loss: 0.9881 - val_acc: 0.7500\n",
      "Epoch 435/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9587 - acc: 0.7646 - val_loss: 0.9938 - val_acc: 0.7500\n",
      "Epoch 436/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9583 - acc: 0.7669 - val_loss: 0.9910 - val_acc: 0.7470\n",
      "Epoch 437/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9571 - acc: 0.7677 - val_loss: 0.9915 - val_acc: 0.7450\n",
      "Epoch 438/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9573 - acc: 0.7660 - val_loss: 0.9876 - val_acc: 0.7510\n",
      "Epoch 439/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9558 - acc: 0.7649 - val_loss: 0.9905 - val_acc: 0.7500\n",
      "Epoch 440/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9550 - acc: 0.7655 - val_loss: 0.9960 - val_acc: 0.7400\n",
      "Epoch 441/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9552 - acc: 0.7678 - val_loss: 0.9946 - val_acc: 0.7440\n",
      "Epoch 442/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.9541 - acc: 0.7695 - val_loss: 0.9872 - val_acc: 0.7480\n",
      "Epoch 443/1000\n",
      "6500/6500 [==============================] - 0s 70us/step - loss: 0.9537 - acc: 0.7674 - val_loss: 0.9938 - val_acc: 0.7460\n",
      "Epoch 444/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9532 - acc: 0.7680 - val_loss: 0.9874 - val_acc: 0.7510\n",
      "Epoch 445/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.9526 - acc: 0.7668 - val_loss: 0.9879 - val_acc: 0.7490\n",
      "Epoch 446/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9521 - acc: 0.7688 - val_loss: 0.9845 - val_acc: 0.7500\n",
      "Epoch 447/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9516 - acc: 0.7677 - val_loss: 0.9940 - val_acc: 0.7520\n",
      "Epoch 448/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9526 - acc: 0.7683 - val_loss: 0.9867 - val_acc: 0.7470\n",
      "Epoch 449/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9503 - acc: 0.7688 - val_loss: 0.9836 - val_acc: 0.7490\n",
      "Epoch 450/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9498 - acc: 0.7680 - val_loss: 0.9809 - val_acc: 0.7480\n",
      "Epoch 451/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9493 - acc: 0.7671 - val_loss: 0.9844 - val_acc: 0.7470\n",
      "Epoch 452/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9493 - acc: 0.7683 - val_loss: 0.9840 - val_acc: 0.7480\n",
      "Epoch 453/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9477 - acc: 0.7675 - val_loss: 0.9830 - val_acc: 0.7490\n",
      "Epoch 454/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9477 - acc: 0.7675 - val_loss: 0.9857 - val_acc: 0.7510\n",
      "Epoch 455/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9474 - acc: 0.7675 - val_loss: 0.9846 - val_acc: 0.7500\n",
      "Epoch 456/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9472 - acc: 0.7678 - val_loss: 0.9809 - val_acc: 0.7520\n",
      "Epoch 457/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9461 - acc: 0.7686 - val_loss: 0.9785 - val_acc: 0.7540\n",
      "Epoch 458/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9454 - acc: 0.7688 - val_loss: 0.9861 - val_acc: 0.7500\n",
      "Epoch 459/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9451 - acc: 0.7702 - val_loss: 0.9866 - val_acc: 0.7450\n",
      "Epoch 460/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9451 - acc: 0.7691 - val_loss: 0.9782 - val_acc: 0.7510\n",
      "Epoch 461/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9435 - acc: 0.7703 - val_loss: 0.9794 - val_acc: 0.7480\n",
      "Epoch 462/1000\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.9428 - acc: 0.7708 - val_loss: 0.9791 - val_acc: 0.7490\n",
      "Epoch 463/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9427 - acc: 0.7711 - val_loss: 0.9755 - val_acc: 0.7580\n",
      "Epoch 464/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9417 - acc: 0.7698 - val_loss: 0.9779 - val_acc: 0.7520\n",
      "Epoch 465/1000\n",
      "6500/6500 [==============================] - 0s 28us/step - loss: 0.9427 - acc: 0.7700 - val_loss: 0.9780 - val_acc: 0.7530\n",
      "Epoch 466/1000\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.9413 - acc: 0.7703 - val_loss: 0.9789 - val_acc: 0.7470\n",
      "Epoch 467/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.9411 - acc: 0.7697 - val_loss: 0.9775 - val_acc: 0.7460\n",
      "Epoch 468/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9400 - acc: 0.7700 - val_loss: 0.9785 - val_acc: 0.7470\n",
      "Epoch 469/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9399 - acc: 0.7712 - val_loss: 0.9777 - val_acc: 0.7470\n",
      "Epoch 470/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9383 - acc: 0.7723 - val_loss: 0.9757 - val_acc: 0.7510\n",
      "Epoch 471/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.9383 - acc: 0.7700 - val_loss: 0.9812 - val_acc: 0.7450\n",
      "Epoch 472/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9382 - acc: 0.7712 - val_loss: 0.9829 - val_acc: 0.7430\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9378 - acc: 0.7702 - val_loss: 0.9790 - val_acc: 0.7520\n",
      "Epoch 474/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9376 - acc: 0.7702 - val_loss: 0.9723 - val_acc: 0.7560\n",
      "Epoch 475/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9370 - acc: 0.7712 - val_loss: 0.9917 - val_acc: 0.7270\n",
      "Epoch 476/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9376 - acc: 0.7698 - val_loss: 0.9758 - val_acc: 0.7530\n",
      "Epoch 477/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9355 - acc: 0.7718 - val_loss: 0.9733 - val_acc: 0.7550\n",
      "Epoch 478/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9351 - acc: 0.7728 - val_loss: 0.9701 - val_acc: 0.7570\n",
      "Epoch 479/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9345 - acc: 0.7720 - val_loss: 0.9798 - val_acc: 0.7460\n",
      "Epoch 480/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9340 - acc: 0.7708 - val_loss: 0.9733 - val_acc: 0.7470\n",
      "Epoch 481/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9336 - acc: 0.7714 - val_loss: 0.9728 - val_acc: 0.7520\n",
      "Epoch 482/1000\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.9329 - acc: 0.7714 - val_loss: 0.9724 - val_acc: 0.7530\n",
      "Epoch 483/1000\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.9323 - acc: 0.7705 - val_loss: 0.9737 - val_acc: 0.7540\n",
      "Epoch 484/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.9324 - acc: 0.7714 - val_loss: 0.9678 - val_acc: 0.7530\n",
      "Epoch 485/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9319 - acc: 0.7734 - val_loss: 0.9687 - val_acc: 0.7530\n",
      "Epoch 486/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9316 - acc: 0.7735 - val_loss: 0.9740 - val_acc: 0.7530\n",
      "Epoch 487/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9314 - acc: 0.7729 - val_loss: 0.9724 - val_acc: 0.7500\n",
      "Epoch 488/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9310 - acc: 0.7722 - val_loss: 0.9758 - val_acc: 0.7460\n",
      "Epoch 489/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9304 - acc: 0.7737 - val_loss: 0.9698 - val_acc: 0.7500\n",
      "Epoch 490/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9294 - acc: 0.7705 - val_loss: 0.9704 - val_acc: 0.7500\n",
      "Epoch 491/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9290 - acc: 0.7702 - val_loss: 0.9714 - val_acc: 0.7500\n",
      "Epoch 492/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9282 - acc: 0.7734 - val_loss: 0.9696 - val_acc: 0.7490\n",
      "Epoch 493/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9285 - acc: 0.7742 - val_loss: 0.9685 - val_acc: 0.7510\n",
      "Epoch 494/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9282 - acc: 0.7740 - val_loss: 0.9661 - val_acc: 0.7520\n",
      "Epoch 495/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9263 - acc: 0.7738 - val_loss: 0.9712 - val_acc: 0.7470\n",
      "Epoch 496/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9266 - acc: 0.7743 - val_loss: 0.9680 - val_acc: 0.7540\n",
      "Epoch 497/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9257 - acc: 0.7749 - val_loss: 0.9699 - val_acc: 0.7490\n",
      "Epoch 498/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9260 - acc: 0.7748 - val_loss: 0.9674 - val_acc: 0.7540\n",
      "Epoch 499/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9256 - acc: 0.7740 - val_loss: 0.9692 - val_acc: 0.7500\n",
      "Epoch 500/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9254 - acc: 0.7743 - val_loss: 0.9676 - val_acc: 0.7550\n",
      "Epoch 501/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9241 - acc: 0.7720 - val_loss: 0.9643 - val_acc: 0.7510\n",
      "Epoch 502/1000\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.9237 - acc: 0.7728 - val_loss: 0.9644 - val_acc: 0.7540\n",
      "Epoch 503/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9237 - acc: 0.7738 - val_loss: 0.9624 - val_acc: 0.7560\n",
      "Epoch 504/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9230 - acc: 0.7746 - val_loss: 0.9649 - val_acc: 0.7560\n",
      "Epoch 505/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9234 - acc: 0.7717 - val_loss: 0.9672 - val_acc: 0.7480\n",
      "Epoch 506/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9217 - acc: 0.7760 - val_loss: 0.9711 - val_acc: 0.7430\n",
      "Epoch 507/1000\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.9220 - acc: 0.7752 - val_loss: 0.9624 - val_acc: 0.7530\n",
      "Epoch 508/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.9210 - acc: 0.7766 - val_loss: 0.9691 - val_acc: 0.7490\n",
      "Epoch 509/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9210 - acc: 0.7762 - val_loss: 0.9656 - val_acc: 0.7490\n",
      "Epoch 510/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9208 - acc: 0.7755 - val_loss: 0.9635 - val_acc: 0.7500\n",
      "Epoch 511/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.9201 - acc: 0.7758 - val_loss: 0.9645 - val_acc: 0.7560\n",
      "Epoch 512/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9189 - acc: 0.7745 - val_loss: 0.9695 - val_acc: 0.7460\n",
      "Epoch 513/1000\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.9196 - acc: 0.7737 - val_loss: 0.9619 - val_acc: 0.7560\n",
      "Epoch 514/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9186 - acc: 0.7734 - val_loss: 0.9639 - val_acc: 0.7480\n",
      "Epoch 515/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9177 - acc: 0.7765 - val_loss: 0.9581 - val_acc: 0.7560\n",
      "Epoch 516/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9178 - acc: 0.7763 - val_loss: 0.9583 - val_acc: 0.7510\n",
      "Epoch 517/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9179 - acc: 0.7743 - val_loss: 0.9659 - val_acc: 0.7590\n",
      "Epoch 518/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9174 - acc: 0.7762 - val_loss: 0.9611 - val_acc: 0.7530\n",
      "Epoch 519/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9160 - acc: 0.7751 - val_loss: 0.9565 - val_acc: 0.7560\n",
      "Epoch 520/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9163 - acc: 0.7729 - val_loss: 0.9591 - val_acc: 0.7550\n",
      "Epoch 521/1000\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.9152 - acc: 0.7746 - val_loss: 0.9576 - val_acc: 0.7570\n",
      "Epoch 522/1000\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.9145 - acc: 0.7780 - val_loss: 0.9561 - val_acc: 0.7610\n",
      "Epoch 523/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9137 - acc: 0.7778 - val_loss: 0.9551 - val_acc: 0.7560\n",
      "Epoch 524/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9146 - acc: 0.7768 - val_loss: 0.9555 - val_acc: 0.7580\n",
      "Epoch 525/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9131 - acc: 0.7765 - val_loss: 0.9601 - val_acc: 0.7550\n",
      "Epoch 526/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9134 - acc: 0.7762 - val_loss: 0.9574 - val_acc: 0.7590\n",
      "Epoch 527/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9124 - acc: 0.7762 - val_loss: 0.9660 - val_acc: 0.7480\n",
      "Epoch 528/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9127 - acc: 0.7754 - val_loss: 0.9522 - val_acc: 0.7580\n",
      "Epoch 529/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9113 - acc: 0.7758 - val_loss: 0.9608 - val_acc: 0.7520\n",
      "Epoch 530/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9115 - acc: 0.7775 - val_loss: 0.9601 - val_acc: 0.7480\n",
      "Epoch 531/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.9113 - acc: 0.7771 - val_loss: 0.9520 - val_acc: 0.7650\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9101 - acc: 0.7775 - val_loss: 0.9539 - val_acc: 0.7570\n",
      "Epoch 533/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9113 - acc: 0.7775 - val_loss: 0.9608 - val_acc: 0.7550\n",
      "Epoch 534/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9101 - acc: 0.7782 - val_loss: 0.9521 - val_acc: 0.7570\n",
      "Epoch 535/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9088 - acc: 0.7772 - val_loss: 0.9535 - val_acc: 0.7540\n",
      "Epoch 536/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9093 - acc: 0.7774 - val_loss: 0.9533 - val_acc: 0.7600\n",
      "Epoch 537/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9090 - acc: 0.7768 - val_loss: 0.9542 - val_acc: 0.7560\n",
      "Epoch 538/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9088 - acc: 0.7765 - val_loss: 0.9533 - val_acc: 0.7570\n",
      "Epoch 539/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9075 - acc: 0.7792 - val_loss: 0.9556 - val_acc: 0.7520\n",
      "Epoch 540/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.9076 - acc: 0.7783 - val_loss: 0.9527 - val_acc: 0.7590\n",
      "Epoch 541/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9067 - acc: 0.7789 - val_loss: 0.9599 - val_acc: 0.7460\n",
      "Epoch 542/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9065 - acc: 0.7775 - val_loss: 0.9486 - val_acc: 0.7570\n",
      "Epoch 543/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.9065 - acc: 0.7797 - val_loss: 0.9563 - val_acc: 0.7510\n",
      "Epoch 544/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9063 - acc: 0.7786 - val_loss: 0.9546 - val_acc: 0.7490\n",
      "Epoch 545/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9051 - acc: 0.7792 - val_loss: 0.9522 - val_acc: 0.7560\n",
      "Epoch 546/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9057 - acc: 0.7791 - val_loss: 0.9612 - val_acc: 0.7520\n",
      "Epoch 547/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9041 - acc: 0.7775 - val_loss: 0.9460 - val_acc: 0.7580\n",
      "Epoch 548/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9037 - acc: 0.7798 - val_loss: 0.9504 - val_acc: 0.7510\n",
      "Epoch 549/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.9037 - acc: 0.7802 - val_loss: 0.9491 - val_acc: 0.7550\n",
      "Epoch 550/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9032 - acc: 0.7808 - val_loss: 0.9482 - val_acc: 0.7580\n",
      "Epoch 551/1000\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.9034 - acc: 0.7783 - val_loss: 0.9480 - val_acc: 0.7610\n",
      "Epoch 552/1000\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.9030 - acc: 0.7803 - val_loss: 0.9472 - val_acc: 0.7590\n",
      "Epoch 553/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.9017 - acc: 0.7809 - val_loss: 0.9573 - val_acc: 0.7520\n",
      "Epoch 554/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.9017 - acc: 0.7811 - val_loss: 0.9549 - val_acc: 0.7520\n",
      "Epoch 555/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9013 - acc: 0.7800 - val_loss: 0.9533 - val_acc: 0.7520\n",
      "Epoch 556/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9025 - acc: 0.7808 - val_loss: 0.9541 - val_acc: 0.7490\n",
      "Epoch 557/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.9006 - acc: 0.7811 - val_loss: 0.9569 - val_acc: 0.7550\n",
      "Epoch 558/1000\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.9003 - acc: 0.7826 - val_loss: 0.9482 - val_acc: 0.7580\n",
      "Epoch 559/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.9006 - acc: 0.7811 - val_loss: 0.9503 - val_acc: 0.7580\n",
      "Epoch 560/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.9003 - acc: 0.7809 - val_loss: 0.9508 - val_acc: 0.7520\n",
      "Epoch 561/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 0.8997 - acc: 0.7808 - val_loss: 0.9594 - val_acc: 0.7500\n",
      "Epoch 562/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.9001 - acc: 0.7800 - val_loss: 0.9520 - val_acc: 0.7640\n",
      "Epoch 563/1000\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.8989 - acc: 0.7785 - val_loss: 0.9499 - val_acc: 0.7520\n",
      "Epoch 564/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.8983 - acc: 0.7823 - val_loss: 0.9511 - val_acc: 0.7560\n",
      "Epoch 565/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8969 - acc: 0.7809 - val_loss: 0.9446 - val_acc: 0.7610\n",
      "Epoch 566/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8982 - acc: 0.7806 - val_loss: 0.9466 - val_acc: 0.7600\n",
      "Epoch 567/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8979 - acc: 0.7820 - val_loss: 0.9461 - val_acc: 0.7570\n",
      "Epoch 568/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8961 - acc: 0.7831 - val_loss: 0.9535 - val_acc: 0.7540\n",
      "Epoch 569/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.8981 - acc: 0.7825 - val_loss: 0.9429 - val_acc: 0.7610\n",
      "Epoch 570/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8957 - acc: 0.7822 - val_loss: 0.9476 - val_acc: 0.7610\n",
      "Epoch 571/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8955 - acc: 0.7832 - val_loss: 0.9454 - val_acc: 0.7560\n",
      "Epoch 572/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8951 - acc: 0.7838 - val_loss: 0.9418 - val_acc: 0.7600\n",
      "Epoch 573/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8951 - acc: 0.7795 - val_loss: 0.9447 - val_acc: 0.7590\n",
      "Epoch 574/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8952 - acc: 0.7820 - val_loss: 0.9462 - val_acc: 0.7530\n",
      "Epoch 575/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8944 - acc: 0.7822 - val_loss: 0.9399 - val_acc: 0.7610\n",
      "Epoch 576/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8941 - acc: 0.7832 - val_loss: 0.9512 - val_acc: 0.7520\n",
      "Epoch 577/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8938 - acc: 0.7832 - val_loss: 0.9464 - val_acc: 0.7500\n",
      "Epoch 578/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8943 - acc: 0.7800 - val_loss: 0.9912 - val_acc: 0.7210\n",
      "Epoch 579/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.8944 - acc: 0.7826 - val_loss: 0.9390 - val_acc: 0.7610\n",
      "Epoch 580/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8922 - acc: 0.7842 - val_loss: 0.9412 - val_acc: 0.7620\n",
      "Epoch 581/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8922 - acc: 0.7840 - val_loss: 0.9419 - val_acc: 0.7590\n",
      "Epoch 582/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8926 - acc: 0.7838 - val_loss: 0.9397 - val_acc: 0.7590\n",
      "Epoch 583/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8919 - acc: 0.7831 - val_loss: 0.9385 - val_acc: 0.7630\n",
      "Epoch 584/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8912 - acc: 0.7826 - val_loss: 0.9407 - val_acc: 0.7560\n",
      "Epoch 585/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8916 - acc: 0.7831 - val_loss: 0.9471 - val_acc: 0.7510\n",
      "Epoch 586/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8913 - acc: 0.7832 - val_loss: 0.9406 - val_acc: 0.7600\n",
      "Epoch 587/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8895 - acc: 0.7837 - val_loss: 0.9395 - val_acc: 0.7550\n",
      "Epoch 588/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8898 - acc: 0.7838 - val_loss: 0.9444 - val_acc: 0.7600\n",
      "Epoch 589/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8890 - acc: 0.7843 - val_loss: 0.9427 - val_acc: 0.7560\n",
      "Epoch 590/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8896 - acc: 0.7854 - val_loss: 0.9411 - val_acc: 0.7620\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8896 - acc: 0.7834 - val_loss: 0.9401 - val_acc: 0.7560\n",
      "Epoch 592/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8886 - acc: 0.7862 - val_loss: 0.9413 - val_acc: 0.7530\n",
      "Epoch 593/1000\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.8889 - acc: 0.7825 - val_loss: 0.9420 - val_acc: 0.7580\n",
      "Epoch 594/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.8897 - acc: 0.7832 - val_loss: 0.9399 - val_acc: 0.7570\n",
      "Epoch 595/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8874 - acc: 0.7837 - val_loss: 0.9410 - val_acc: 0.7610\n",
      "Epoch 596/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8879 - acc: 0.7842 - val_loss: 0.9364 - val_acc: 0.7620\n",
      "Epoch 597/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.8875 - acc: 0.7840 - val_loss: 0.9487 - val_acc: 0.7490\n",
      "Epoch 598/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8872 - acc: 0.7832 - val_loss: 0.9378 - val_acc: 0.7620\n",
      "Epoch 599/1000\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.8855 - acc: 0.7849 - val_loss: 0.9493 - val_acc: 0.7480\n",
      "Epoch 600/1000\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.8873 - acc: 0.7845 - val_loss: 0.9434 - val_acc: 0.7600\n",
      "Epoch 601/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8869 - acc: 0.7838 - val_loss: 0.9416 - val_acc: 0.7540\n",
      "Epoch 602/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8863 - acc: 0.7825 - val_loss: 0.9413 - val_acc: 0.7580\n",
      "Epoch 603/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8852 - acc: 0.7852 - val_loss: 0.9415 - val_acc: 0.7560\n",
      "Epoch 604/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8849 - acc: 0.7838 - val_loss: 0.9418 - val_acc: 0.7590\n",
      "Epoch 605/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8853 - acc: 0.7857 - val_loss: 0.9354 - val_acc: 0.7630\n",
      "Epoch 606/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8843 - acc: 0.7848 - val_loss: 0.9471 - val_acc: 0.7570\n",
      "Epoch 607/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8850 - acc: 0.7834 - val_loss: 0.9594 - val_acc: 0.7420\n",
      "Epoch 608/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8847 - acc: 0.7846 - val_loss: 0.9472 - val_acc: 0.7540\n",
      "Epoch 609/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8833 - acc: 0.7865 - val_loss: 0.9383 - val_acc: 0.7590\n",
      "Epoch 610/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8838 - acc: 0.7849 - val_loss: 0.9398 - val_acc: 0.7570\n",
      "Epoch 611/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8826 - acc: 0.7862 - val_loss: 0.9429 - val_acc: 0.7580\n",
      "Epoch 612/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8831 - acc: 0.7851 - val_loss: 0.9426 - val_acc: 0.7580\n",
      "Epoch 613/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8829 - acc: 0.7843 - val_loss: 0.9359 - val_acc: 0.7600\n",
      "Epoch 614/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8816 - acc: 0.7874 - val_loss: 0.9413 - val_acc: 0.7560\n",
      "Epoch 615/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8815 - acc: 0.7845 - val_loss: 0.9370 - val_acc: 0.7550\n",
      "Epoch 616/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8818 - acc: 0.7857 - val_loss: 0.9554 - val_acc: 0.7480\n",
      "Epoch 617/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8834 - acc: 0.7852 - val_loss: 0.9390 - val_acc: 0.7520\n",
      "Epoch 618/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8816 - acc: 0.7842 - val_loss: 0.9413 - val_acc: 0.7550\n",
      "Epoch 619/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8805 - acc: 0.7837 - val_loss: 0.9468 - val_acc: 0.7540\n",
      "Epoch 620/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8807 - acc: 0.7851 - val_loss: 0.9367 - val_acc: 0.7590\n",
      "Epoch 621/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8803 - acc: 0.7871 - val_loss: 0.9492 - val_acc: 0.7550\n",
      "Epoch 622/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8799 - acc: 0.7857 - val_loss: 0.9342 - val_acc: 0.7600\n",
      "Epoch 623/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8797 - acc: 0.7840 - val_loss: 0.9368 - val_acc: 0.7550\n",
      "Epoch 624/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8790 - acc: 0.7878 - val_loss: 0.9385 - val_acc: 0.7540\n",
      "Epoch 625/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8796 - acc: 0.7855 - val_loss: 0.9410 - val_acc: 0.7550\n",
      "Epoch 626/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8780 - acc: 0.7872 - val_loss: 0.9439 - val_acc: 0.7540\n",
      "Epoch 627/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8785 - acc: 0.7882 - val_loss: 0.9344 - val_acc: 0.7660\n",
      "Epoch 628/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8784 - acc: 0.7875 - val_loss: 0.9332 - val_acc: 0.7590\n",
      "Epoch 629/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.8775 - acc: 0.7895 - val_loss: 0.9364 - val_acc: 0.7580\n",
      "Epoch 630/1000\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.8775 - acc: 0.7871 - val_loss: 0.9563 - val_acc: 0.7530\n",
      "Epoch 631/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.8790 - acc: 0.7854 - val_loss: 0.9318 - val_acc: 0.7610\n",
      "Epoch 632/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8771 - acc: 0.7875 - val_loss: 0.9338 - val_acc: 0.7580\n",
      "Epoch 633/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8774 - acc: 0.7855 - val_loss: 0.9395 - val_acc: 0.7500\n",
      "Epoch 634/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8773 - acc: 0.7883 - val_loss: 0.9382 - val_acc: 0.7620\n",
      "Epoch 635/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8766 - acc: 0.7871 - val_loss: 0.9348 - val_acc: 0.7560\n",
      "Epoch 636/1000\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.8763 - acc: 0.7875 - val_loss: 0.9402 - val_acc: 0.7590\n",
      "Epoch 637/1000\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.8769 - acc: 0.7855 - val_loss: 0.9343 - val_acc: 0.7530\n",
      "Epoch 638/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 0.8757 - acc: 0.7892 - val_loss: 0.9328 - val_acc: 0.7580\n",
      "Epoch 639/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8746 - acc: 0.7886 - val_loss: 0.9376 - val_acc: 0.7610\n",
      "Epoch 640/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8746 - acc: 0.7858 - val_loss: 0.9364 - val_acc: 0.7600\n",
      "Epoch 641/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8756 - acc: 0.7872 - val_loss: 0.9350 - val_acc: 0.7540\n",
      "Epoch 642/1000\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.8743 - acc: 0.7883 - val_loss: 0.9314 - val_acc: 0.7600\n",
      "Epoch 643/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8732 - acc: 0.7874 - val_loss: 0.9340 - val_acc: 0.7580\n",
      "Epoch 644/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8731 - acc: 0.7886 - val_loss: 0.9313 - val_acc: 0.7620\n",
      "Epoch 645/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8736 - acc: 0.7891 - val_loss: 0.9391 - val_acc: 0.7560\n",
      "Epoch 646/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8737 - acc: 0.7889 - val_loss: 0.9337 - val_acc: 0.7570\n",
      "Epoch 647/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8743 - acc: 0.7883 - val_loss: 0.9406 - val_acc: 0.7510\n",
      "Epoch 648/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8738 - acc: 0.7880 - val_loss: 0.9331 - val_acc: 0.7640\n",
      "Epoch 649/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8728 - acc: 0.7880 - val_loss: 0.9319 - val_acc: 0.7610\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8716 - acc: 0.7902 - val_loss: 0.9329 - val_acc: 0.7580\n",
      "Epoch 651/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8711 - acc: 0.7885 - val_loss: 0.9413 - val_acc: 0.7590\n",
      "Epoch 652/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8722 - acc: 0.7880 - val_loss: 0.9337 - val_acc: 0.7620\n",
      "Epoch 653/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8719 - acc: 0.7895 - val_loss: 0.9352 - val_acc: 0.7560\n",
      "Epoch 654/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8713 - acc: 0.7862 - val_loss: 0.9345 - val_acc: 0.7630\n",
      "Epoch 655/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8711 - acc: 0.7874 - val_loss: 0.9347 - val_acc: 0.7580\n",
      "Epoch 656/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8711 - acc: 0.7895 - val_loss: 0.9366 - val_acc: 0.7670\n",
      "Epoch 657/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8697 - acc: 0.7872 - val_loss: 0.9388 - val_acc: 0.7610\n",
      "Epoch 658/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8701 - acc: 0.7897 - val_loss: 0.9403 - val_acc: 0.7510\n",
      "Epoch 659/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8693 - acc: 0.7858 - val_loss: 0.9344 - val_acc: 0.7520\n",
      "Epoch 660/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8701 - acc: 0.7889 - val_loss: 0.9296 - val_acc: 0.7530\n",
      "Epoch 661/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8688 - acc: 0.7894 - val_loss: 0.9346 - val_acc: 0.7590\n",
      "Epoch 662/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8689 - acc: 0.7885 - val_loss: 0.9378 - val_acc: 0.7580\n",
      "Epoch 663/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8690 - acc: 0.7892 - val_loss: 0.9308 - val_acc: 0.7610\n",
      "Epoch 664/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8680 - acc: 0.7885 - val_loss: 0.9271 - val_acc: 0.7600\n",
      "Epoch 665/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8693 - acc: 0.7902 - val_loss: 0.9462 - val_acc: 0.7530\n",
      "Epoch 666/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8692 - acc: 0.7908 - val_loss: 0.9288 - val_acc: 0.7630\n",
      "Epoch 667/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8682 - acc: 0.7897 - val_loss: 0.9318 - val_acc: 0.7550\n",
      "Epoch 668/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8695 - acc: 0.7882 - val_loss: 0.9277 - val_acc: 0.7660\n",
      "Epoch 669/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8678 - acc: 0.7920 - val_loss: 0.9420 - val_acc: 0.7530\n",
      "Epoch 670/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8686 - acc: 0.7894 - val_loss: 0.9320 - val_acc: 0.7620\n",
      "Epoch 671/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8689 - acc: 0.7902 - val_loss: 0.9384 - val_acc: 0.7500\n",
      "Epoch 672/1000\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.8657 - acc: 0.7908 - val_loss: 0.9258 - val_acc: 0.7640\n",
      "Epoch 673/1000\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.8672 - acc: 0.7891 - val_loss: 0.9441 - val_acc: 0.7570\n",
      "Epoch 674/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.8675 - acc: 0.7866 - val_loss: 0.9410 - val_acc: 0.7560\n",
      "Epoch 675/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8680 - acc: 0.7905 - val_loss: 0.9290 - val_acc: 0.7550\n",
      "Epoch 676/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8672 - acc: 0.7902 - val_loss: 0.9292 - val_acc: 0.7610\n",
      "Epoch 677/1000\n",
      "6500/6500 [==============================] - 0s 48us/step - loss: 0.8651 - acc: 0.7888 - val_loss: 0.9286 - val_acc: 0.7560\n",
      "Epoch 678/1000\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.8670 - acc: 0.7900 - val_loss: 0.9282 - val_acc: 0.7550\n",
      "Epoch 679/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.8659 - acc: 0.7894 - val_loss: 0.9425 - val_acc: 0.7480\n",
      "Epoch 680/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8664 - acc: 0.7891 - val_loss: 0.9474 - val_acc: 0.7450\n",
      "Epoch 681/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.8650 - acc: 0.7892 - val_loss: 0.9281 - val_acc: 0.7520\n",
      "Epoch 682/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8647 - acc: 0.7909 - val_loss: 0.9267 - val_acc: 0.7580\n",
      "Epoch 683/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8626 - acc: 0.7929 - val_loss: 0.9786 - val_acc: 0.7290\n",
      "Epoch 684/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8665 - acc: 0.7906 - val_loss: 0.9337 - val_acc: 0.7520\n",
      "Epoch 685/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8644 - acc: 0.7895 - val_loss: 0.9285 - val_acc: 0.7570\n",
      "Epoch 686/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8645 - acc: 0.7912 - val_loss: 0.9407 - val_acc: 0.7510\n",
      "Epoch 687/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8630 - acc: 0.7882 - val_loss: 0.9379 - val_acc: 0.7520\n",
      "Epoch 688/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8639 - acc: 0.7892 - val_loss: 0.9314 - val_acc: 0.7500\n",
      "Epoch 689/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8640 - acc: 0.7886 - val_loss: 0.9381 - val_acc: 0.7580\n",
      "Epoch 690/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8623 - acc: 0.7917 - val_loss: 0.9323 - val_acc: 0.7550\n",
      "Epoch 691/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8623 - acc: 0.7931 - val_loss: 0.9286 - val_acc: 0.7600\n",
      "Epoch 692/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8615 - acc: 0.7917 - val_loss: 0.9341 - val_acc: 0.7520\n",
      "Epoch 693/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8624 - acc: 0.7905 - val_loss: 0.9316 - val_acc: 0.7410\n",
      "Epoch 694/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8614 - acc: 0.7914 - val_loss: 0.9309 - val_acc: 0.7600\n",
      "Epoch 695/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8615 - acc: 0.7912 - val_loss: 0.9272 - val_acc: 0.7620\n",
      "Epoch 696/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8612 - acc: 0.7926 - val_loss: 0.9310 - val_acc: 0.7630\n",
      "Epoch 697/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8622 - acc: 0.7898 - val_loss: 0.9239 - val_acc: 0.7630\n",
      "Epoch 698/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8610 - acc: 0.7923 - val_loss: 0.9225 - val_acc: 0.7630\n",
      "Epoch 699/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8613 - acc: 0.7892 - val_loss: 0.9248 - val_acc: 0.7570\n",
      "Epoch 700/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8618 - acc: 0.7914 - val_loss: 0.9266 - val_acc: 0.7620\n",
      "Epoch 701/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8602 - acc: 0.7906 - val_loss: 0.9226 - val_acc: 0.7530\n",
      "Epoch 702/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8590 - acc: 0.7948 - val_loss: 0.9305 - val_acc: 0.7520\n",
      "Epoch 703/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8584 - acc: 0.7928 - val_loss: 0.9331 - val_acc: 0.7550\n",
      "Epoch 704/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8597 - acc: 0.7915 - val_loss: 0.9283 - val_acc: 0.7640\n",
      "Epoch 705/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8601 - acc: 0.7931 - val_loss: 0.9380 - val_acc: 0.7520\n",
      "Epoch 706/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8604 - acc: 0.7926 - val_loss: 0.9257 - val_acc: 0.7570\n",
      "Epoch 707/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8594 - acc: 0.7926 - val_loss: 0.9286 - val_acc: 0.7590\n",
      "Epoch 708/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8590 - acc: 0.7925 - val_loss: 0.9285 - val_acc: 0.7540\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8582 - acc: 0.7940 - val_loss: 0.9336 - val_acc: 0.7490\n",
      "Epoch 710/1000\n",
      "6500/6500 [==============================] - 0s 30us/step - loss: 0.8575 - acc: 0.7920 - val_loss: 0.9298 - val_acc: 0.7500\n",
      "Epoch 711/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.8572 - acc: 0.7926 - val_loss: 0.9217 - val_acc: 0.7590\n",
      "Epoch 712/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.8568 - acc: 0.7902 - val_loss: 0.9272 - val_acc: 0.7570\n",
      "Epoch 713/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8576 - acc: 0.7926 - val_loss: 0.9299 - val_acc: 0.7540\n",
      "Epoch 714/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8576 - acc: 0.7946 - val_loss: 0.9274 - val_acc: 0.7540\n",
      "Epoch 715/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8571 - acc: 0.7915 - val_loss: 0.9285 - val_acc: 0.7550\n",
      "Epoch 716/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.8560 - acc: 0.7926 - val_loss: 0.9333 - val_acc: 0.7600\n",
      "Epoch 717/1000\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 0.8578 - acc: 0.7931 - val_loss: 0.9226 - val_acc: 0.7590\n",
      "Epoch 718/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8566 - acc: 0.7929 - val_loss: 0.9220 - val_acc: 0.7560\n",
      "Epoch 719/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8570 - acc: 0.7909 - val_loss: 0.9288 - val_acc: 0.7650\n",
      "Epoch 720/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8558 - acc: 0.7934 - val_loss: 0.9297 - val_acc: 0.7560\n",
      "Epoch 721/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8554 - acc: 0.7918 - val_loss: 0.9244 - val_acc: 0.7640\n",
      "Epoch 722/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8553 - acc: 0.7926 - val_loss: 0.9212 - val_acc: 0.7630\n",
      "Epoch 723/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8536 - acc: 0.7943 - val_loss: 0.9234 - val_acc: 0.7570\n",
      "Epoch 724/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8560 - acc: 0.7934 - val_loss: 0.9253 - val_acc: 0.7520\n",
      "Epoch 725/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8544 - acc: 0.7931 - val_loss: 0.9331 - val_acc: 0.7580\n",
      "Epoch 726/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8540 - acc: 0.7940 - val_loss: 0.9264 - val_acc: 0.7540\n",
      "Epoch 727/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8558 - acc: 0.7926 - val_loss: 0.9371 - val_acc: 0.7500\n",
      "Epoch 728/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8553 - acc: 0.7909 - val_loss: 0.9205 - val_acc: 0.7660\n",
      "Epoch 729/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8540 - acc: 0.7934 - val_loss: 0.9255 - val_acc: 0.7590\n",
      "Epoch 730/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8559 - acc: 0.7917 - val_loss: 0.9216 - val_acc: 0.7570\n",
      "Epoch 731/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8525 - acc: 0.7920 - val_loss: 0.9308 - val_acc: 0.7590\n",
      "Epoch 732/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8543 - acc: 0.7908 - val_loss: 0.9200 - val_acc: 0.7630\n",
      "Epoch 733/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8536 - acc: 0.7934 - val_loss: 0.9260 - val_acc: 0.7600\n",
      "Epoch 734/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8531 - acc: 0.7951 - val_loss: 0.9269 - val_acc: 0.7580\n",
      "Epoch 735/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8533 - acc: 0.7920 - val_loss: 0.9242 - val_acc: 0.7630\n",
      "Epoch 736/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8531 - acc: 0.7957 - val_loss: 0.9326 - val_acc: 0.7550\n",
      "Epoch 737/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8531 - acc: 0.7932 - val_loss: 0.9248 - val_acc: 0.7550\n",
      "Epoch 738/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8525 - acc: 0.7915 - val_loss: 0.9304 - val_acc: 0.7580\n",
      "Epoch 739/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8528 - acc: 0.7920 - val_loss: 0.9213 - val_acc: 0.7560\n",
      "Epoch 740/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8514 - acc: 0.7943 - val_loss: 0.9334 - val_acc: 0.7500\n",
      "Epoch 741/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8512 - acc: 0.7954 - val_loss: 0.9295 - val_acc: 0.7600\n",
      "Epoch 742/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8520 - acc: 0.7940 - val_loss: 0.9383 - val_acc: 0.7510\n",
      "Epoch 743/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8517 - acc: 0.7942 - val_loss: 0.9332 - val_acc: 0.7530\n",
      "Epoch 744/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8520 - acc: 0.7948 - val_loss: 0.9270 - val_acc: 0.7480\n",
      "Epoch 745/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8505 - acc: 0.7965 - val_loss: 0.9448 - val_acc: 0.7520\n",
      "Epoch 746/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8513 - acc: 0.7938 - val_loss: 0.9284 - val_acc: 0.7490\n",
      "Epoch 747/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8515 - acc: 0.7918 - val_loss: 0.9214 - val_acc: 0.7520\n",
      "Epoch 748/1000\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.8511 - acc: 0.7940 - val_loss: 0.9274 - val_acc: 0.7540\n",
      "Epoch 749/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8498 - acc: 0.7935 - val_loss: 0.9317 - val_acc: 0.7560\n",
      "Epoch 750/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8496 - acc: 0.7945 - val_loss: 0.9327 - val_acc: 0.7520\n",
      "Epoch 751/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.8498 - acc: 0.7975 - val_loss: 0.9289 - val_acc: 0.7590\n",
      "Epoch 752/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8492 - acc: 0.7955 - val_loss: 0.9343 - val_acc: 0.7530\n",
      "Epoch 753/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8491 - acc: 0.7960 - val_loss: 0.9301 - val_acc: 0.7600\n",
      "Epoch 754/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8503 - acc: 0.7935 - val_loss: 0.9413 - val_acc: 0.7590\n",
      "Epoch 755/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8487 - acc: 0.7974 - val_loss: 0.9288 - val_acc: 0.7610\n",
      "Epoch 756/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8488 - acc: 0.7928 - val_loss: 0.9224 - val_acc: 0.7600\n",
      "Epoch 757/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8523 - acc: 0.7946 - val_loss: 0.9184 - val_acc: 0.7540\n",
      "Epoch 758/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.8466 - acc: 0.7963 - val_loss: 0.9262 - val_acc: 0.7560\n",
      "Epoch 759/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8483 - acc: 0.7935 - val_loss: 0.9265 - val_acc: 0.7560\n",
      "Epoch 760/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8483 - acc: 0.7943 - val_loss: 0.9219 - val_acc: 0.7650\n",
      "Epoch 761/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8477 - acc: 0.7966 - val_loss: 0.9265 - val_acc: 0.7510\n",
      "Epoch 762/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8470 - acc: 0.7949 - val_loss: 0.9239 - val_acc: 0.7560\n",
      "Epoch 763/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8480 - acc: 0.7960 - val_loss: 0.9295 - val_acc: 0.7510\n",
      "Epoch 764/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8488 - acc: 0.7963 - val_loss: 0.9251 - val_acc: 0.7580\n",
      "Epoch 765/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8467 - acc: 0.7942 - val_loss: 0.9289 - val_acc: 0.7610\n",
      "Epoch 766/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8467 - acc: 0.7957 - val_loss: 0.9230 - val_acc: 0.7480\n",
      "Epoch 767/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8470 - acc: 0.7946 - val_loss: 0.9312 - val_acc: 0.7600\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8474 - acc: 0.7954 - val_loss: 0.9252 - val_acc: 0.7640\n",
      "Epoch 769/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8463 - acc: 0.7966 - val_loss: 0.9250 - val_acc: 0.7520\n",
      "Epoch 770/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8472 - acc: 0.7968 - val_loss: 0.9187 - val_acc: 0.7600\n",
      "Epoch 771/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8485 - acc: 0.7954 - val_loss: 0.9239 - val_acc: 0.7630\n",
      "Epoch 772/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8466 - acc: 0.7960 - val_loss: 0.9204 - val_acc: 0.7540\n",
      "Epoch 773/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8451 - acc: 0.7968 - val_loss: 0.9188 - val_acc: 0.7570\n",
      "Epoch 774/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8450 - acc: 0.7966 - val_loss: 0.9286 - val_acc: 0.7560\n",
      "Epoch 775/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8456 - acc: 0.7983 - val_loss: 0.9218 - val_acc: 0.7580\n",
      "Epoch 776/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8461 - acc: 0.7977 - val_loss: 0.9468 - val_acc: 0.7540\n",
      "Epoch 777/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.8449 - acc: 0.7966 - val_loss: 0.9229 - val_acc: 0.7590\n",
      "Epoch 778/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8464 - acc: 0.7925 - val_loss: 0.9206 - val_acc: 0.7680\n",
      "Epoch 779/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 0.8459 - acc: 0.7977 - val_loss: 0.9164 - val_acc: 0.7570\n",
      "Epoch 780/1000\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.8444 - acc: 0.7942 - val_loss: 0.9247 - val_acc: 0.7490\n",
      "Epoch 781/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8439 - acc: 0.7989 - val_loss: 0.9191 - val_acc: 0.7620\n",
      "Epoch 782/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8452 - acc: 0.7982 - val_loss: 0.9238 - val_acc: 0.7600\n",
      "Epoch 783/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8429 - acc: 0.7972 - val_loss: 0.9203 - val_acc: 0.7520\n",
      "Epoch 784/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8429 - acc: 0.7983 - val_loss: 0.9424 - val_acc: 0.7470\n",
      "Epoch 785/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.8437 - acc: 0.7954 - val_loss: 0.9217 - val_acc: 0.7570\n",
      "Epoch 786/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8429 - acc: 0.7952 - val_loss: 0.9253 - val_acc: 0.7570\n",
      "Epoch 787/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8437 - acc: 0.7969 - val_loss: 0.9244 - val_acc: 0.7650\n",
      "Epoch 788/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8427 - acc: 0.7980 - val_loss: 0.9185 - val_acc: 0.7590\n",
      "Epoch 789/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8423 - acc: 0.7978 - val_loss: 0.9182 - val_acc: 0.7600\n",
      "Epoch 790/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8442 - acc: 0.7971 - val_loss: 0.9241 - val_acc: 0.7610\n",
      "Epoch 791/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.8437 - acc: 0.7977 - val_loss: 0.9354 - val_acc: 0.7510\n",
      "Epoch 792/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8429 - acc: 0.7983 - val_loss: 0.9186 - val_acc: 0.7570\n",
      "Epoch 793/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8424 - acc: 0.7972 - val_loss: 0.9240 - val_acc: 0.7520\n",
      "Epoch 794/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8440 - acc: 0.7969 - val_loss: 0.9275 - val_acc: 0.7530\n",
      "Epoch 795/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8433 - acc: 0.7945 - val_loss: 0.9277 - val_acc: 0.7650\n",
      "Epoch 796/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8439 - acc: 0.7949 - val_loss: 0.9209 - val_acc: 0.7600\n",
      "Epoch 797/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8405 - acc: 0.7975 - val_loss: 0.9249 - val_acc: 0.7580\n",
      "Epoch 798/1000\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.8410 - acc: 0.7980 - val_loss: 0.9326 - val_acc: 0.7540\n",
      "Epoch 799/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8422 - acc: 0.7992 - val_loss: 0.9687 - val_acc: 0.7400\n",
      "Epoch 800/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.8428 - acc: 0.7983 - val_loss: 0.9187 - val_acc: 0.7530\n",
      "Epoch 801/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8403 - acc: 0.7998 - val_loss: 0.9242 - val_acc: 0.7530\n",
      "Epoch 802/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8408 - acc: 0.7980 - val_loss: 0.9266 - val_acc: 0.7570\n",
      "Epoch 803/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8407 - acc: 0.7980 - val_loss: 0.9248 - val_acc: 0.7510\n",
      "Epoch 804/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8401 - acc: 0.7989 - val_loss: 0.9377 - val_acc: 0.7550\n",
      "Epoch 805/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8409 - acc: 0.7977 - val_loss: 0.9167 - val_acc: 0.7620\n",
      "Epoch 806/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8389 - acc: 0.8012 - val_loss: 0.9392 - val_acc: 0.7550\n",
      "Epoch 807/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8395 - acc: 0.7982 - val_loss: 0.9252 - val_acc: 0.7630\n",
      "Epoch 808/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8394 - acc: 0.7988 - val_loss: 0.9177 - val_acc: 0.7560\n",
      "Epoch 809/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8386 - acc: 0.7991 - val_loss: 0.9282 - val_acc: 0.7620\n",
      "Epoch 810/1000\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.8389 - acc: 0.7958 - val_loss: 0.9192 - val_acc: 0.7520\n",
      "Epoch 811/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.8391 - acc: 0.7983 - val_loss: 0.9142 - val_acc: 0.7640\n",
      "Epoch 812/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8391 - acc: 0.7965 - val_loss: 0.9246 - val_acc: 0.7620\n",
      "Epoch 813/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8383 - acc: 0.7968 - val_loss: 0.9168 - val_acc: 0.7590\n",
      "Epoch 814/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.8371 - acc: 0.7997 - val_loss: 0.9181 - val_acc: 0.7590\n",
      "Epoch 815/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8393 - acc: 0.7966 - val_loss: 0.9192 - val_acc: 0.7650\n",
      "Epoch 816/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8397 - acc: 0.7975 - val_loss: 0.9271 - val_acc: 0.7560\n",
      "Epoch 817/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8388 - acc: 0.7951 - val_loss: 0.9521 - val_acc: 0.7480\n",
      "Epoch 818/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8371 - acc: 0.7982 - val_loss: 0.9249 - val_acc: 0.7670\n",
      "Epoch 819/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8365 - acc: 0.7985 - val_loss: 0.9339 - val_acc: 0.7510\n",
      "Epoch 820/1000\n",
      "6500/6500 [==============================] - 0s 29us/step - loss: 0.8378 - acc: 0.8011 - val_loss: 0.9188 - val_acc: 0.7540\n",
      "Epoch 821/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8367 - acc: 0.7988 - val_loss: 0.9389 - val_acc: 0.7450\n",
      "Epoch 822/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8371 - acc: 0.8002 - val_loss: 0.9230 - val_acc: 0.7580\n",
      "Epoch 823/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8363 - acc: 0.8003 - val_loss: 0.9256 - val_acc: 0.7610\n",
      "Epoch 824/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8377 - acc: 0.7972 - val_loss: 0.9218 - val_acc: 0.7570\n",
      "Epoch 825/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8361 - acc: 0.7986 - val_loss: 0.9228 - val_acc: 0.7620\n",
      "Epoch 826/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.8380 - acc: 0.7986 - val_loss: 0.9183 - val_acc: 0.7670\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8356 - acc: 0.7986 - val_loss: 0.9212 - val_acc: 0.7610\n",
      "Epoch 828/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8353 - acc: 0.7986 - val_loss: 0.9145 - val_acc: 0.7600\n",
      "Epoch 829/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8367 - acc: 0.7977 - val_loss: 0.9182 - val_acc: 0.7620\n",
      "Epoch 830/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8361 - acc: 0.7986 - val_loss: 0.9253 - val_acc: 0.7640\n",
      "Epoch 831/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8370 - acc: 0.7997 - val_loss: 0.9219 - val_acc: 0.7570\n",
      "Epoch 832/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.8341 - acc: 0.7989 - val_loss: 0.9238 - val_acc: 0.7550\n",
      "Epoch 833/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8339 - acc: 0.7983 - val_loss: 0.9193 - val_acc: 0.7590\n",
      "Epoch 834/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8354 - acc: 0.8017 - val_loss: 0.9233 - val_acc: 0.7610\n",
      "Epoch 835/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8340 - acc: 0.7968 - val_loss: 0.9432 - val_acc: 0.7510\n",
      "Epoch 836/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8376 - acc: 0.7985 - val_loss: 0.9204 - val_acc: 0.7600\n",
      "Epoch 837/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.8360 - acc: 0.7968 - val_loss: 0.9204 - val_acc: 0.7600\n",
      "Epoch 838/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.8326 - acc: 0.8003 - val_loss: 0.9225 - val_acc: 0.7560\n",
      "Epoch 839/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8329 - acc: 0.7994 - val_loss: 0.9177 - val_acc: 0.7620\n",
      "Epoch 840/1000\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.8337 - acc: 0.7988 - val_loss: 0.9436 - val_acc: 0.7510\n",
      "Epoch 841/1000\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.8360 - acc: 0.7988 - val_loss: 0.9219 - val_acc: 0.7620\n",
      "Epoch 842/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8349 - acc: 0.7989 - val_loss: 0.9235 - val_acc: 0.7610\n",
      "Epoch 843/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8360 - acc: 0.8003 - val_loss: 0.9159 - val_acc: 0.7610\n",
      "Epoch 844/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8352 - acc: 0.7966 - val_loss: 0.9470 - val_acc: 0.7510\n",
      "Epoch 845/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8366 - acc: 0.7965 - val_loss: 0.9167 - val_acc: 0.7620\n",
      "Epoch 846/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8338 - acc: 0.7982 - val_loss: 0.9225 - val_acc: 0.7530\n",
      "Epoch 847/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8329 - acc: 0.7989 - val_loss: 0.9169 - val_acc: 0.7550\n",
      "Epoch 848/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8330 - acc: 0.8005 - val_loss: 0.9282 - val_acc: 0.7580\n",
      "Epoch 849/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8315 - acc: 0.8006 - val_loss: 0.9265 - val_acc: 0.7560\n",
      "Epoch 850/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8323 - acc: 0.7992 - val_loss: 0.9246 - val_acc: 0.7620\n",
      "Epoch 851/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.8325 - acc: 0.7992 - val_loss: 0.9211 - val_acc: 0.7650\n",
      "Epoch 852/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8338 - acc: 0.7998 - val_loss: 0.9319 - val_acc: 0.7570\n",
      "Epoch 853/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8331 - acc: 0.8006 - val_loss: 0.9196 - val_acc: 0.7650\n",
      "Epoch 854/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8320 - acc: 0.7988 - val_loss: 0.9284 - val_acc: 0.7490\n",
      "Epoch 855/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8335 - acc: 0.7986 - val_loss: 0.9203 - val_acc: 0.7560\n",
      "Epoch 856/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8349 - acc: 0.7972 - val_loss: 0.9134 - val_acc: 0.7620\n",
      "Epoch 857/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8309 - acc: 0.8017 - val_loss: 0.9221 - val_acc: 0.7610\n",
      "Epoch 858/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8352 - acc: 0.7983 - val_loss: 0.9201 - val_acc: 0.7580\n",
      "Epoch 859/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8336 - acc: 0.7992 - val_loss: 0.9198 - val_acc: 0.7560\n",
      "Epoch 860/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8298 - acc: 0.8008 - val_loss: 0.9345 - val_acc: 0.7520\n",
      "Epoch 861/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8306 - acc: 0.8014 - val_loss: 0.9414 - val_acc: 0.7570\n",
      "Epoch 862/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8308 - acc: 0.8000 - val_loss: 0.9162 - val_acc: 0.7620\n",
      "Epoch 863/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8281 - acc: 0.8020 - val_loss: 0.9183 - val_acc: 0.7620\n",
      "Epoch 864/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8337 - acc: 0.8000 - val_loss: 0.9379 - val_acc: 0.7550\n",
      "Epoch 865/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8304 - acc: 0.7992 - val_loss: 0.9189 - val_acc: 0.7590\n",
      "Epoch 866/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8327 - acc: 0.7995 - val_loss: 0.9456 - val_acc: 0.7480\n",
      "Epoch 867/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.8310 - acc: 0.8012 - val_loss: 0.9244 - val_acc: 0.7620\n",
      "Epoch 868/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8326 - acc: 0.7995 - val_loss: 0.9159 - val_acc: 0.7630\n",
      "Epoch 869/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8290 - acc: 0.8009 - val_loss: 0.9201 - val_acc: 0.7590\n",
      "Epoch 870/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8312 - acc: 0.8055 - val_loss: 0.9268 - val_acc: 0.7640\n",
      "Epoch 871/1000\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.8310 - acc: 0.7997 - val_loss: 0.9208 - val_acc: 0.7570\n",
      "Epoch 872/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.8309 - acc: 0.7994 - val_loss: 0.9300 - val_acc: 0.7550\n",
      "Epoch 873/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8290 - acc: 0.8025 - val_loss: 0.9288 - val_acc: 0.7610\n",
      "Epoch 874/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.8309 - acc: 0.8014 - val_loss: 0.9196 - val_acc: 0.7550\n",
      "Epoch 875/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8280 - acc: 0.8003 - val_loss: 0.9932 - val_acc: 0.7260\n",
      "Epoch 876/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.8333 - acc: 0.7946 - val_loss: 0.9127 - val_acc: 0.7560\n",
      "Epoch 877/1000\n",
      "6500/6500 [==============================] - 0s 66us/step - loss: 0.8299 - acc: 0.7986 - val_loss: 0.9370 - val_acc: 0.7600\n",
      "Epoch 878/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8266 - acc: 0.8020 - val_loss: 0.9282 - val_acc: 0.7590\n",
      "Epoch 879/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8300 - acc: 0.7995 - val_loss: 0.9188 - val_acc: 0.7570\n",
      "Epoch 880/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8284 - acc: 0.8002 - val_loss: 0.9158 - val_acc: 0.7570\n",
      "Epoch 881/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8281 - acc: 0.7998 - val_loss: 0.9211 - val_acc: 0.7620\n",
      "Epoch 882/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8269 - acc: 0.8015 - val_loss: 0.9204 - val_acc: 0.7580\n",
      "Epoch 883/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8275 - acc: 0.7995 - val_loss: 0.9236 - val_acc: 0.7540\n",
      "Epoch 884/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8270 - acc: 0.8009 - val_loss: 0.9130 - val_acc: 0.7650\n",
      "Epoch 885/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8280 - acc: 0.7998 - val_loss: 0.9225 - val_acc: 0.7540\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8289 - acc: 0.8017 - val_loss: 0.9163 - val_acc: 0.7590\n",
      "Epoch 887/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.8269 - acc: 0.8002 - val_loss: 0.9390 - val_acc: 0.7470\n",
      "Epoch 888/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8264 - acc: 0.8009 - val_loss: 0.9424 - val_acc: 0.7500\n",
      "Epoch 889/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8274 - acc: 0.8002 - val_loss: 0.9213 - val_acc: 0.7620\n",
      "Epoch 890/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8257 - acc: 0.8022 - val_loss: 0.9225 - val_acc: 0.7580\n",
      "Epoch 891/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8278 - acc: 0.8000 - val_loss: 0.9241 - val_acc: 0.7650\n",
      "Epoch 892/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8253 - acc: 0.8006 - val_loss: 0.9233 - val_acc: 0.7630\n",
      "Epoch 893/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.8275 - acc: 0.8029 - val_loss: 0.9192 - val_acc: 0.7560\n",
      "Epoch 894/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8245 - acc: 0.7994 - val_loss: 0.9183 - val_acc: 0.7620\n",
      "Epoch 895/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8269 - acc: 0.8040 - val_loss: 0.9112 - val_acc: 0.7620\n",
      "Epoch 896/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8246 - acc: 0.8031 - val_loss: 0.9350 - val_acc: 0.7580\n",
      "Epoch 897/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8269 - acc: 0.8011 - val_loss: 0.9229 - val_acc: 0.7580\n",
      "Epoch 898/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8254 - acc: 0.7998 - val_loss: 0.9193 - val_acc: 0.7620\n",
      "Epoch 899/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8235 - acc: 0.8025 - val_loss: 0.9351 - val_acc: 0.7520\n",
      "Epoch 900/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8266 - acc: 0.7997 - val_loss: 0.9201 - val_acc: 0.7560\n",
      "Epoch 901/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8255 - acc: 0.8011 - val_loss: 0.9184 - val_acc: 0.7640\n",
      "Epoch 902/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8238 - acc: 0.7991 - val_loss: 0.9266 - val_acc: 0.7620\n",
      "Epoch 903/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8264 - acc: 0.7995 - val_loss: 0.9179 - val_acc: 0.7690\n",
      "Epoch 904/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8270 - acc: 0.7992 - val_loss: 0.9308 - val_acc: 0.7630\n",
      "Epoch 905/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8261 - acc: 0.8018 - val_loss: 0.9152 - val_acc: 0.7660\n",
      "Epoch 906/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8267 - acc: 0.8028 - val_loss: 0.9222 - val_acc: 0.7630\n",
      "Epoch 907/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8242 - acc: 0.8015 - val_loss: 0.9194 - val_acc: 0.7580\n",
      "Epoch 908/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8228 - acc: 0.8034 - val_loss: 0.9457 - val_acc: 0.7530\n",
      "Epoch 909/1000\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.8238 - acc: 0.8035 - val_loss: 0.9252 - val_acc: 0.7600\n",
      "Epoch 910/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8248 - acc: 0.8025 - val_loss: 0.9834 - val_acc: 0.7360\n",
      "Epoch 911/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8274 - acc: 0.8035 - val_loss: 0.9127 - val_acc: 0.7720\n",
      "Epoch 912/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8255 - acc: 0.8014 - val_loss: 0.9157 - val_acc: 0.7670\n",
      "Epoch 913/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8228 - acc: 0.8020 - val_loss: 0.9130 - val_acc: 0.7660\n",
      "Epoch 914/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.8231 - acc: 0.8028 - val_loss: 0.9263 - val_acc: 0.7630\n",
      "Epoch 915/1000\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.8219 - acc: 0.8006 - val_loss: 0.9249 - val_acc: 0.7540\n",
      "Epoch 916/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8227 - acc: 0.8022 - val_loss: 0.9129 - val_acc: 0.7620\n",
      "Epoch 917/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.8227 - acc: 0.8014 - val_loss: 0.9247 - val_acc: 0.7510\n",
      "Epoch 918/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.8231 - acc: 0.8040 - val_loss: 0.9120 - val_acc: 0.7680\n",
      "Epoch 919/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.8210 - acc: 0.8018 - val_loss: 0.9241 - val_acc: 0.7620\n",
      "Epoch 920/1000\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.8218 - acc: 0.8020 - val_loss: 0.9392 - val_acc: 0.7550\n",
      "Epoch 921/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8219 - acc: 0.8000 - val_loss: 0.9174 - val_acc: 0.7610\n",
      "Epoch 922/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8241 - acc: 0.8020 - val_loss: 0.9170 - val_acc: 0.7620\n",
      "Epoch 923/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8250 - acc: 0.8062 - val_loss: 0.9131 - val_acc: 0.7710\n",
      "Epoch 924/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8221 - acc: 0.8052 - val_loss: 0.9279 - val_acc: 0.7590\n",
      "Epoch 925/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8240 - acc: 0.8003 - val_loss: 0.9193 - val_acc: 0.7610\n",
      "Epoch 926/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8219 - acc: 0.8040 - val_loss: 0.9191 - val_acc: 0.7570\n",
      "Epoch 927/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8247 - acc: 0.8026 - val_loss: 0.9181 - val_acc: 0.7590\n",
      "Epoch 928/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8198 - acc: 0.8025 - val_loss: 0.9210 - val_acc: 0.7600\n",
      "Epoch 929/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8218 - acc: 0.8046 - val_loss: 0.9224 - val_acc: 0.7660\n",
      "Epoch 930/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8211 - acc: 0.8029 - val_loss: 0.9140 - val_acc: 0.7590\n",
      "Epoch 931/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8203 - acc: 0.8029 - val_loss: 0.9192 - val_acc: 0.7580\n",
      "Epoch 932/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.8201 - acc: 0.8051 - val_loss: 0.9273 - val_acc: 0.7590\n",
      "Epoch 933/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8205 - acc: 0.8028 - val_loss: 0.9281 - val_acc: 0.7650\n",
      "Epoch 934/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8218 - acc: 0.8057 - val_loss: 0.9348 - val_acc: 0.7570\n",
      "Epoch 935/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8238 - acc: 0.7998 - val_loss: 0.9120 - val_acc: 0.7590\n",
      "Epoch 936/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8199 - acc: 0.8046 - val_loss: 0.9161 - val_acc: 0.7600\n",
      "Epoch 937/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8212 - acc: 0.8051 - val_loss: 0.9124 - val_acc: 0.7570\n",
      "Epoch 938/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8222 - acc: 0.8018 - val_loss: 0.9452 - val_acc: 0.7520\n",
      "Epoch 939/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8227 - acc: 0.8035 - val_loss: 0.9199 - val_acc: 0.7630\n",
      "Epoch 940/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8207 - acc: 0.8043 - val_loss: 0.9306 - val_acc: 0.7650\n",
      "Epoch 941/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8196 - acc: 0.8058 - val_loss: 0.9245 - val_acc: 0.7640\n",
      "Epoch 942/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8193 - acc: 0.8052 - val_loss: 0.9129 - val_acc: 0.7720\n",
      "Epoch 943/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8211 - acc: 0.8043 - val_loss: 0.9133 - val_acc: 0.7680\n",
      "Epoch 944/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8195 - acc: 0.8046 - val_loss: 0.9158 - val_acc: 0.7620\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8200 - acc: 0.8054 - val_loss: 0.9186 - val_acc: 0.7700\n",
      "Epoch 946/1000\n",
      "6500/6500 [==============================] - 0s 44us/step - loss: 0.8184 - acc: 0.8051 - val_loss: 0.9164 - val_acc: 0.7710\n",
      "Epoch 947/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8216 - acc: 0.8042 - val_loss: 0.9187 - val_acc: 0.7580\n",
      "Epoch 948/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8196 - acc: 0.8034 - val_loss: 0.9203 - val_acc: 0.7600\n",
      "Epoch 949/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8245 - acc: 0.8051 - val_loss: 0.9140 - val_acc: 0.7660\n",
      "Epoch 950/1000\n",
      "6500/6500 [==============================] - 0s 45us/step - loss: 0.8175 - acc: 0.8031 - val_loss: 0.9412 - val_acc: 0.7500\n",
      "Epoch 951/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8201 - acc: 0.8025 - val_loss: 0.9260 - val_acc: 0.7540\n",
      "Epoch 952/1000\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.8189 - acc: 0.8051 - val_loss: 0.9172 - val_acc: 0.7720\n",
      "Epoch 953/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8189 - acc: 0.8060 - val_loss: 0.9153 - val_acc: 0.7610\n",
      "Epoch 954/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8200 - acc: 0.8043 - val_loss: 0.9464 - val_acc: 0.7490\n",
      "Epoch 955/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8174 - acc: 0.8051 - val_loss: 0.9560 - val_acc: 0.7420\n",
      "Epoch 956/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8228 - acc: 0.8045 - val_loss: 0.9195 - val_acc: 0.7650\n",
      "Epoch 957/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8236 - acc: 0.8034 - val_loss: 0.9704 - val_acc: 0.7350\n",
      "Epoch 958/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.8157 - acc: 0.8038 - val_loss: 0.9368 - val_acc: 0.7510\n",
      "Epoch 959/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.8178 - acc: 0.8040 - val_loss: 0.9142 - val_acc: 0.7690\n",
      "Epoch 960/1000\n",
      "6500/6500 [==============================] - 0s 32us/step - loss: 0.8163 - acc: 0.8063 - val_loss: 0.9145 - val_acc: 0.7570\n",
      "Epoch 961/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8165 - acc: 0.8060 - val_loss: 0.9186 - val_acc: 0.7580\n",
      "Epoch 962/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8172 - acc: 0.8049 - val_loss: 0.9172 - val_acc: 0.7640\n",
      "Epoch 963/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8149 - acc: 0.8051 - val_loss: 0.9557 - val_acc: 0.7420\n",
      "Epoch 964/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8193 - acc: 0.8049 - val_loss: 0.9283 - val_acc: 0.7570\n",
      "Epoch 965/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8175 - acc: 0.8045 - val_loss: 0.9211 - val_acc: 0.7520\n",
      "Epoch 966/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8195 - acc: 0.8049 - val_loss: 0.9161 - val_acc: 0.7630\n",
      "Epoch 967/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8148 - acc: 0.8066 - val_loss: 0.9219 - val_acc: 0.7690\n",
      "Epoch 968/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8190 - acc: 0.8038 - val_loss: 0.9163 - val_acc: 0.7640\n",
      "Epoch 969/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8193 - acc: 0.8038 - val_loss: 0.9288 - val_acc: 0.7640\n",
      "Epoch 970/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8129 - acc: 0.8052 - val_loss: 0.9178 - val_acc: 0.7660\n",
      "Epoch 971/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8157 - acc: 0.8054 - val_loss: 0.9125 - val_acc: 0.7660\n",
      "Epoch 972/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8145 - acc: 0.8091 - val_loss: 0.9435 - val_acc: 0.7520\n",
      "Epoch 973/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8153 - acc: 0.8048 - val_loss: 0.9259 - val_acc: 0.7610\n",
      "Epoch 974/1000\n",
      "6500/6500 [==============================] - 0s 39us/step - loss: 0.8168 - acc: 0.8069 - val_loss: 0.9296 - val_acc: 0.7520\n",
      "Epoch 975/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8185 - acc: 0.8028 - val_loss: 0.9311 - val_acc: 0.7530\n",
      "Epoch 976/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8211 - acc: 0.8058 - val_loss: 0.9281 - val_acc: 0.7540\n",
      "Epoch 977/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.8133 - acc: 0.8080 - val_loss: 0.9160 - val_acc: 0.7620\n",
      "Epoch 978/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8161 - acc: 0.8063 - val_loss: 0.9208 - val_acc: 0.7600\n",
      "Epoch 979/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8207 - acc: 0.8051 - val_loss: 0.9252 - val_acc: 0.7580\n",
      "Epoch 980/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8181 - acc: 0.8028 - val_loss: 0.9263 - val_acc: 0.7610\n",
      "Epoch 981/1000\n",
      "6500/6500 [==============================] - 0s 36us/step - loss: 0.8173 - acc: 0.8014 - val_loss: 0.9257 - val_acc: 0.7630\n",
      "Epoch 982/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8134 - acc: 0.8058 - val_loss: 0.9546 - val_acc: 0.7390\n",
      "Epoch 983/1000\n",
      "6500/6500 [==============================] - 0s 40us/step - loss: 0.8203 - acc: 0.8029 - val_loss: 0.9191 - val_acc: 0.7620\n",
      "Epoch 984/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8172 - acc: 0.8042 - val_loss: 0.9353 - val_acc: 0.7620\n",
      "Epoch 985/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8154 - acc: 0.8068 - val_loss: 0.9137 - val_acc: 0.7650\n",
      "Epoch 986/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8133 - acc: 0.8074 - val_loss: 0.9168 - val_acc: 0.7640\n",
      "Epoch 987/1000\n",
      "6500/6500 [==============================] - 0s 41us/step - loss: 0.8150 - acc: 0.8071 - val_loss: 0.9163 - val_acc: 0.7650\n",
      "Epoch 988/1000\n",
      "6500/6500 [==============================] - 0s 35us/step - loss: 0.8122 - acc: 0.8082 - val_loss: 0.9297 - val_acc: 0.7490\n",
      "Epoch 989/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8185 - acc: 0.8051 - val_loss: 0.9726 - val_acc: 0.7470\n",
      "Epoch 990/1000\n",
      "6500/6500 [==============================] - 0s 49us/step - loss: 0.8197 - acc: 0.8043 - val_loss: 0.9226 - val_acc: 0.7640\n",
      "Epoch 991/1000\n",
      "6500/6500 [==============================] - 0s 46us/step - loss: 0.8180 - acc: 0.8057 - val_loss: 0.9171 - val_acc: 0.7680\n",
      "Epoch 992/1000\n",
      "6500/6500 [==============================] - 0s 38us/step - loss: 0.8140 - acc: 0.8069 - val_loss: 0.9169 - val_acc: 0.7570\n",
      "Epoch 993/1000\n",
      "6500/6500 [==============================] - 0s 37us/step - loss: 0.8136 - acc: 0.8066 - val_loss: 0.9199 - val_acc: 0.7570\n",
      "Epoch 994/1000\n",
      "6500/6500 [==============================] - 0s 31us/step - loss: 0.8149 - acc: 0.8069 - val_loss: 0.9316 - val_acc: 0.7620\n",
      "Epoch 995/1000\n",
      "6500/6500 [==============================] - 0s 42us/step - loss: 0.8145 - acc: 0.8078 - val_loss: 0.9257 - val_acc: 0.7600\n",
      "Epoch 996/1000\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.8130 - acc: 0.8075 - val_loss: 0.9157 - val_acc: 0.7620\n",
      "Epoch 997/1000\n",
      "6500/6500 [==============================] - 0s 47us/step - loss: 0.8132 - acc: 0.8028 - val_loss: 0.9588 - val_acc: 0.7470\n",
      "Epoch 998/1000\n",
      "6500/6500 [==============================] - 0s 34us/step - loss: 0.8153 - acc: 0.8057 - val_loss: 0.9247 - val_acc: 0.7500\n",
      "Epoch 999/1000\n",
      "6500/6500 [==============================] - 0s 43us/step - loss: 0.8109 - acc: 0.8094 - val_loss: 0.9776 - val_acc: 0.7390\n",
      "Epoch 1000/1000\n",
      "6500/6500 [==============================] - 0s 33us/step - loss: 0.8132 - acc: 0.8086 - val_loss: 0.9385 - val_acc: 0.7600\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 65us/step\n",
      "2500/2500 [==============================] - 0s 62us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(X_train, y_train)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8200673137077918, 0.8023076923076923]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9338104454040528, 0.7412]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "6500/6500 [==============================] - 1s 132us/step - loss: 1.9839 - acc: 0.1306 - val_loss: 1.9573 - val_acc: 0.1300\n",
      "Epoch 2/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 1.9627 - acc: 0.1426 - val_loss: 1.9443 - val_acc: 0.1720\n",
      "Epoch 3/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 1.9509 - acc: 0.1575 - val_loss: 1.9354 - val_acc: 0.1900\n",
      "Epoch 4/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.9405 - acc: 0.1700 - val_loss: 1.9278 - val_acc: 0.2000\n",
      "Epoch 5/200\n",
      "6500/6500 [==============================] - 0s 64us/step - loss: 1.9324 - acc: 0.1869 - val_loss: 1.9206 - val_acc: 0.2090\n",
      "Epoch 6/200\n",
      "6500/6500 [==============================] - 1s 79us/step - loss: 1.9268 - acc: 0.1883 - val_loss: 1.9137 - val_acc: 0.2210\n",
      "Epoch 7/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 1.9209 - acc: 0.1974 - val_loss: 1.9064 - val_acc: 0.2230\n",
      "Epoch 8/200\n",
      "6500/6500 [==============================] - 0s 72us/step - loss: 1.9164 - acc: 0.2025 - val_loss: 1.8992 - val_acc: 0.2230\n",
      "Epoch 9/200\n",
      "6500/6500 [==============================] - 0s 75us/step - loss: 1.9088 - acc: 0.2040 - val_loss: 1.8925 - val_acc: 0.2280\n",
      "Epoch 10/200\n",
      "6500/6500 [==============================] - 0s 75us/step - loss: 1.8986 - acc: 0.2178 - val_loss: 1.8842 - val_acc: 0.2280\n",
      "Epoch 11/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.8914 - acc: 0.2277 - val_loss: 1.8756 - val_acc: 0.2270\n",
      "Epoch 12/200\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 1.8877 - acc: 0.2305 - val_loss: 1.8670 - val_acc: 0.2340\n",
      "Epoch 13/200\n",
      "6500/6500 [==============================] - 0s 63us/step - loss: 1.8813 - acc: 0.2322 - val_loss: 1.8577 - val_acc: 0.2420\n",
      "Epoch 14/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 1.8742 - acc: 0.2342 - val_loss: 1.8476 - val_acc: 0.2510\n",
      "Epoch 15/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.8612 - acc: 0.2491 - val_loss: 1.8350 - val_acc: 0.2660\n",
      "Epoch 16/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.8571 - acc: 0.2525 - val_loss: 1.8220 - val_acc: 0.2920\n",
      "Epoch 17/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.8497 - acc: 0.2529 - val_loss: 1.8077 - val_acc: 0.3190\n",
      "Epoch 18/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 1.8373 - acc: 0.2640 - val_loss: 1.7923 - val_acc: 0.3360\n",
      "Epoch 19/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.8241 - acc: 0.2729 - val_loss: 1.7749 - val_acc: 0.3520\n",
      "Epoch 20/200\n",
      "6500/6500 [==============================] - 0s 61us/step - loss: 1.8060 - acc: 0.2952 - val_loss: 1.7565 - val_acc: 0.3650\n",
      "Epoch 21/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.7983 - acc: 0.2894 - val_loss: 1.7369 - val_acc: 0.3710\n",
      "Epoch 22/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 1.7828 - acc: 0.3020 - val_loss: 1.7159 - val_acc: 0.3980\n",
      "Epoch 23/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 1.7708 - acc: 0.3118 - val_loss: 1.6944 - val_acc: 0.4160\n",
      "Epoch 24/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.7523 - acc: 0.3172 - val_loss: 1.6716 - val_acc: 0.4310\n",
      "Epoch 25/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.7357 - acc: 0.3223 - val_loss: 1.6475 - val_acc: 0.4450\n",
      "Epoch 26/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 1.7171 - acc: 0.3388 - val_loss: 1.6230 - val_acc: 0.4630\n",
      "Epoch 27/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 1.7081 - acc: 0.3440 - val_loss: 1.6012 - val_acc: 0.4810\n",
      "Epoch 28/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 1.6932 - acc: 0.3454 - val_loss: 1.5769 - val_acc: 0.4940\n",
      "Epoch 29/200\n",
      "6500/6500 [==============================] - 1s 82us/step - loss: 1.6828 - acc: 0.3503 - val_loss: 1.5561 - val_acc: 0.5080\n",
      "Epoch 30/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 1.6439 - acc: 0.3697 - val_loss: 1.5301 - val_acc: 0.5120\n",
      "Epoch 31/200\n",
      "6500/6500 [==============================] - 0s 67us/step - loss: 1.6384 - acc: 0.3702 - val_loss: 1.5080 - val_acc: 0.5370\n",
      "Epoch 32/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 1.6238 - acc: 0.3911 - val_loss: 1.4851 - val_acc: 0.5360\n",
      "Epoch 33/200\n",
      "6500/6500 [==============================] - 1s 93us/step - loss: 1.6084 - acc: 0.3923 - val_loss: 1.4624 - val_acc: 0.5490\n",
      "Epoch 34/200\n",
      "6500/6500 [==============================] - 0s 68us/step - loss: 1.5913 - acc: 0.3977 - val_loss: 1.4408 - val_acc: 0.5490\n",
      "Epoch 35/200\n",
      "6500/6500 [==============================] - 0s 63us/step - loss: 1.5827 - acc: 0.3945 - val_loss: 1.4204 - val_acc: 0.5650\n",
      "Epoch 36/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 1.5603 - acc: 0.4088 - val_loss: 1.3992 - val_acc: 0.5740\n",
      "Epoch 37/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 1.5374 - acc: 0.4262 - val_loss: 1.3786 - val_acc: 0.5780\n",
      "Epoch 38/200\n",
      "6500/6500 [==============================] - 0s 63us/step - loss: 1.5244 - acc: 0.4263 - val_loss: 1.3587 - val_acc: 0.5920\n",
      "Epoch 39/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.5169 - acc: 0.4285 - val_loss: 1.3366 - val_acc: 0.6010\n",
      "Epoch 40/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.5000 - acc: 0.4354 - val_loss: 1.3170 - val_acc: 0.6080\n",
      "Epoch 41/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 1.4851 - acc: 0.4394 - val_loss: 1.2984 - val_acc: 0.6140\n",
      "Epoch 42/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 1.4713 - acc: 0.4437 - val_loss: 1.2791 - val_acc: 0.6280\n",
      "Epoch 43/200\n",
      "6500/6500 [==============================] - 0s 65us/step - loss: 1.4474 - acc: 0.4551 - val_loss: 1.2585 - val_acc: 0.6290\n",
      "Epoch 44/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 1.4324 - acc: 0.4669 - val_loss: 1.2387 - val_acc: 0.6350\n",
      "Epoch 45/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.4325 - acc: 0.4563 - val_loss: 1.2235 - val_acc: 0.6360\n",
      "Epoch 46/200\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 1.4238 - acc: 0.4603 - val_loss: 1.2099 - val_acc: 0.6380\n",
      "Epoch 47/200\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 1.4003 - acc: 0.4825 - val_loss: 1.1937 - val_acc: 0.6360\n",
      "Epoch 48/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 1.3779 - acc: 0.4825 - val_loss: 1.1752 - val_acc: 0.6450\n",
      "Epoch 49/200\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 1.3731 - acc: 0.4943 - val_loss: 1.1577 - val_acc: 0.6480\n",
      "Epoch 50/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 1.3554 - acc: 0.4975 - val_loss: 1.1408 - val_acc: 0.6580\n",
      "Epoch 51/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 1.3409 - acc: 0.5074 - val_loss: 1.1270 - val_acc: 0.6520\n",
      "Epoch 52/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.3337 - acc: 0.5031 - val_loss: 1.1137 - val_acc: 0.6670\n",
      "Epoch 53/200\n",
      "6500/6500 [==============================] - 1s 86us/step - loss: 1.3282 - acc: 0.5120 - val_loss: 1.0999 - val_acc: 0.6600\n",
      "Epoch 54/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.3104 - acc: 0.5132 - val_loss: 1.0904 - val_acc: 0.6720\n",
      "Epoch 55/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 1.3032 - acc: 0.5175 - val_loss: 1.0788 - val_acc: 0.6660\n",
      "Epoch 56/200\n",
      "6500/6500 [==============================] - 0s 61us/step - loss: 1.2861 - acc: 0.5186 - val_loss: 1.0647 - val_acc: 0.6710\n",
      "Epoch 57/200\n",
      "6500/6500 [==============================] - 1s 83us/step - loss: 1.2740 - acc: 0.5283 - val_loss: 1.0544 - val_acc: 0.6730\n",
      "Epoch 58/200\n",
      "6500/6500 [==============================] - 0s 68us/step - loss: 1.2713 - acc: 0.5251 - val_loss: 1.0423 - val_acc: 0.6760\n",
      "Epoch 59/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 1.2569 - acc: 0.5283 - val_loss: 1.0283 - val_acc: 0.6790\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.2486 - acc: 0.5318 - val_loss: 1.0198 - val_acc: 0.6830\n",
      "Epoch 61/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.2404 - acc: 0.5366 - val_loss: 1.0087 - val_acc: 0.6830\n",
      "Epoch 62/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.2376 - acc: 0.5375 - val_loss: 0.9992 - val_acc: 0.6880\n",
      "Epoch 63/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 1.2172 - acc: 0.5603 - val_loss: 0.9881 - val_acc: 0.6920\n",
      "Epoch 64/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.2124 - acc: 0.5414 - val_loss: 0.9797 - val_acc: 0.6930\n",
      "Epoch 65/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.2025 - acc: 0.5551 - val_loss: 0.9705 - val_acc: 0.7010\n",
      "Epoch 66/200\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 1.1820 - acc: 0.5626 - val_loss: 0.9594 - val_acc: 0.6990\n",
      "Epoch 67/200\n",
      "6500/6500 [==============================] - 0s 61us/step - loss: 1.1881 - acc: 0.5578 - val_loss: 0.9532 - val_acc: 0.7000\n",
      "Epoch 68/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.1786 - acc: 0.5652 - val_loss: 0.9444 - val_acc: 0.7090\n",
      "Epoch 69/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 1.1792 - acc: 0.5642 - val_loss: 0.9384 - val_acc: 0.7100\n",
      "Epoch 70/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 1.1849 - acc: 0.5589 - val_loss: 0.9314 - val_acc: 0.7070\n",
      "Epoch 71/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 1.1486 - acc: 0.5802 - val_loss: 0.9230 - val_acc: 0.7110\n",
      "Epoch 72/200\n",
      "6500/6500 [==============================] - 0s 63us/step - loss: 1.1536 - acc: 0.5711 - val_loss: 0.9169 - val_acc: 0.7150\n",
      "Epoch 73/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.1302 - acc: 0.5857 - val_loss: 0.9078 - val_acc: 0.7210\n",
      "Epoch 74/200\n",
      "6500/6500 [==============================] - ETA: 0s - loss: 1.1500 - acc: 0.570 - 0s 57us/step - loss: 1.1484 - acc: 0.5708 - val_loss: 0.9025 - val_acc: 0.7150\n",
      "Epoch 75/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 1.1200 - acc: 0.5885 - val_loss: 0.8936 - val_acc: 0.7150\n",
      "Epoch 76/200\n",
      "6500/6500 [==============================] - 0s 65us/step - loss: 1.1168 - acc: 0.5949 - val_loss: 0.8876 - val_acc: 0.7180\n",
      "Epoch 77/200\n",
      "6500/6500 [==============================] - 1s 80us/step - loss: 1.1175 - acc: 0.5897 - val_loss: 0.8843 - val_acc: 0.7200\n",
      "Epoch 78/200\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 1.0998 - acc: 0.6022 - val_loss: 0.8750 - val_acc: 0.7230\n",
      "Epoch 79/200\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 1.1165 - acc: 0.5880 - val_loss: 0.8730 - val_acc: 0.7230\n",
      "Epoch 80/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.0846 - acc: 0.6037 - val_loss: 0.8648 - val_acc: 0.7300\n",
      "Epoch 81/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 1.1003 - acc: 0.5955 - val_loss: 0.8608 - val_acc: 0.7310\n",
      "Epoch 82/200\n",
      "6500/6500 [==============================] - 0s 61us/step - loss: 1.0955 - acc: 0.6028 - val_loss: 0.8549 - val_acc: 0.7300\n",
      "Epoch 83/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 1.0832 - acc: 0.6005 - val_loss: 0.8507 - val_acc: 0.7280\n",
      "Epoch 84/200\n",
      "6500/6500 [==============================] - 0s 74us/step - loss: 1.0763 - acc: 0.5991 - val_loss: 0.8446 - val_acc: 0.7310\n",
      "Epoch 85/200\n",
      "6500/6500 [==============================] - 0s 63us/step - loss: 1.0632 - acc: 0.6098 - val_loss: 0.8384 - val_acc: 0.7320\n",
      "Epoch 86/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 1.0680 - acc: 0.6122 - val_loss: 0.8341 - val_acc: 0.7330\n",
      "Epoch 87/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.0592 - acc: 0.6097 - val_loss: 0.8292 - val_acc: 0.7380\n",
      "Epoch 88/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 1.0492 - acc: 0.6057 - val_loss: 0.8262 - val_acc: 0.7390\n",
      "Epoch 89/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 1.0373 - acc: 0.6103 - val_loss: 0.8190 - val_acc: 0.7400\n",
      "Epoch 90/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 1.0428 - acc: 0.6169 - val_loss: 0.8153 - val_acc: 0.7420\n",
      "Epoch 91/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 1.0394 - acc: 0.6218 - val_loss: 0.8122 - val_acc: 0.7460\n",
      "Epoch 92/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 1.0473 - acc: 0.6260 - val_loss: 0.8103 - val_acc: 0.7430\n",
      "Epoch 93/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 1.0280 - acc: 0.6238 - val_loss: 0.8047 - val_acc: 0.7430\n",
      "Epoch 94/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 1.0383 - acc: 0.6188 - val_loss: 0.8013 - val_acc: 0.7440\n",
      "Epoch 95/200\n",
      "6500/6500 [==============================] - 0s 72us/step - loss: 1.0121 - acc: 0.6300 - val_loss: 0.7975 - val_acc: 0.7420\n",
      "Epoch 96/200\n",
      "6500/6500 [==============================] - 0s 71us/step - loss: 1.0155 - acc: 0.6286 - val_loss: 0.7925 - val_acc: 0.7420\n",
      "Epoch 97/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 1.0189 - acc: 0.6186 - val_loss: 0.7890 - val_acc: 0.7440\n",
      "Epoch 98/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 1.0076 - acc: 0.6272 - val_loss: 0.7872 - val_acc: 0.7400\n",
      "Epoch 99/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 1.0038 - acc: 0.6368 - val_loss: 0.7822 - val_acc: 0.7450\n",
      "Epoch 100/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.9913 - acc: 0.6285 - val_loss: 0.7807 - val_acc: 0.7480\n",
      "Epoch 101/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.9772 - acc: 0.6471 - val_loss: 0.7755 - val_acc: 0.7470\n",
      "Epoch 102/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.9950 - acc: 0.6277 - val_loss: 0.7721 - val_acc: 0.7480\n",
      "Epoch 103/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.9715 - acc: 0.6463 - val_loss: 0.7687 - val_acc: 0.7500\n",
      "Epoch 104/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.9907 - acc: 0.6375 - val_loss: 0.7651 - val_acc: 0.7510\n",
      "Epoch 105/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.9745 - acc: 0.6385 - val_loss: 0.7617 - val_acc: 0.7520\n",
      "Epoch 106/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.9735 - acc: 0.6469 - val_loss: 0.7604 - val_acc: 0.7500\n",
      "Epoch 107/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.9610 - acc: 0.6449 - val_loss: 0.7540 - val_acc: 0.7510\n",
      "Epoch 108/200\n",
      "6500/6500 [==============================] - 0s 61us/step - loss: 0.9793 - acc: 0.6357 - val_loss: 0.7560 - val_acc: 0.7510\n",
      "Epoch 109/200\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 0.9593 - acc: 0.6485 - val_loss: 0.7524 - val_acc: 0.7510\n",
      "Epoch 110/200\n",
      "6500/6500 [==============================] - 0s 73us/step - loss: 0.9577 - acc: 0.6554 - val_loss: 0.7471 - val_acc: 0.7560\n",
      "Epoch 111/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.9532 - acc: 0.6515 - val_loss: 0.7428 - val_acc: 0.7510\n",
      "Epoch 112/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.9579 - acc: 0.6508 - val_loss: 0.7409 - val_acc: 0.7550\n",
      "Epoch 113/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.9419 - acc: 0.6562 - val_loss: 0.7397 - val_acc: 0.7570\n",
      "Epoch 114/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.9538 - acc: 0.6469 - val_loss: 0.7373 - val_acc: 0.7530\n",
      "Epoch 115/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.9399 - acc: 0.6611 - val_loss: 0.7356 - val_acc: 0.7540\n",
      "Epoch 116/200\n",
      "6500/6500 [==============================] - 0s 76us/step - loss: 0.9242 - acc: 0.6591 - val_loss: 0.7321 - val_acc: 0.7560\n",
      "Epoch 117/200\n",
      "6500/6500 [==============================] - 0s 67us/step - loss: 0.9231 - acc: 0.6662 - val_loss: 0.7293 - val_acc: 0.7540\n",
      "Epoch 118/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 0.9275 - acc: 0.6562 - val_loss: 0.7275 - val_acc: 0.7580\n",
      "Epoch 119/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.9176 - acc: 0.6646 - val_loss: 0.7249 - val_acc: 0.7610\n",
      "Epoch 120/200\n",
      "6500/6500 [==============================] - 0s 64us/step - loss: 0.9314 - acc: 0.6609 - val_loss: 0.7226 - val_acc: 0.7550\n",
      "Epoch 121/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.9277 - acc: 0.6537 - val_loss: 0.7232 - val_acc: 0.7570\n",
      "Epoch 122/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.9109 - acc: 0.6677 - val_loss: 0.7180 - val_acc: 0.7600\n",
      "Epoch 123/200\n",
      "6500/6500 [==============================] - 0s 64us/step - loss: 0.9096 - acc: 0.6695 - val_loss: 0.7169 - val_acc: 0.7580\n",
      "Epoch 124/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.9120 - acc: 0.6714 - val_loss: 0.7142 - val_acc: 0.7610\n",
      "Epoch 125/200\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 0.9147 - acc: 0.6588 - val_loss: 0.7131 - val_acc: 0.7600\n",
      "Epoch 126/200\n",
      "6500/6500 [==============================] - 0s 67us/step - loss: 0.9109 - acc: 0.6658 - val_loss: 0.7131 - val_acc: 0.7600\n",
      "Epoch 127/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.9118 - acc: 0.6614 - val_loss: 0.7133 - val_acc: 0.7610\n",
      "Epoch 128/200\n",
      "6500/6500 [==============================] - 0s 63us/step - loss: 0.8990 - acc: 0.6723 - val_loss: 0.7109 - val_acc: 0.7600\n",
      "Epoch 129/200\n",
      "6500/6500 [==============================] - 0s 65us/step - loss: 0.8940 - acc: 0.6720 - val_loss: 0.7086 - val_acc: 0.7610\n",
      "Epoch 130/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.8995 - acc: 0.6725 - val_loss: 0.7052 - val_acc: 0.7620\n",
      "Epoch 131/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 0.8880 - acc: 0.6748 - val_loss: 0.7038 - val_acc: 0.7600\n",
      "Epoch 132/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.9020 - acc: 0.6655 - val_loss: 0.7024 - val_acc: 0.7650\n",
      "Epoch 133/200\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 0.8934 - acc: 0.6763 - val_loss: 0.7005 - val_acc: 0.7620\n",
      "Epoch 134/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.8826 - acc: 0.6772 - val_loss: 0.6995 - val_acc: 0.7630\n",
      "Epoch 135/200\n",
      "6500/6500 [==============================] - 1s 81us/step - loss: 0.8844 - acc: 0.6726 - val_loss: 0.6958 - val_acc: 0.7630\n",
      "Epoch 136/200\n",
      "6500/6500 [==============================] - 1s 77us/step - loss: 0.8776 - acc: 0.6726 - val_loss: 0.6938 - val_acc: 0.7600\n",
      "Epoch 137/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.8742 - acc: 0.6808 - val_loss: 0.6903 - val_acc: 0.7650\n",
      "Epoch 138/200\n",
      "6500/6500 [==============================] - 0s 61us/step - loss: 0.8786 - acc: 0.6748 - val_loss: 0.6909 - val_acc: 0.7640\n",
      "Epoch 139/200\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 0.8728 - acc: 0.6772 - val_loss: 0.6889 - val_acc: 0.7630\n",
      "Epoch 140/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.8530 - acc: 0.6877 - val_loss: 0.6867 - val_acc: 0.7630\n",
      "Epoch 141/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 0.8744 - acc: 0.6763 - val_loss: 0.6866 - val_acc: 0.7640\n",
      "Epoch 142/200\n",
      "6500/6500 [==============================] - 0s 63us/step - loss: 0.8647 - acc: 0.6845 - val_loss: 0.6840 - val_acc: 0.7640\n",
      "Epoch 143/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.8731 - acc: 0.6775 - val_loss: 0.6828 - val_acc: 0.7650\n",
      "Epoch 144/200\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 0.8571 - acc: 0.6780 - val_loss: 0.6830 - val_acc: 0.7660\n",
      "Epoch 145/200\n",
      "6500/6500 [==============================] - 0s 61us/step - loss: 0.8735 - acc: 0.6811 - val_loss: 0.6828 - val_acc: 0.7660\n",
      "Epoch 146/200\n",
      "6500/6500 [==============================] - 0s 61us/step - loss: 0.8528 - acc: 0.6854 - val_loss: 0.6812 - val_acc: 0.7660\n",
      "Epoch 147/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 0.8666 - acc: 0.6785 - val_loss: 0.6801 - val_acc: 0.7680\n",
      "Epoch 148/200\n",
      "6500/6500 [==============================] - 0s 63us/step - loss: 0.8459 - acc: 0.6823 - val_loss: 0.6776 - val_acc: 0.7650\n",
      "Epoch 149/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.8383 - acc: 0.6955 - val_loss: 0.6773 - val_acc: 0.7640\n",
      "Epoch 150/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.8481 - acc: 0.6874 - val_loss: 0.6741 - val_acc: 0.7660\n",
      "Epoch 151/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.8435 - acc: 0.6888 - val_loss: 0.6749 - val_acc: 0.7650\n",
      "Epoch 152/200\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 0.8410 - acc: 0.6902 - val_loss: 0.6710 - val_acc: 0.7660\n",
      "Epoch 153/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.8535 - acc: 0.6895 - val_loss: 0.6724 - val_acc: 0.7670\n",
      "Epoch 154/200\n",
      "6500/6500 [==============================] - 1s 87us/step - loss: 0.8334 - acc: 0.6988 - val_loss: 0.6706 - val_acc: 0.7670\n",
      "Epoch 155/200\n",
      "6500/6500 [==============================] - 0s 64us/step - loss: 0.8248 - acc: 0.6992 - val_loss: 0.6683 - val_acc: 0.7660\n",
      "Epoch 156/200\n",
      "6500/6500 [==============================] - 0s 63us/step - loss: 0.8349 - acc: 0.6954 - val_loss: 0.6668 - val_acc: 0.7650\n",
      "Epoch 157/200\n",
      "6500/6500 [==============================] - 0s 50us/step - loss: 0.8338 - acc: 0.6929 - val_loss: 0.6682 - val_acc: 0.7630\n",
      "Epoch 158/200\n",
      "6500/6500 [==============================] - 1s 85us/step - loss: 0.8243 - acc: 0.6985 - val_loss: 0.6683 - val_acc: 0.7660\n",
      "Epoch 159/200\n",
      "6500/6500 [==============================] - 1s 81us/step - loss: 0.8255 - acc: 0.6972 - val_loss: 0.6639 - val_acc: 0.7670\n",
      "Epoch 160/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.8259 - acc: 0.7017 - val_loss: 0.6629 - val_acc: 0.7680\n",
      "Epoch 161/200\n",
      "6500/6500 [==============================] - 0s 71us/step - loss: 0.8311 - acc: 0.6925 - val_loss: 0.6636 - val_acc: 0.7620\n",
      "Epoch 162/200\n",
      "6500/6500 [==============================] - 1s 80us/step - loss: 0.8360 - acc: 0.6977 - val_loss: 0.6615 - val_acc: 0.7660\n",
      "Epoch 163/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.8327 - acc: 0.6931 - val_loss: 0.6605 - val_acc: 0.7690\n",
      "Epoch 164/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.8086 - acc: 0.7015 - val_loss: 0.6610 - val_acc: 0.7650\n",
      "Epoch 165/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.8191 - acc: 0.7026 - val_loss: 0.6586 - val_acc: 0.7670\n",
      "Epoch 166/200\n",
      "6500/6500 [==============================] - 0s 65us/step - loss: 0.8199 - acc: 0.7006 - val_loss: 0.6579 - val_acc: 0.7640\n",
      "Epoch 167/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.8163 - acc: 0.7022 - val_loss: 0.6557 - val_acc: 0.7650\n",
      "Epoch 168/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.8144 - acc: 0.7008 - val_loss: 0.6538 - val_acc: 0.7690\n",
      "Epoch 169/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.8114 - acc: 0.6960 - val_loss: 0.6552 - val_acc: 0.7690\n",
      "Epoch 170/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.8002 - acc: 0.7000 - val_loss: 0.6526 - val_acc: 0.7660\n",
      "Epoch 171/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.8057 - acc: 0.7023 - val_loss: 0.6523 - val_acc: 0.7680\n",
      "Epoch 172/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.8066 - acc: 0.7005 - val_loss: 0.6515 - val_acc: 0.7660\n",
      "Epoch 173/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.8068 - acc: 0.7032 - val_loss: 0.6510 - val_acc: 0.7720\n",
      "Epoch 174/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.8105 - acc: 0.7011 - val_loss: 0.6500 - val_acc: 0.7660\n",
      "Epoch 175/200\n",
      "6500/6500 [==============================] - 0s 60us/step - loss: 0.8083 - acc: 0.7028 - val_loss: 0.6522 - val_acc: 0.7690\n",
      "Epoch 176/200\n",
      "6500/6500 [==============================] - 0s 58us/step - loss: 0.7939 - acc: 0.7009 - val_loss: 0.6502 - val_acc: 0.7670\n",
      "Epoch 177/200\n",
      "6500/6500 [==============================] - 0s 56us/step - loss: 0.7920 - acc: 0.7083 - val_loss: 0.6474 - val_acc: 0.7700\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.7745 - acc: 0.7158 - val_loss: 0.6451 - val_acc: 0.7710\n",
      "Epoch 179/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.7860 - acc: 0.7097 - val_loss: 0.6444 - val_acc: 0.7690\n",
      "Epoch 180/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.7967 - acc: 0.7094 - val_loss: 0.6472 - val_acc: 0.7680\n",
      "Epoch 181/200\n",
      "6500/6500 [==============================] - 0s 66us/step - loss: 0.7827 - acc: 0.7091 - val_loss: 0.6448 - val_acc: 0.7690\n",
      "Epoch 182/200\n",
      "6500/6500 [==============================] - 0s 73us/step - loss: 0.7938 - acc: 0.7085 - val_loss: 0.6436 - val_acc: 0.7690\n",
      "Epoch 183/200\n",
      "6500/6500 [==============================] - 0s 53us/step - loss: 0.7958 - acc: 0.7068 - val_loss: 0.6434 - val_acc: 0.7690\n",
      "Epoch 184/200\n",
      "6500/6500 [==============================] - 0s 72us/step - loss: 0.7843 - acc: 0.7063 - val_loss: 0.6440 - val_acc: 0.7670\n",
      "Epoch 185/200\n",
      "6500/6500 [==============================] - 1s 84us/step - loss: 0.7813 - acc: 0.7111 - val_loss: 0.6417 - val_acc: 0.7670\n",
      "Epoch 186/200\n",
      "6500/6500 [==============================] - 0s 66us/step - loss: 0.7855 - acc: 0.7083 - val_loss: 0.6414 - val_acc: 0.7720\n",
      "Epoch 187/200\n",
      "6500/6500 [==============================] - 0s 61us/step - loss: 0.7770 - acc: 0.7125 - val_loss: 0.6411 - val_acc: 0.7680\n",
      "Epoch 188/200\n",
      "6500/6500 [==============================] - 0s 62us/step - loss: 0.7708 - acc: 0.7095 - val_loss: 0.6409 - val_acc: 0.7660\n",
      "Epoch 189/200\n",
      "6500/6500 [==============================] - 0s 61us/step - loss: 0.7786 - acc: 0.7163 - val_loss: 0.6388 - val_acc: 0.7690\n",
      "Epoch 190/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 0.7650 - acc: 0.7214 - val_loss: 0.6370 - val_acc: 0.7670\n",
      "Epoch 191/200\n",
      "6500/6500 [==============================] - 0s 61us/step - loss: 0.7686 - acc: 0.7149 - val_loss: 0.6384 - val_acc: 0.7690\n",
      "Epoch 192/200\n",
      "6500/6500 [==============================] - 0s 55us/step - loss: 0.7540 - acc: 0.7183 - val_loss: 0.6346 - val_acc: 0.7690\n",
      "Epoch 193/200\n",
      "6500/6500 [==============================] - 0s 59us/step - loss: 0.7709 - acc: 0.7158 - val_loss: 0.6354 - val_acc: 0.7710\n",
      "Epoch 194/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.7789 - acc: 0.7123 - val_loss: 0.6362 - val_acc: 0.7700\n",
      "Epoch 195/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.7650 - acc: 0.7198 - val_loss: 0.6328 - val_acc: 0.7680\n",
      "Epoch 196/200\n",
      "6500/6500 [==============================] - 0s 54us/step - loss: 0.7610 - acc: 0.7140 - val_loss: 0.6317 - val_acc: 0.7700\n",
      "Epoch 197/200\n",
      "6500/6500 [==============================] - 0s 52us/step - loss: 0.7557 - acc: 0.7229 - val_loss: 0.6322 - val_acc: 0.7680\n",
      "Epoch 198/200\n",
      "6500/6500 [==============================] - 0s 61us/step - loss: 0.7600 - acc: 0.7195 - val_loss: 0.6322 - val_acc: 0.7690\n",
      "Epoch 199/200\n",
      "6500/6500 [==============================] - 0s 57us/step - loss: 0.7580 - acc: 0.7205 - val_loss: 0.6326 - val_acc: 0.7700\n",
      "Epoch 200/200\n",
      "6500/6500 [==============================] - 0s 51us/step - loss: 0.7625 - acc: 0.7185 - val_loss: 0.6315 - val_acc: 0.7720\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500/6500 [==============================] - 0s 75us/step\n",
      "2500/2500 [==============================] - 0s 72us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(X_train, y_train)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.47411381301513084, 0.8387692307692308]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6167132836341858, 0.7556]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 1.9122 - acc: 0.2208 - val_loss: 1.8812 - val_acc: 0.2573\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 1.8427 - acc: 0.2870 - val_loss: 1.7928 - val_acc: 0.3263\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 1.7286 - acc: 0.3839 - val_loss: 1.6558 - val_acc: 0.4410\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 44us/step - loss: 1.5693 - acc: 0.4897 - val_loss: 1.4817 - val_acc: 0.5410\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 1.3822 - acc: 0.5838 - val_loss: 1.2913 - val_acc: 0.6227\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 1.1930 - acc: 0.6533 - val_loss: 1.1140 - val_acc: 0.6673\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 1.0296 - acc: 0.6931 - val_loss: 0.9724 - val_acc: 0.7053\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.9077 - acc: 0.7127 - val_loss: 0.8736 - val_acc: 0.7227\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.8234 - acc: 0.7285 - val_loss: 0.8088 - val_acc: 0.7313\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.7653 - acc: 0.7397 - val_loss: 0.7590 - val_acc: 0.7477\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.7231 - acc: 0.7473 - val_loss: 0.7254 - val_acc: 0.7493\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.6912 - acc: 0.7538 - val_loss: 0.6984 - val_acc: 0.7543\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.6660 - acc: 0.7602 - val_loss: 0.6776 - val_acc: 0.7610\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.6453 - acc: 0.7670 - val_loss: 0.6620 - val_acc: 0.7640\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.6280 - acc: 0.7717 - val_loss: 0.6482 - val_acc: 0.7630\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.6130 - acc: 0.7775 - val_loss: 0.6365 - val_acc: 0.7673\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5997 - acc: 0.7827 - val_loss: 0.6258 - val_acc: 0.7710\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5879 - acc: 0.7863 - val_loss: 0.6174 - val_acc: 0.7730\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5775 - acc: 0.7904 - val_loss: 0.6084 - val_acc: 0.7790\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.5675 - acc: 0.7943 - val_loss: 0.6024 - val_acc: 0.7817\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5585 - acc: 0.7977 - val_loss: 0.5956 - val_acc: 0.7823\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.5502 - acc: 0.8000 - val_loss: 0.5893 - val_acc: 0.7887\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5425 - acc: 0.8043 - val_loss: 0.5837 - val_acc: 0.7853\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5352 - acc: 0.8071 - val_loss: 0.5799 - val_acc: 0.7863\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5280 - acc: 0.8095 - val_loss: 0.5746 - val_acc: 0.7920\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5218 - acc: 0.8108 - val_loss: 0.5712 - val_acc: 0.7917\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.5157 - acc: 0.8142 - val_loss: 0.5665 - val_acc: 0.7927\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.5095 - acc: 0.8158 - val_loss: 0.5645 - val_acc: 0.7930\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.5041 - acc: 0.8175 - val_loss: 0.5592 - val_acc: 0.7940\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 41us/step - loss: 0.4986 - acc: 0.8202 - val_loss: 0.5571 - val_acc: 0.7970\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4936 - acc: 0.8208 - val_loss: 0.5543 - val_acc: 0.8003\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4884 - acc: 0.8241 - val_loss: 0.5509 - val_acc: 0.8033\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4837 - acc: 0.8254 - val_loss: 0.5476 - val_acc: 0.8033\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4790 - acc: 0.8276 - val_loss: 0.5452 - val_acc: 0.8023\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4745 - acc: 0.8301 - val_loss: 0.5444 - val_acc: 0.8030\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4703 - acc: 0.8300 - val_loss: 0.5427 - val_acc: 0.8020\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4661 - acc: 0.8318 - val_loss: 0.5377 - val_acc: 0.8073\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.4620 - acc: 0.8334 - val_loss: 0.5367 - val_acc: 0.8047\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.4582 - acc: 0.8347 - val_loss: 0.5366 - val_acc: 0.8040\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4543 - acc: 0.8368 - val_loss: 0.5335 - val_acc: 0.8070\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4506 - acc: 0.8378 - val_loss: 0.5369 - val_acc: 0.8063\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4470 - acc: 0.8394 - val_loss: 0.5303 - val_acc: 0.8087\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4436 - acc: 0.8401 - val_loss: 0.5294 - val_acc: 0.8067\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4404 - acc: 0.8422 - val_loss: 0.5274 - val_acc: 0.8140\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4371 - acc: 0.8430 - val_loss: 0.5263 - val_acc: 0.8107\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4338 - acc: 0.8438 - val_loss: 0.5257 - val_acc: 0.8113\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4308 - acc: 0.8453 - val_loss: 0.5253 - val_acc: 0.8100\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4276 - acc: 0.8470 - val_loss: 0.5235 - val_acc: 0.8120\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4249 - acc: 0.8465 - val_loss: 0.5222 - val_acc: 0.8093\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4218 - acc: 0.8489 - val_loss: 0.5222 - val_acc: 0.8117\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4189 - acc: 0.8501 - val_loss: 0.5211 - val_acc: 0.8087\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4163 - acc: 0.8510 - val_loss: 0.5195 - val_acc: 0.8107\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4131 - acc: 0.8518 - val_loss: 0.5217 - val_acc: 0.8117\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.4110 - acc: 0.8526 - val_loss: 0.5186 - val_acc: 0.8127\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.4082 - acc: 0.8551 - val_loss: 0.5195 - val_acc: 0.8093\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.4057 - acc: 0.8559 - val_loss: 0.5185 - val_acc: 0.8100\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.4031 - acc: 0.8572 - val_loss: 0.5190 - val_acc: 0.8137\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.4009 - acc: 0.8572 - val_loss: 0.5209 - val_acc: 0.8120\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3987 - acc: 0.8585 - val_loss: 0.5174 - val_acc: 0.8123\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3959 - acc: 0.8593 - val_loss: 0.5180 - val_acc: 0.8107\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.3938 - acc: 0.8594 - val_loss: 0.5181 - val_acc: 0.8137\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.3919 - acc: 0.8614 - val_loss: 0.5164 - val_acc: 0.8137\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3894 - acc: 0.8622 - val_loss: 0.5170 - val_acc: 0.8110\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3873 - acc: 0.8628 - val_loss: 0.5174 - val_acc: 0.8107\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3850 - acc: 0.8630 - val_loss: 0.5165 - val_acc: 0.8097\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3830 - acc: 0.8642 - val_loss: 0.5173 - val_acc: 0.8100\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3813 - acc: 0.8653 - val_loss: 0.5166 - val_acc: 0.8130\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3790 - acc: 0.8652 - val_loss: 0.5175 - val_acc: 0.8093\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 29us/step - loss: 0.3769 - acc: 0.8668 - val_loss: 0.5178 - val_acc: 0.8107\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3749 - acc: 0.8673 - val_loss: 0.5211 - val_acc: 0.8097\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3734 - acc: 0.8675 - val_loss: 0.5185 - val_acc: 0.8117\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.3715 - acc: 0.8690 - val_loss: 0.5201 - val_acc: 0.8097\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.3695 - acc: 0.8686 - val_loss: 0.5174 - val_acc: 0.8127\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3674 - acc: 0.8689 - val_loss: 0.5190 - val_acc: 0.8117\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 43us/step - loss: 0.3658 - acc: 0.8702 - val_loss: 0.5191 - val_acc: 0.8117\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3639 - acc: 0.8708 - val_loss: 0.5218 - val_acc: 0.8103\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3625 - acc: 0.8710 - val_loss: 0.5198 - val_acc: 0.8117\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3607 - acc: 0.8716 - val_loss: 0.5193 - val_acc: 0.8113\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.3594 - acc: 0.8720 - val_loss: 0.5242 - val_acc: 0.8083\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3574 - acc: 0.8726 - val_loss: 0.5213 - val_acc: 0.8143\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3554 - acc: 0.8739 - val_loss: 0.5223 - val_acc: 0.8107\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3540 - acc: 0.8745 - val_loss: 0.5208 - val_acc: 0.8130\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 42us/step - loss: 0.3523 - acc: 0.8748 - val_loss: 0.5229 - val_acc: 0.8120\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3508 - acc: 0.8762 - val_loss: 0.5220 - val_acc: 0.8123\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3490 - acc: 0.8766 - val_loss: 0.5248 - val_acc: 0.8087\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3479 - acc: 0.8777 - val_loss: 0.5229 - val_acc: 0.8133\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3463 - acc: 0.8772 - val_loss: 0.5236 - val_acc: 0.8140\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.3447 - acc: 0.8783 - val_loss: 0.5260 - val_acc: 0.8113\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3430 - acc: 0.8803 - val_loss: 0.5260 - val_acc: 0.8127\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3415 - acc: 0.8801 - val_loss: 0.5255 - val_acc: 0.8130\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3406 - acc: 0.8797 - val_loss: 0.5285 - val_acc: 0.8107\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3389 - acc: 0.8812 - val_loss: 0.5271 - val_acc: 0.8163\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3374 - acc: 0.8817 - val_loss: 0.5296 - val_acc: 0.8100\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3357 - acc: 0.8811 - val_loss: 0.5308 - val_acc: 0.8087\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3346 - acc: 0.8824 - val_loss: 0.5299 - val_acc: 0.8127\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3331 - acc: 0.8828 - val_loss: 0.5311 - val_acc: 0.8117\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 34us/step - loss: 0.3320 - acc: 0.8832 - val_loss: 0.5321 - val_acc: 0.8127\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3306 - acc: 0.8831 - val_loss: 0.5309 - val_acc: 0.8130\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3297 - acc: 0.8830 - val_loss: 0.5328 - val_acc: 0.8117\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 40us/step - loss: 0.3279 - acc: 0.8855 - val_loss: 0.5367 - val_acc: 0.8087\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3266 - acc: 0.8852 - val_loss: 0.5337 - val_acc: 0.8127\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3253 - acc: 0.8859 - val_loss: 0.5353 - val_acc: 0.8133\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3242 - acc: 0.8862 - val_loss: 0.5345 - val_acc: 0.8127\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3228 - acc: 0.8871 - val_loss: 0.5364 - val_acc: 0.8140\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 0.3214 - acc: 0.8871 - val_loss: 0.5382 - val_acc: 0.8103\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3207 - acc: 0.8875 - val_loss: 0.5391 - val_acc: 0.8130\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3188 - acc: 0.8889 - val_loss: 0.5399 - val_acc: 0.8097\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3179 - acc: 0.8882 - val_loss: 0.5398 - val_acc: 0.8090\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3166 - acc: 0.8885 - val_loss: 0.5395 - val_acc: 0.8133\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3150 - acc: 0.8899 - val_loss: 0.5412 - val_acc: 0.8120\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 35us/step - loss: 0.3144 - acc: 0.8891 - val_loss: 0.5439 - val_acc: 0.8110\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3131 - acc: 0.8909 - val_loss: 0.5439 - val_acc: 0.8107\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3119 - acc: 0.8913 - val_loss: 0.5436 - val_acc: 0.8130\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3106 - acc: 0.8903 - val_loss: 0.5451 - val_acc: 0.8110\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3097 - acc: 0.8914 - val_loss: 0.5466 - val_acc: 0.8130\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.3082 - acc: 0.8929 - val_loss: 0.5459 - val_acc: 0.8113\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 1s 37us/step - loss: 0.3071 - acc: 0.8925 - val_loss: 0.5495 - val_acc: 0.8107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 39us/step - loss: 0.3063 - acc: 0.8926 - val_loss: 0.5494 - val_acc: 0.8103\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 0.3047 - acc: 0.8926 - val_loss: 0.5500 - val_acc: 0.8107\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3037 - acc: 0.8934 - val_loss: 0.5517 - val_acc: 0.8103\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 2s 66us/step\n",
      "4000/4000 [==============================] - 0s 66us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3007782207474564, 0.8943333333333333]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6266642330884934, 0.79125]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
